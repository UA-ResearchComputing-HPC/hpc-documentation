{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"policies/acceptable_use/","title":"Acceptable Use","text":"<p>High Performance Computing (HPC) facility users are responsible for complying with all University policies.</p> <p>The supercomputers represent a unique resource for the campus community. These computers have special characteristics that are not found, or are of limited availability, on other central computers, including parallel processing, large memory, and a Linux operating system. The UArizona HPC resources require close supervision by those charged with management of these resources.  </p>"},{"location":"policies/acceptable_use/#controlled-data","title":"Controlled Data","text":"<p>UArizona HPC does not provide support for any type of controlled data. No controlled data (HIPAA, EAR, FERPA, PII, CUI, ITAR, etc.) can be analysed or stored on any HPC storage.</p> <p>For HIPAA data we maintain a separate cluster called Soteria. See Secure Services.</p>"},{"location":"policies/acceptable_use/#federal-regulations","title":"Federal Regulations","text":"<p>Equipment purchased exclusively for research purposes is exempt from Arizona State Sales Tax. See UA FSO statement of Research Equipment Tax Exemption. However, there are exceptions to this exemption that impact central, shared use research facilities. These exceptions include \"research in social sciences or psychology\"\u2014see AZ ARS 42-5061 subsection B.15.</p> <p>Machinery or equipment used in research and development. For the purposes of this paragraph, \"research and development\" means basic and applied research in the sciences and engineering, and designing, developing or testing prototypes, processes or new products, including research and development of computer software that is embedded in or an integral part of the prototype or new product or that is required for machinery or equipment otherwise exempt under this section to function effectively. Research and development do not include manufacturing quality control, routine consumer product testing, market research, sales promotion, sales service, research in social sciences or psychology, computer software research that is not included in the definition of research and development, or other nontechnological activities or technical services. (emphasis added)</p> <p>In order to provide research computing resources to researchers from these exception areas, some of the research computing resources have been purchased with Arizona taxes included. The result is that there are resources available to all campus researchers, with the caveat that researchers in the social sciences, psychology and instructional projects areas are restricted to using resources that are purchased with taxes paid.</p> <p>Please contact our HPC consultants to learn about the resources that are available for social sciences, psychology and instructional purposes.</p>"},{"location":"policies/acceptable_use/#access-for-research-and-limited-access-for-instruction","title":"Access for Research and Limited Access for Instruction","text":"<p>As described in the 'Sales Tax Exemption' section above, most of the HPC systems are limited to research applications as defined in Section B-14 of ARS Statute 42-5061 by the Arizona Legislature. All users are expected to use these resources accordingly and to use other computing systems for non-research purposes.</p>"},{"location":"policies/acknowledgements/","title":"Acknowledgements","text":"<p>PIs should notify our HPC consultants about posters or other publications (published, accepted, submitted, or in preparation) that benefited from the use of UA High Performance Computing, Statistical Consulting, and/or Data &amp; Visualization Consulting. These will be listed in the Collection of Published Results.</p> <p>Results</p>"},{"location":"policies/acknowledgements/#acknowledging-the-uarizona-hpc-resources","title":"Acknowledging the UArizona HPC Resources","text":"<p>The suggested format to acknowledge University of Arizona High-Performance Computing in a paper, poster, or presentation is:</p> <p>This material is based upon High Performance Computing (HPC) resources supported by the University of Arizona TRIF, UITS, and Research, Innovation, and Impact (RII) and maintained by the UArizona Research Technologies department.</p>"},{"location":"policies/acknowledgements/#acknowledging-contributions-from-a-uarizona-research-technologies-staff-member-or-consultant","title":"Acknowledging Contributions from a UArizona Research Technologies Staff Member or Consultant","text":"<p>If you wish to additionally acknowledge an individual who assisted you from University of Arizona High-Performance Computing, the suggested format is:</p> <p>We thank [consultant's name(s)] for [his/her/their] assistance with [describe tasks accomplished], which was made possible through University of Arizona Research Technologies Collaborative Support program. </p>"},{"location":"policies/buy_in/","title":"Buy-in","text":""},{"location":"policies/buy_in/#overview","title":"Overview","text":"<p>The University of Arizona's High Performance Computing (HPC) clusters are servers (computing nodes) and associated high performance storage. There are additional nodes to meet specific needs like high amounts of memory or GPUs. All UA research faculty can sign up for free monthly allocation following these directions. For researchers who need compute resource beyond the free standard allocation, and who have funding available, we encourage 'buy-in' of additional compute nodes.</p>"},{"location":"policies/buy_in/#benefits-of-buy-in","title":"Benefits of Buy-in","text":"Dedicated Research Compute Research groups can 'Buy-In' (add resources such as processors, memory, etc.) to the base HPC systems as funding becomes available. Researchers receive 100% of the CPU*hour time their purchases create as a monthly high-priority allocation. This time receives the highest priority queue on the HPC systems. Quality Environment The Buy-In option allows research groups to take advantage of the central machine room space that is designed for maintaining high performance computing resources. The UITS Research Technologies group physically maintains the purchased nodes, applies updates and patches, monitors the systems for performance and security, and manages software. Additionally, Research Technologies staff is available for research support. In short, essentially all costs associated with maintaining compute resources are covered by UITS rather than individual researchers. Flexible Capacity Buy-in research group members also benefit from their resources being integrated into a larger computing resource. This means the buy-in resources can be used in conjunction with the free allocation and resources provided to address computational projects that would be beyond the capacity of a group running an independent system alone. Shared Resource The University research computing community as a whole benefits from buy-in expansions to the HPC systems. As mentioned above, researchers who buy-in receive 100% of the allocation of time for their purchase. However if the buy-in resources are not fully utilized, they are made available as windfall resources. This helps to ensure full use of all HPC resources and can be used to justify future purchases of computing resources. Cost Competitiveness Lower costs included in the grant proposals (i.e. hardware only, no operations costs) and evidence of campus cost\u2010sharing give a positive advantage during funding agency review. Pricing For the year following the award the UA HPC request for proposal (RFP) pricing is locked in and is often considerably less than the \"market price.\""},{"location":"policies/buy_in/#buy-in-policies","title":"Buy-in Policies","text":"<ul> <li>For Puma, the University of Arizona could only purchase whole chassis units from Penguin Computing. That is 4 CPU nodes (option 1D), 1 GPU node with 4 GPUs (option 2D), or 1 high memory nodes (option 3). Research Computing worked to match partial node buy-in requests to make full nodes.</li> <li>Monthly high priority time is calculated as: (Number of CPUs * 24 hours * 365 year) / 12 months</li> <li>Purchasing GPUs expands the limit the PI has on number of GPUs that can be used at any time</li> <li>Buy-in high priority allocations will last the lifetime of the system. Puma was purchased in August 2020 and will be officially end-of-life August 2025.</li> <li>The HPC Buy-in program is not designed to replace or compete with the very large\u2010scale resources at national NSF and DOE facilities, e.g. ACCESS, the Open Science Grid. National resources are available at no financial cost to most US-based researchers through competitive proposal processes. Please contact our consulting team if you are interested in applying for these resources.</li> <li>The HPC Buy-in program is designed to meet the needs of researchers with medium\u2010scale HPC requirements who want guaranteed, consistent access to compute resources.</li> </ul>"},{"location":"policies/buy_in/#high-priority-allocation-policies","title":"High-priority Allocation Policies","text":"<ul> <li>Standard and high priority jobs will preempt windfall jobs when necessary. </li> <li>Standard jobs do not run on high priority nodes since standard jobs can not be preempted</li> <li>High priority jobs are run on both the buy-in nodes and the centrally-funded nodes. This is advantageous if there is a short-term project deadline.</li> </ul>"},{"location":"policies/buy_in/#compute-buy-in-details-puma-2020","title":"Compute Buy-in Details (Puma 2020)","text":""},{"location":"policies/buy_in/#hardware","title":"Hardware","text":"<p>The buy-in process for Puma has ended. The community will be informed when the next purchase cycle is announced.</p> Buy-in Option Technical Specs CPU-Only NodePenguin Computing Altus XE2242 There are 4 CPU nodes in an Altus XE2242 chassisTechnical specs for 1 node of 4 in an Altus XE2242 chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225 W) - 512 GB RAM, DDR4-3200MHz REG, ECC, 2Rx4 (16 x 32 GB) - 2 TB SSD local hard drive, 2.5\u201d, NVMe, 4 Lane, 1 DWPD, 3D TLC GPU Node Penguin Computing Altus XE2214GT GPU chassis have 4 GPUs in themTechnical specs for the full XE2214GT chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.3 GHz, 225 W) - 4 NVIDIA Tesla V100S-PCIe, 32 GB video memory, 5120 CUDA, 640 Tensor, 250 W   - 512 GB RAM, DDR4-3200 MHz REG, ECC, 2Rx4 (16 x 32 GB) - 2 TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC High Memory NodePenguin Computing Altus XE1212 - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225 W) - 3072 GB RAM, DDR4-2933 MHz LR, ECC, 4R (24 x 128 GB)  - 2 TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC"},{"location":"policies/buy_in/#cost-and-allocations","title":"Cost and Allocations","text":"Notice <ul> <li>With V100S GPU's no longer available the pricing will be different.</li> <li>The locked in pricing expired February 28, 2022.  </li> </ul> Option Number CPU Cores V100s GPU RAM (GB) Monthly High-priority Allocation Cost CPU-only Options 1A - One CPU node 96 512 70,080 $8,037.50  (expired) 1B - Two CPU nodes 192 512 140,160 $16,075.00  (expired) 1C - Three CPU nodes 288 512 210,240 $24,112.00  (expired) 1D - Full Altus XE2242 384 512 280,320 $32,150.00  (expired) GPU Node Options 2A - 1/4 Altus XE2214GT 24 1 512 17,520 $8,523.75  (expired) 2B - 2/4 Altus XE2214GT 48 2 512 35,040 $17,047.50  (expired) 2C - 3/4 Altus XE2214GT 72 3 512 52,560 $25,571.25  (expired) 2D - Full Altus XE2214GT 96 4 512 70,080 $34,095.00  (expired) High Memory Node 3 - Full Altus XE1212 96 3072 70,080 $42,230.00  (expired)"},{"location":"policies/committees/","title":"Committees","text":"<p>The Research Computing Guidance Committee (RCGC) is a cross-departmental group of researchers and IT professionals at the University of Arizona with oversight of central research computing resources.</p> <p>The charge to the committee is to design and implement the policies and procedures; the oversight of the operations; and promotion and recommendations for the centrally funded and administered research computing resources. The policies and procedures will be monitored and updated by this steering committee and related task forces created by this committee. The resources will be administered, maintained, and supported by UITS Research Computing, Systems, and Operations.</p>"},{"location":"policies/loss_of_university_affiliation/","title":"Loss of University Affiliation","text":""},{"location":"policies/loss_of_university_affiliation/#affiliation-loss-policy","title":"Affiliation Loss Policy","text":"<p>Losing affiliation with the university (e.g. via graduating, leaving a work position, etc) will result in the denial of access to HPC resources. This will happen automatically on the day of termination according to the University of Arizona Records Database. </p> <p>We unfortunately cannot extend HPC access prior to or after affiliation loss, however, if you require continued access to HPC services, you may register as a Designated Campus Colleague (DCC) through Human Resources. Once your DCC status is approved, you may request sponsorship from a university faculty member. </p>"},{"location":"policies/loss_of_university_affiliation/#data-after-affiliation-loss","title":"Data After Affiliation Loss","text":"<p>Home directory data are maintained for 90 days following affiliation loss. If university affiliation is reestablished and HPC access is restored within those 90 days, when you log into HPC again you will have access to your files. Data in shared storage (e.g. <code>/groups</code> or <code>/xdisk</code>) persist for as long as the storage location is available and are not deleted after a user loses access to the system.</p>"},{"location":"policies/maintenance/","title":"Maintenance","text":""},{"location":"policies/maintenance/#planned-maintenance","title":"Planned Maintenance","text":"<p>Most maintenance is performed during regular hours with no interruption to service.  System wide maintenance is usually planned ahead of time and is scheduled for Wednesdays from 8:00 AM to 5:00 PM with at least 10 days notice.  These will be planned to occur four times per year.</p> <p>Maintenance windows represent periods when UITS may choose to drain the queues of running jobs and suspend access to the cluster operation for HPC maintenance purposes.</p> <p>The notification will describe the nature and extent (partial or full) of the interruptions of HPC services. </p>"},{"location":"policies/maintenance/#system-wide-maintenance","title":"System-wide Maintenance","text":"<p>Some maintenance cycles require the entire system to be taken offline. In preparation, batch queues will be modified prior to scheduled downtimes to hold jobs which request more wallclock time than remains before the shutdown. Held jobs will be released to run once maintenance concludes.</p>"},{"location":"policies/maintenance/#rolling-maintenance","title":"Rolling maintenance","text":"<p>Rolling maintenance cycles facilitate updates or maintenance tasks without requiring a complete system shutdown. Nodes are sequentially taken offline, allowing running jobs to complete before they are updated and brought back online. This rolling process ensures minimal disruption to ongoing computational tasks while the maintenance is carried out. During rolling maintenance cycles, the job queues may be slower than average while nodes are pending a reboot.</p>"},{"location":"policies/maintenance/#emergency-maintenance","title":"Emergency Maintenance","text":"<p>Unavoidable (emergency) downtime may occur as a result of any of the above reasons at almost any time. Such events are rare and great effort is made to avoid these situations. However, when emergency maintenance is needed, the UITS unit responsible for the item affected will provide as much notice to users as possible and work to resolve the fault as quickly as possible.</p> <p>Any emergency outages will be announced via email through the hpc-announce@list.arizona.edu mailing list. </p>"},{"location":"policies/special_projects/","title":"Special Projects","text":""},{"location":"policies/special_projects/#overview","title":"Overview","text":"<p>The RCGC (Research Computing Governance Committee) has approved support for \"Special Projects\" that use more than the standard allocation of hours. When a special project request is granted, an additional allocation of standard hours is provided for a limited period of time.   </p> <p>There is not a specific definition of what comprises a project, but it is often support for publication or grant deadlines, or graduation dates. Each request is considered on a case by case basis.</p>"},{"location":"policies/special_projects/#authorized-requestors","title":"Authorized requestors","text":"<p>Project requests must be submitted by a PI; partly because the additional time granted will go to the allocation of the PI.</p>"},{"location":"policies/special_projects/#guidelines","title":"Guidelines","text":"<ul> <li>All special project allocations are temporary</li> <li>Special project allocations cannot be granted if they will impact the community of users with a standard allocation</li> <li>PIs can only be granted a special project once within a 12 month period (starting from the ending period of the last special project)<ul> <li>PI groups that routinely need more standard computing hours should supplement their needs in other ways (e.g. buy-in, accessing national/international computing resources, etc)</li> <li>The Research Computing group can help UArizona PIs access national-scale computing centers (e.g. ACCESS, TACC, Open Science Grid, etc)</li> </ul> </li> </ul>"},{"location":"policies/special_projects/#categories","title":"Categories","text":"Size Requirements 10,000 hours per month or less The majority of requests fall into this category. These requests provide a general statement of the need and are one month or less in duration. These requests are occasional and they can be granted automatically by UITS staff with minimal impact to standard queue usage. This will be done at the discretion of the Research Technologies staff. 10,000 to 100,000 hours per month These requests must provide a defined number of hours and not to exceed 3 months. A defined statement of need (e.g. publication deadline) will be provided. These requests are occasional, and can be granted by Research Technologies staff after considering the researcher\u2019s need, alternative ways to solve those needs, and assessing that there is no overall impact to the system. Greater than 100,000 hours per month These requests must provide a defined number of hours, defined duration not to exceed 6 months, and a defined statement of need (e.g. publication deadline). These requests required PIs to report back the results of their calculations (publications, conference presentations, etc) for potential inclusion in our online documentation. Benchmarking, profiling, or assessment of analyses run should also be provided to help the UArizona HPC understand how the additional computing time was used and ways that need can be met in the future without a special project. These requests will be forwarded to the HPC policies subcommittee of RCGC and require committee approval before being granted."},{"location":"policies/special_projects/#submitting-a-request","title":"Submitting a Request","text":"<p>Requests are submitted via a web form in the HPC user portal under Support Requests.  Please include:</p> <ul> <li>Number of additional standard hours needed for the special project</li> <li>Duration of the Special Project in months</li> <li>Reason for the temporary increase</li> </ul>"},{"location":"quick_start/accessing_compute/","title":"Accessing Compute Nodes","text":""},{"location":"quick_start/accessing_compute/#accessing-compute-nodes","title":"Accessing Compute Nodes","text":""},{"location":"quick_start/accessing_compute/#the-compute-nodes","title":"The Compute Nodes","text":"<p>Unlike the bastion host and login nodes, there are many compute nodes and each has, as the name suggests, a large amount of computational resources available to run your work. For example, Puma standard nodes have 94 available CPUs and a whopping 470 GB of RAM. </p> <p></p> <p>To get a sense of what the cluster looks like, try running the command <code>nodes-busy</code>. The output should look something like this:</p> <pre><code>\u271a    Buy-in nodes. Only accept high_priority and windfall jobs\n(puma) [netid@wentletrap ~]$ nodes-busy \n==============================================================\n\n                      \u2592 System Status \u2592\n              Wed Feb 14, 03:42:09 PM (MST) 2024\n\nStandard Nodes\n==============================================================\nr1u16n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u17n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u18n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u25n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u26n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u26n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u27n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \n</code></pre> <p>Each line shows one compute node on the cluster you're connected to and how busy it currently is running jobs. By default, when you first log in you're connected to the Puma cluster. This is the largest and newest and generally provides the most in terms of computational resources. However, we have two other clusters available: Ocelote and ElGato, each with a good number of computational resources available and shorter wait times to access them. </p> <p>When you first connected to a login node in the previous section, your terminal should have displayed:</p> <pre><code> ***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n</code></pre> <p>This shows you the various shortcuts you can use to connect to the different clusters. Try running the command <code>elgato</code> now. You should see a change in your terminal prompt to indicate that your cluster has changed. </p> <pre><code>(puma) [user@wentletrap ~]$ elgato\n(elgato) [user@wentletrap ~]$ \n</code></pre>"},{"location":"quick_start/accessing_compute/#job-charging","title":"Job Charging","text":"<p>Before we connect to a compute node, let's quickly cover how access is charged. </p> <p>Every HPC group gets a free allocation of CPU hours that they can spend every month to access compute resources. You can think of a CPU hour as a token to buy one CPU for one hour, so if you want to reserve 5 CPUs for 10 hours, this will charge 50 CPU hours to your account. You can see more detailed information on job queues, allocations, and job charging on our Time Allocations page which has a comprehensive breakdown. </p> <p>For this tutorial, we'll focus on the standard partition. This is a job queue and is the one that consumes your standard allocation. To use this job queue, you'll need to know your account name. To check, use the command <code>va</code> which stands for \"view allocation\". The output will look something like:</p> <pre><code>(elgato) [user@gpu5 ~]$ va\nWindfall: Unlimited\n\nPI: parent_974 Total time: 7000:00:00\n    Total used*: 1306:39:00\n    Total encumbered: 92:49:00\n    Total remaining: 5600:32:00\n    Group: group_name Time used: 862:08:00 Time encumbered: 92:49:00\n\n*Usage includes all subgroups, some of which may not be displayed here\n</code></pre> <p>You should see a name next to the <code>Group</code> field (in the example above, this is <code>group_name</code>). If you see multiple groups, then you are sponsored in multiple groups and can choose any one of your group names. Note the name of your account and hang onto it for the upcoming sections.</p>"},{"location":"quick_start/accessing_compute/#interactive-jobs","title":"Interactive Jobs","text":"<p>Now, let's actually access a compute node. When you're connected to a login node, you can initiate a Slurm job to interactively connect to a compute node by using the command <code>interactive</code>. By default, this will give you one CPU for one hour. You can adjust this using the different flags available which are documented on our Interactive Jobs page. For now, we'll stick with the default resources. </p> <p>To access a session, run the following, substituting your own group name (that you found with <code>va</code> in the section above) for <code>&lt;group_name&gt;</code>: <pre><code>interactive -a &lt;group_name&gt;\n</code></pre></p> <p>For example, in my case: <pre><code>(elgato) [netid@wentletrap ~]$ interactive -a hpcteam\nRun \"interactive -h for help customizing interactive use\"\nSubmitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=1 --time=01:00:00 --account=hpcteam --partition=standard\nsalloc: Granted job allocation 1800857\nsalloc: Nodes cpu39 are ready for job\n[netid@cpu39 ~]$ hostname\ncpu39.elgato.hpc.arizona.edu\n</code></pre></p> <p>You'll notice once your job starts that your command line prompt changes to display the name of the compute node. If you run <code>hostname</code>, this should match your command line prompt and show you the name of the compute node you're connected to. In my case, I'm connected to the ElGato compute node <code>cpu39</code>.</p> <p>You'll also notice that your session has been assigned a job number (in the above, you can see this as <code>Granted job allocation 1800857</code>). A job number is assigned to every job on the system and is used to keep track of job statistics and metrics. </p> <p>Since you're in an interactive session, you now have exclusive access to the resources you've reserved which means you can do things like use software to develop, test, run, and debug your code. Interactive sessions are optimal for these sorts of actions. However, you might run into problems executing your analyses if:</p> <ul> <li>Your session times out due to inactivity</li> <li>Your internet connection gets disrupted</li> <li>Your computer gets closed or turned off</li> <li>You want to run more than one job at a time</li> </ul> <p>That's where batch jobs come in. </p>"},{"location":"quick_start/accessing_compute/#batch-jobs","title":"Batch Jobs","text":"<p>Batch jobs are the real workhorses of HPC. In contrast to interactive jobs, batch jobs are a way of submitting work to run on a compute node without the need for an active connection. Batch scripts are text files that act as blueprints that the scheduler uses to allocate resources and run the terminal commands needed to run your analysis. </p> <p>Batch jobs are initiated by submitting a batch script using the command <code>sbatch myscript.slurm</code>. The instructions are then sent to the scheduler, which finds CPUs for your job, and puts it in the queue to wait until they become available. The wait time depends on many factors, including the scale of your resource request and the overall system usage. To check on jobs you have submitted, use the command <code>squeue --user=&lt;your_netid&gt;</code>. </p> <p>After the scheduler has received your request, the rest happens automatically. This means you can close your SSH connection, or even turn off your personal computer and walk away without interrupting your jobs. This workflow enables you to submit tens, hundreds, or even thousands of jobs to run simultaneously, dramatically increasing your productivity over what is possible with a local workstation.</p> <p>Constructing batch scripts involves specific syntax and parameters that are too detailed to cover in this Quick Start. Instead, see Intro to Batch Jobs for details. </p>"},{"location":"quick_start/common_misconceptions/","title":"Common Misconceptions","text":""},{"location":"quick_start/common_misconceptions/#common-misconceptions","title":"Common Misconceptions","text":"<p>Both experienced and novice users may benefit from reading through these common misconceptions. </p>"},{"location":"quick_start/common_misconceptions/#if-i-move-my-code-to-hpc-it-will-automatically-run-faster","title":"If I move my code to HPC, it will automatically run faster","text":"<p>You might be surprised to learn that if you move code from a local computer to a supercomputer, it will not automatically run faster and may even run slower. This is because the power of a supercomputer comes from the volume of resources available (compute nodes, CPUs, GPUs, etc.) and not the clockspeed of the processors themselves. Performance boosts come from optimizing your code to make use of the additional processors available on HPC, a practice known as parallelization.</p> <p>Parallelization enables jobs to 'divide-and-conquer' independent tasks within a process when multiple threads are available. In practice, this typically means running a job with multiple CPUs on the HPC. On your local machine, running apps like your web browser is natively parallelized, meaning you don't have to worry about having so many tabs open. However, on the HPC, parallelization must almost always be explicitly configured and called from your job. This process is highly software-dependent, so please research the proper method for running your program of choice in parallel. </p>"},{"location":"quick_start/common_misconceptions/#if-i-allocate-more-cpus-to-my-job-my-software-will-use-them","title":"If I allocate more CPUs to my job, my software will use them","text":"<p>Running a job with a large number of CPUs when the software has not been configured to use them is a waste of your allocation, your time, and community resources. Software needs to be designed to use multiple CPUs as part of its execution. You will need to ensure your software has the capability to make use of multiple CPUs for it to be able to take advantage of additional hardware. The job scheduler only provides the resources, the code itself is what needs to know how to make use of them.</p>"},{"location":"quick_start/common_misconceptions/#all-nodes-on-a-supercomputer-are-the-same","title":"All nodes on a supercomputer are the same","text":"<p>Navigating the HPC means being aware of the different types of nodes you can land on. For example, the login node is available to all users by default upon login, and is designed for managing and editing files. However, it is not designed to run production computations. Running jobs that are too computationally intensive on the login node can severely impact performance for other users. Such jobs will be noticed and stopped by the HPC systems team.</p> <p>Types of nodes on the UArizona HPC system include the Bastion Host, the Login Node, the Compute Nodes, and the Data Transfer Node. See Compute Resources for information on the compute hardware available.</p>"},{"location":"quick_start/common_misconceptions/#as-a-user-i-amam-not-allowed-to-install-my-own-software","title":"As a user I (am)(am not) allowed to install my own software","text":"<p>Well, it depends. Users can create custom environments and install packages for languages like Python and R by using their built-in package managers. Users are even encouraged to download software from GitHub or other repositories and compile it themselves (provided it is done on a compute node). However, system-wide modules are generally taken care of by the HPC team. If you would like something to be installed as software available to all HPC users, you can make a request through ServiceNow. But, if you would like something to be installed for personal use, or use between members of your group, you are encouraged to install it in one of your shares on the HPC filesystem. </p>"},{"location":"quick_start/logging_in/","title":"Logging In","text":""},{"location":"quick_start/logging_in/#logging-in-and-system-layout","title":"Logging In and System Layout","text":"Account creation is necessary to log in <p>If you have not yet done so, you will need to register for an account to log in. See our registration documentation for steps. </p>"},{"location":"quick_start/logging_in/#system-access","title":"System Access","text":"<p>Tip</p> <p>If you experience any issues during the login process, see our FAQs for common problems.</p> <p>Once you've succesfully registered for an HPC account, you're ready to log in. There are two main methods to access the HPC system</p> <ol> <li> <p>Open OnDemand</p> <p>This is a browser-based application that provides users with a graphical interface to access the HPC filesystem, run software that requires a graphical component, or access an interactive desktop environment. The login portal for Open OnDemand uses the familiar UArizona WebAuth login screen. HPC accounts are tied to university accounts, so use your standard NetID and password (i.e. the one used for your email).</p> </li> <li> <p>Terminal</p> <p>The terminal is a text-based command interpreter provided by the operating system on your local machine. Mac and Linux users can access the \"Secure SHell\" (SSH) command by default, and Windows users will either have to use the Linux subsystem for Windows, or a program called PuTTY. Using these tools, users can access a command-line environment on the HPC, which can be used to manage files, write code, install software, and submit jobs. See our Bash Cheat Sheet for an overview of common commands.</p> </li> </ol>"},{"location":"quick_start/logging_in/#system-layout","title":"System Layout","text":"<p>The inner workings of HPC systems may be somewhat obscured to new users. In this section, we'll give you an idea of how the system is laid out so you understand exactly where you are at each stage of the login process and what activities are performed where. </p>"},{"location":"quick_start/logging_in/#the-bastion-host","title":"The bastion host","text":"<p>In another browser window, open our instructions on logging in from the command line. Start by following the first step shown that's specific to your operating system. Stop when your terminal displays </p> <p><pre><code>Success. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n</code></pre> If all has gone well, you are now connected to what is known as the bastion host. </p> <p></p> <p>The bastion host is the first computer you land on when you log in using the hostname <code>hpc.arizona.edu</code>. This machine is only used to validate your credentials and provide a gateway to the rest of the HPC environment. It is not used for storing files and has no software installed so no computational work is done at this stage. As a test, try running the command <code>hostname</code>:</p> <pre><code>[user@gatekeeper 14:50:49 ~]$ hostname\ngatekeeper.hpc.arizona.edu\n</code></pre> <p>The output shows <code>gatekeeper</code> which is the name of this node and is how you can tell you're connected to the bastion. </p> <p>Next, to advance from the bastion host, type the command <code>shell</code>.</p>"},{"location":"quick_start/logging_in/#the-login-nodes","title":"The login nodes","text":"<p>After you type <code>shell</code> on the bastion host, you're connected to a computer called a login node. </p> <p></p> <p>We have two of these available and you will be assigned one at random. If you run the <code>hostname</code> command as you did on the bastion host, you should see either <code>wentletrap</code> or <code>junonia</code>. </p> <p>A login node is a shared workspace with minimal computational capabilities and very little software installed. This is not the place where computational work is done so users should not run their analyses, compile their software, or perform computationally intensive work in this location. Instead, the login nodes are meant for activities such as managing files, writing scripts, submitting and monitoring jobs, and viewing system resources.</p>"},{"location":"quick_start/overview/","title":"UArizona HPC Quick Start Guide","text":""},{"location":"quick_start/overview/#overview","title":"Overview","text":"<p>Just getting started with our HPC and don't know where to start? You've found the place!</p> <p>If you're using an HPC system for the first time, this tutorial is designed to give you the basic tools to get you started. </p> <p>If you're experienced with HPC already but haven't used our system, this guide is intended to help familiarize you with the specifics of our HPC clusters. This should help you get up and running quickly since every HPC system is slightly different; each with its own specific commands, allocation policy, partitions, accounts, and even system layout. </p>"},{"location":"quick_start/overview/#getting-started-checklist","title":"Getting Started Checklist","text":"<p>Before beginning this tutorial, you'll want to ensure that you're familiarized with our system policies and have registered for an account. Take a moment to review the following pages before proceeding with the rest of the tutorial.</p> <ul> <li> Policies: Read up on our HPC guidelines. In particular:<ul> <li> Acceptable use</li> <li> Acknowledgements</li> <li> What happens if you lose university affiliation</li> </ul> </li> <li> Register for an account: To log into HPC, you'll need to have registered for an account. If you have not yet done so, see our registration documentation for steps. Note that the process varies based on your university affiliation. </li> </ul>"},{"location":"quick_start/overview/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this quick start, you should know:</p> <ul> <li> What HPC is</li> <li> How the UA HPC is structured</li> <li> How to log in</li> <li> What storage you have access to</li> <li> What time allocations are</li> <li> How to access computational resources</li> <li> How to access software</li> </ul> <p></p>"},{"location":"quick_start/software/","title":"Software","text":""},{"location":"quick_start/software/#software-on-hpc","title":"Software on HPC","text":""},{"location":"quick_start/software/#accessing-software","title":"Accessing Software","text":"<p>It's hard to perform analyses without software. We learned that software packages are not available on the login nodes, but now that we're connected to a compute node, we can see and use what's available. </p> <p>Software on HPC comes installed as modules. Modules make it easy to load and unload software from your environment. This allows hundreds of packages to be available on the same system without dependency or versioning conflicts. It's always good practice to specify which version of the software you need when loading to ensure a stable environment.</p> <p>You can view and load software modules using the commands <code>module avail</code> and <code>module load</code>, respectively. For example:</p> <pre><code>[netid@gpu66 ~]$ module avail python\n\n------------------------------------------------- /opt/ohpc/pub/modulefiles --------------------------------------------------\n   python/3.6/3.6.5    python/3.8/3.8.2 (D)    python/3.8/3.8.12    python/3.9/3.9.10\n\n[netid@gpu66 ~]$ module load python/3.9\n[netid@gpu66 ~]$ python3 --version\nPython 3.9.10\n</code></pre> <p>Try running <code>module avail</code> without specifying any arguments. You'll notice we have a lot available.</p>"},{"location":"quick_start/storage_and_transfers/","title":"Storage and Transfers","text":""},{"location":"quick_start/storage_and_transfers/#storage-and-transfers","title":"Storage and Transfers","text":"<p>When you first access a login node, you're located in your home directory. This is a space with a 50 GB limit and is accessible to only you. The files you store here are housed on a large storage array and are accessible anywhere you are on the system except the bastion host. </p> <p>Do not overfill your home directory</p> <p>Once your data reaches the 50 GB limit you will experience problems, like the inability to log into Open OnDemand because the session file cannot be created. And your jobs may fail with unexpected errors depending on how your software reacts to no more space for writing output. </p> <p></p> <p>To store your files in your home, you will need to transfer them to the system. Small files can most easily be transferred to/from HPC using our web interface Open OnDemand. In the upper-left you'll see a dropdown called Files where you can select Home Directory. </p> <p></p> <p>On the following page, select \"Upload\" to open a window where you can drag/drop files.</p> <p></p> <p>For larger files, we have a designated Data Transfer Node (DTN). Comprehensive instructions for alternative methods for file transfers can be found on our data transfer page. </p> <p>With larger files comes the need for more storage. If you find your home is insufficient to store your data, group allocations are available. See our storage documentation for details on options that are available. </p> <p>Now that we're on the login nodes and know where our files are, it's time to access a compute node. </p>"},{"location":"quick_start/summary/","title":"Summary","text":""},{"location":"quick_start/summary/#summary","title":"Summary","text":"<p>That's it! You now should have a sense of how UArizona's HPC systems are laid out, how to log in, and how to access compute nodes and software.</p> <p>To continue learning about HPC, our online documentation has a lot more information that can help get you started. For example FAQs, information on HPC storage, and file transfers. </p> <p>Other great resources include: virtual office hours every Wednesday from 2:00-4:00pm, consultation services offered through ServiceNow, a Github page with example jobs, and a YouTube channel with training videos. </p>"},{"location":"quick_start/summary/#external-resources","title":"External Resources","text":"<p>While this documentation covers some concepts and techniques that are widely applicable, it's beyond our scope to cover everything you may need or want to know while using the HPC. We highly encourage users to supplement their learning with resources from around the web! Below are some recommended resources to get started. </p> Bash &amp; Linux reference External Documentation Misc HPC Tutorials Bash Manual Slurm Intro to HPC Bash scripting beginner tutorial HPC Wiki Lawrence Livermore HPC Tutorials Intro to Bash scripting e-book Software Carpentry General purpose linux tutorials Cornell HPC Roadmaps <p>Please note that information related to other HPC clusters may not necessarily apply to the UA HPC cluster. Each system is configured differently, and what may be supported on another system may not be supported here, or vice versa. Please refer to this documentation site for information specific to the UA HPC, or ask HPC Consult if you have any questions.</p> <p></p>"},{"location":"quick_start/supercomputing_in_plain_english/","title":"Supercomputing in Plain English","text":""},{"location":"quick_start/supercomputing_in_plain_english/#supercomputing-in-plain-english","title":"Supercomputing in Plain English","text":""},{"location":"quick_start/supercomputing_in_plain_english/#what-is-hpcwhat-is-a-supercomputer","title":"What is HPC/What is a Supercomputer?","text":"<p>If you've never used an HPC system before, you may be wondering what one is and why you'd want to use it. HPC stands for High Perfomance Computing and is synonymous with Supercomputer. A supercomputer is a collection, or cluster, of a large number of regular computers (referred to as compute nodes) connected over a network. Each of the computers is like a local workstation though typically much more capable. For example, a standard laptop might have 4 CPUs and 8 GB of RAM. Compare this with a standard compute node on Puma which has 94 CPUs and 470 GB of RAM. </p> <p>Another main difference between a supercomputer and a personal workstation is that the supercomputer is a shared resource. This means there may be hundreds or even thousands of simultaneous users. Without some sort of coordination, managing which users get which resources turns into a major logistical challenge. That's why supercomputers use job schedulers, like Slurm. </p> <p>A job scheduler is software used to coordinate user jobs. You can use it by writing a 'batch' script that requests compute resources (e.g., CPUs, RAM, GPUs) and includes instructions for running your code. You submit this script to the job scheduler using the <code>sbatch</code> command, which finds available resources on the supercomputer for your job. When the resources become available, it initiates the commands included in your batch script, and outputs the results to a text file. More information on running batch jobs can be found in the Running Jobs section.</p>"},{"location":"quick_start/supercomputing_in_plain_english/#why-should-i-use-a-supercomputer","title":"Why Should I Use a Supercomputer?","text":"<p>Supercomputers provide opportunities for data storage and parallel processing that far surpass what is capable in a standard workstation. These systems provide researchers with the ability to scale up or scale out their work.</p> <p>Increasing the data throughput of a single job is known as scaling up. This may mean moving from a 500 GB database on a workstation to a 5 TB database on the HPC, or raising the resolution of your simulation by a factor of 10 or 100. </p> <p>Other types of analysis may benefit from an increased number of jobs, such performing parameter sweeps, running Monte Carlo simulations, or performing molecular dynamics simulations to study the behavior of complex biological systems, such as protein folding or drug interactions. Local machines are limited by the number of cores accessible to them, decreasing the number of simultaneous computations as compared to an HPC. An increase in the number of CPUs used during analysis is known as scaling out your work.</p> <p>Automation is another feature of HPC systems that allows users to schedule jobs ahead of time, and for those jobs to be run without supervision. Managing a workstation or keeping an SSH terminal active while scripts are running can lead to major complications when running extended analyses. Batch scripts allow a prewritten set of instructions to be executed when the scheduler determines that sufficient resources will be available. This allows for jobs with extended completion times to be run for up to 10 days (a limit imposed by the scheduler). Real-time output is saved to a text file, allowing you to check the progress of the job. Checkpointing is recommended for jobs that require longer than 10 days.</p>"},{"location":"registration_and_access/account_creation/","title":"Account Creation","text":""},{"location":"registration_and_access/account_creation/#overview","title":"Overview","text":"<p>All UArizona Faculty, Staff, Students, and Affiliates are eligible for HPC accounts free of cost.</p> <p>If you are considered to be a PI in the University's employee system (EDS) you may establish a PI account. PI Accounts are self-sponsored, manage their own HPC group(s), and receive their own time and storage allocations to be shared among group members. Staff, students and affiliates must be sponsored by a PI at the PI's discretion to obtain HPC access. Users may be a member of more than one HPC group.</p>"},{"location":"registration_and_access/account_creation/#how-to-register","title":"How to Register","text":"<p>The process of registering for an HPC account varies depending on your affiliation with the university. Take a look at the list below to determine how you should register:</p> I'm a faculty member/principal investigator (PI) <p>If you are a research faculty member or principal investigator, you can sponsor yourself for access through our user portal.       Step 1: Visit https://portal.hpc.arizona.edu/. This will automatically create an HPC account for you.       Step 2: Go to https://portal.hpc.arizona.edu/portal/sendlink.php and click the link on the left-hand side as shown below             This will automatically redirect you back to the user portal, create a research group for you, and add you as a member. You will briefly see a pop-up notification verifying you've been added to your group. You can check this by going to the Manage Groups tab and clicking your group's dropdown menu to view its members.        </p> I'm a student, postdoc, staff member, or Designated Campus Colleague <p>      If you are affiliated with the University of Arizona but are not faculty, you will need to request sponsorship from a faculty member. This can be done through our web portal.       Step 1: Create an HPC account by navigating to https://portal.hpc.arizona.edu/.      Step 2: Request sponsorship from a UArizona faculty member. Note: Your faculty sponsor will need their own HPC account before they are able to sponsor others.           To request sponsorship, navigate to https://portal.hpc.arizona.edu/portal/sendlink.php. On the right-hand side, enter your sponsor's email address and click send.             Your sponsor will then receive an email with a link used to authorize your account. Once they confirm your request, you will receive an email with instructions for accessing the HPC systems.          Note: it may take up to 15 minutes after approval to receive a confirmation email and for your account to officially be activated.          If you do not receive an email verification, you should contact your sponsor and confirm receipt and approval of the HPC account request. If your account has been approved but you have not received verification, you should contact HPC consulting and provide your NetID, your name, and the email address of your sponsor.      </p> I'm not affiliated with the university <p>     HPC systems are restricted to those with valid university credentials and are not available for general public use. However, if you are not officially affiliated with the university but are actively collaborating with university members, you may register for Designated Campus Colleague (DCC) status. This is done through human resources and provides collaborators with active UArizona credentials. Once your DCC status is approved, you may request sponsorship from a university faculty member (see the section above).     </p>"},{"location":"registration_and_access/account_deletion/","title":"Account Deletion","text":""},{"location":"registration_and_access/account_deletion/#manual-deletion","title":"Manual Deletion","text":"<p>If you wish to delete your HPC account, you may do so through the User Portal. Navigate to the Support tab and click the Close Your HPC Account link. You will be prompted to manually confirm by entering confirm at the prompt. Click Close Account to complete the process.</p> <p></p>"},{"location":"registration_and_access/account_deletion/#loss-of-university-affiliation","title":"Loss of University Affiliation","text":"<p>Losing affiliation with the university will result in the denial of access to HPC resources. This will happen automatically on the day of termination according to the University of Arizona Records Database. Data may be retrievable if a student or employee is reinstated, or by the PI. Please contact us for support in this case. </p> <p>If you are losing affiliation and require continued access to HPC services, you may register as a Designated Campus Colleague (DCC) through Human Resources. Once your DCC status is approved, you may request sponsorship from a university faculty member. </p>"},{"location":"registration_and_access/group_management/","title":"Group Management","text":""},{"location":"registration_and_access/group_management/#overview","title":"Overview","text":"<p>HPC groups allow faculty members to manage file permissions, job allocations, and group members. When a PI creates a new HPC account, a group is created with an allocation of space and time. Additional storage is available for free upon request, called xdisk</p> <p>There are two types of Groups: Research Groups and Class Groups.</p> <ul> <li>Research groups include any faculty, postdocs, graduate students, DCCs, staff, or student workers actively affiliated with your group's research. </li> <li>Class groups are for educational purposes only and will include students enrolled in a semester-long course.</li> </ul>"},{"location":"registration_and_access/group_management/#research-groups","title":"Research Groups","text":"<p>If you are a faculty member who has registered for an HPC account, a research group named after your UArizona NetID has been automatically created for you. This group has an allocation of CPU hours associated with it as well as communal storage for your data.</p> <p>Membership and Allocation</p> <p>Members of research groups have full access to the PI's allocation. PIs are able to create multiple research groups that pull from the same allocation. Users are able to be members of multiple research groups. Creating multiple research groups does not change the total CPU-time allocation.</p> <p>Permissions</p> <p>Research groups can also be used to manage access permissions to files and folders on HPC. The PI can use the <code>chgrp</code> command to change which group has access rights corresponding to the 'group' setting on a particular item. See our Linux File Permissions cheat sheet for more information.</p>"},{"location":"registration_and_access/group_management/#adding-members","title":"Adding Members","text":"<p>To add members to your research group, go to https://portal.hpc.arizona.edu/ and click the Manage Groups tab at the top of the screen. Click your group's dropdown tab and click Add Member</p> <p></p> <p>Enter the user's UArizona NetID in the box that appears, and select Add</p> <p></p> <p>To add members in bulk, you may also select Upload Member List and upload a CSV file of UArizona NetIDs.</p> <p>The process of adding new members may take a few seconds to complete. Once the changes have taken place, you will see the user's NetID in your group:</p> <p></p>"},{"location":"registration_and_access/group_management/#creating-a-new-group","title":"Creating a New Group","text":"<p>A new group can be created at any time through the user portal. New groups will share their time and storage allocations with your primary group. Alternate research groups can be a good solution for managing file permissions. For example, if a particular directory and its contents needs restricted access, you could do this by creating a new research group, adding the group members who need access to those files, and then changing the group ownership of the files/directories.</p> <p>To create a new group, log into the user portal, select the Manage Groups and select the Add New Group dropdown menu. </p> <p></p> <p>Once your group has been created, you will see it when running <code>va</code> (short for View Allocation) in the same block as your primary group:</p> <pre><code>(puma) [faculty-netid@junonia ~]$ va\nPI: parent_1206 Total time: 7000:00:00\n    Group: faculty-netid Time used: 0:00:00 Time encumbered: 0:00:00\n    Group: your-new-group Time used: 0:00:00 Time encumbered: 0:00:00\n    Total used: 0:00:00\n    Total encumbered: 0:00:00\n    Total remaining: 7000:00:00\n</code></pre>"},{"location":"registration_and_access/group_management/#class-groups","title":"Class Groups","text":"<p>Tip</p> <p>If you are interested in having an HPC staff member come to your class to do an Intro to HPC presentation, reach out to our consultants.</p> <p>If you are a faculty member and are teaching a course that makes use of HPC resources, you can create a class group that will grant your students system access. Class groups are designed to be created and used for one semester only.</p>"},{"location":"registration_and_access/group_management/#creating-a-class-group","title":"Creating a Class Group","text":"<p>Log into your user portal, navigate to the Manage Groups tab, and select the Add New Group dropdown option at the top of the page.There will be an option to specify your Group Type on the right. Choose Class from the dropdown menu</p> <p></p> <p>Under Group Name, enter something descriptive and then complete the process by clicking Add Group.</p> <p></p> <p>Once this process is complete, you can find your group's dropdown tab under Manage Groups. There you can add students either individually or in batch by uploading a CSV file with your student's NetIDs. You may also remove students from the group by clicking their NetIDs and then selecting Remove Member(s), or delete the group itself by selecting Delete Group.</p> <p></p>"},{"location":"registration_and_access/group_management/#file-permissions-and-storage","title":"File Permissions and Storage","text":"<p>Students in your class group will only be able to access files and directories owned by the class group. This means they will not be able to access files and directories owned by your standard research group. </p>"},{"location":"registration_and_access/group_management/#running-jobs-and-allocations","title":"Running Jobs and Allocations","text":"<p>Due to Arizona sales tax restrictions, class groups may only use the windfall queue on Puma. However, standard hours may be used by students on Ocelote. To submit standard jobs on Ocelote, students will use the class group's name for the <code>--account</code> SLURM directive. For example:</p> <p><pre><code>#SBATCH --account=hpc101\n#SBATCH --partition=standard\n</code></pre> Standard hours used on Ocelote are pulled from the same pool as your research group so make sure to plan accordingly. If you run the command <code>va</code>, you will see the class group as being nested under the total time allocated to your primary research group as well as any others you may have created. Students will not see the names of your other research groups if they run va unless they are members. </p> <pre><code>(ocelote) [faculty_netid@wentletrap ~]$ va\nWindfall: Unlimited\n\nPI: parent_000 Total time: 100000:00:00\n    Group: hpc101 Time used: 0:00:00 Time encumbered: 0:00:00\n    Group: faculty_netid Time used: 0:00:00 Time encumbered: 0:00:00\n    Total used: 0:00:00\n    Total encumbered: 0:00:00\n    Total remaining: 100000:00:00\n</code></pre>"},{"location":"registration_and_access/group_management/#delegating-group-management-rights","title":"Delegating Group Management Rights","text":"<p>PI's can delegate management rights to trusted group members. Delegates may create research and class groups, sponsor users, remove users, and request and manage storage offerings on behalf of their faculty sponsor. To add a group member as a delegate, the PI can click the Manage Delegates link on the home page of the user portal. In the Manage Delegates window that appears, select Add Delegate, enter your group member's NetID, and click Add.</p> <p></p> <p>Once a group member has been added as a delegate, they can log into the user portal, select Switch User, enter their PI's NetID in the pop-up field, and click Switch User. This will allow them to perform functions on their PI's behalf. They may switch back to their own account at any time by selecting Switch User and entering their own NetID.</p> <p></p>"},{"location":"registration_and_access/system_access/","title":"System Access","text":""},{"location":"registration_and_access/system_access/#overview","title":"Overview","text":"<p>Logging into the HPC supercomputers starts with your UArizona NetID and password with two-factor authentication enabled. This section is intended to provide you with instructions on getting terminal access to the system from your specific OS, how to log into the system from our web interface (Open OnDemand), how to set up X11 (image) forwarding, and how to configure your account to allow for a password-less login with SSH keys.</p> <p>If you experience any problems, refer to our FAQ page which provides some solutions to common problems.</p>"},{"location":"registration_and_access/system_access/#web-access","title":"Web Access","text":"<p>Open OnDemand</p> Browser TerminalVirtual Desktop <p>Users can gain command line access to HPC through our OOD web interface as an alternative to using a local SSH Client. To use this interface:</p> <ol> <li>Log into Open OnDemand</li> <li>Go to the dropdown menu at the top of the screen and select <code>Clusters</code></li> <li> <p>Click <code>&gt;_Shell Access</code></p> <p></p> </li> <li> <p>This will put you on the command line on one of the login nodes where you may perform regular housekeeping work, submit jobs, or request an interactive session. By default, you will automatically be connected to Puma. To navigate to a different cluster, use the displayed shortcuts. </p> </li> </ol> <p>Users may also interact with a cluster using a virtual desktop interface. To do this:</p> <ol> <li> <p>Log into Open OnDemand and, under My Interactive Sessions, select Interactive Desktop under Desktops on the left-hand side of the page.</p> </li> <li> <p>A form will appear where you will select the target cluster, enter the amount of time you'd like to be allotted (in hours), the number of cores you need, your PI Group (if you are unsure what your group name is, you can check in https://portal.hpc.arizona.edu/portal/), and the queue. Once you've filled in your request, click Launch.</p> <p></p> </li> <li> <p>A window will appear with the status of your request. It will start in a Pending state and will switch to Running when your desktop session is ready. Click Launch Interactive Desktop to access your session.</p> <p></p> </li> <li> <p>That's it! You can now use the cluster with a Desktop interface</p> <p></p> </li> </ol>"},{"location":"registration_and_access/system_access/#command-line-access","title":"Command Line Access","text":"Tip <ul> <li>Credentials: To log into HPC, you will need NetID+ enabled, an HPC account, and internet access. Because we require Duo-authentication to access the system, no VPN is required. </li> <li>Password Visibility: When entering your password in the terminal at the prompt, you will not see any characters appear on the screen while typing during this step. This is normal and everything is working as it should.</li> </ul> Linux/MacWindows Tip <p>Mac systems provide a built-in SSH client, so there is no need to install any additional software. You will find the terminal application under Applications \u2192 Utilities \u2192 Terminal.</p> <p>Open the terminal and enter: <pre><code>ssh netid@hpc.arizona.edu\n</code></pre> where netid is your UArizona NetID. When you press enter, you will be prompted for your university password. After successfully entering your password, you will be prompted to Duo Authenticate. If everything is successful, you will be connected to the bastion host.</p> <p>Windows systems do not have any built-in support for using SSH, so you will have to download a software package to do so. There are several available for Windows workstations.  Free SSH clients are available for download from the University of Arizona's Site License website.  </p> PuTTYMobaXterm <p>PuTTY is the most popular open source SSH Windows client. To use it: download, install, and open the Putty client. Next, open a connection and enter <code>hpc.arizona.edu</code> under Host Name and press Open</p> <p></p> <p>This will open a terminal. At the prompt, enter the following, replacing <code>&lt;netid&gt;</code> with your own NetID:</p> <pre><code>Login as: &lt;netid&gt;\n</code></pre> <p>You will then be prompted to Duo-Authenticate. If the process is successful, you will be connected to the bastion host.</p> <p>MobaXterm is another available SSH Windows client. To connect to HPC, download and install MobaXterm, open the software, select Session \u2192 SSH and enter <code>hpc.arizona.edu</code> under Remote host. Next, select the box next to Specify username and enter your UArizona NetID. To connect, click OK at the bottom of the screen:</p> <p></p> <p>This will open a terminal and will prompt you for your UArizona password. You will then need to Duo-authenticate. If everything is successful, you will be connected to the bastion host.</p> <p>Once you reach the bastion host, regardless of method, you should see the following: <pre><code>Success. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n</code></pre> From there, type <code>shell</code> to connect to the login nodes that will provide access to our three clusters. On the login nodes, you should see: <pre><code>***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n</code></pre></p>"},{"location":"registration_and_access/system_access/#x11-forwarding","title":"X11 Forwarding","text":"<p>X11 forwarding is a mechanism that allows a user to start up a remote application (e.g. VisIt or Matlab) and forward the application display to their local machine. The key to make forwarding work successfully is to include the <code>-X</code> flag at each login step. To check whether X11 forwarding is active, you may run the command:</p> <p><pre><code>echo $DISPLAY\n</code></pre> If it comes back blank, X11 forwarding is not enabled.</p> Mac/LinuxWindows Tips <ul> <li> <p>Mac users will want to install the additional software package XQuartz onto their machines to use X11 forwarding with HPC. </p> </li> <li> <p>On a Mac, if you get a blank response to <code>echo $DISPLAY</code>, you might need this line in your <code>~/.ssh/config</code> file: <code>ForwardX11Trusted yes</code></p> </li> <li> <p>Be aware forwarding X traffic does not work with the DEPRECATED menu interface enabled.  You should disable the menu option and use the hostname shortcuts instead.</p> </li> </ul> <p>Start a terminal session and connect as you typically would with an additional flag <code>-X</code> in your ssh command. Once you're connected to the bastion host, enter the name of the cluster you want to access, including the additional <code>-X</code> flag again. An example of this process is provided below: <pre><code>$ ssh -X netid@hpc.arizona.edu\nPassword:\nDuo two-factor login for netid\nEnter a passcode or select one of the following options:\n\n1. Duo Push to XXX-XXX-8969\n2. Phone call to XXX-XXX-8969\n3. Phone call to XXX-XXX-0502\n4. SMS passcodes to XXX-XXX-8969\n\nPasscode or option (1-4): 1\nSuccess. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n-----------------------------------------            \n[netid@gatekeeper ~]$ echo $DISPLAY\nlocalhost:13.0\n\n[netid@gatekeeper ~]$ shell -X\n***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nOcelote:\n$ ocelote\n(ocelote) $\nPuma:\n$ puma\n(puma) $\n\n(puma)[netid@junonia ~]$ echo $DISPLAY\nlocalhost:18.0\n</code></pre></p> <p>To use X11 forwarding on a Windows system, you will need to download an X11 display server such as Xming. </p> PuTTYMobaXterm <p>To enable X11 forwarding in PuTTY, go to SSH \u2192 X11 and select the box next to Enable X11 forwarding.</p> <p></p> <p>Once you've connected to the bastion host, connect to the login nodes with the an additional flag <code>-X</code>:</p> <pre><code>shell -X\n</code></pre> <p>To enable X11 forwarding in MobaXterm, open a new session, select SSH, and open Advanced SSH settings. Select the option below called X11-Forwarding.</p> <p></p> <p>Once you've connected to the bastion host, connect to the login nodes with the an additional flag <code>-X</code>: <pre><code>shell -X\n</code></pre></p>"},{"location":"registration_and_access/system_access/#ssh-keys","title":"SSH Keys","text":""},{"location":"registration_and_access/system_access/#why-use-ssh-keys","title":"Why Use SSH Keys?","text":"<p>The Bastion Host uses two-factor authentication and will, by default, prompt you for a password and 2nd factor when you attempt to log in. As an alternative, you can use PKI (Public Key Authentication). This means you will not have to provide a password or Duo-authenticate for any future sessions. To do this, you will need to create an SSH Key on your local workstation and copy the public key to the <code>~/.ssh/authorized_keys</code> file in your HPC account on the bastion host.</p>"},{"location":"registration_and_access/system_access/#create-and-use-ssh-keys","title":"Create and Use SSH Keys","text":"Linux/MacWindows <p>In a Terminal session on your local workstation:</p> <ol> <li> <p>Create a public-key pair:  <pre><code>ssh-keygen -t rsa\n</code></pre> You will be prompted to enter a passphrase. This is optional, but we strongly recommend that you do so.</p> </li> <li> <p>After running that command, you will have two new files on your local computer: <code>~/.ssh/id_rsa</code> and <code>~/.ssh/id_rsa.pub</code> <code>id_rsa</code> is your private key file. Do not share this with anybody! It is analagous to your password; anybody who has this file can impersonate you.     <code>id_rsa.pub</code> is your public key file. You will upload this onto any servers that you wish to automatically login to.</p> </li> <li> <p>Copy the public key to the Bastion Host (you will need to enter your password this one time):  <pre><code>ssh-copy-id netid@hpc.arizona.edu\n</code></pre> 3b. If your computer does not support the ssh-copy-id command, run the following commands: <pre><code>scp ~/.ssh/id_rsa.pub netid@hpc.arizona.edu:\nssh netid@hpc.arizona.edu # (you will need to use your password this time)\nmkdir -p ~/.ssh &amp;&amp; cat ~/id_rsa.pub &gt;&gt; .ssh/authorized_keys &amp;&amp; rm ~/id_rsa.pub # On the server, copies the key into the appropriate file\n</code></pre></p> </li> </ol> <p>Now, logout and attempt to login to the server again. You should not be prompted for a password!</p> <p>To setup SSH keys on Windows with the PuTTy client, refer to the official PuTTy documentation.</p>"},{"location":"registration_and_access/system_access/#using-ssh-keys-for-file-transfers","title":"Using SSH Keys for file transfers","text":"<p>Note that SSH Keys can also be used to avoid entering a password and 2nd factor when transferring files to to the cluster via the file transfer node (<code>filexfer.hpc.arizona.edu</code>) using command line programs like <code>scp</code> or <code>sftp</code>.  Follow the steps above (2-4 under the Mac and Linux instructions), except use <code>filexfer.hpc.arizona.edu</code> instead of <code>hpc.arizona.edu</code>. Note that you only need to generate the keys in Step 1 once. The same <code>~/.ssh/id_rsa.pub</code> file may be used to identify yourself to multiple hosts.</p> <p>If you would like to learn more about SSH keys and more, please refer to this in-depth guide created by our friends at Digital Ocean.</p>"},{"location":"registration_and_access/vpn/","title":"VPN","text":""},{"location":"registration_and_access/vpn/#overview","title":"Overview","text":"<p>A virtual private network (VPN) is a mechanism for creating a secure connection between a computer and a computing network using an insecure communication medium like the Internet. You can access the resources available within the network from your computer using a VPN.</p> <p>You will find the following VPN services useful for accessing some of the resources offered by the Research and Discovery Technologies:</p> <ul> <li>UA SSL VPN (<code>vpn.arizona.edu</code>): If you are not connected to the UA campus network you will need to connect to this VPN to access R-DAS.</li> <li>UA HPC VPN (<code>vpn.hpc.arizona.edu</code>): You will need to connect to this VPN to use graphical applications that need X11 forwarding with the HPC clusters.</li> </ul>"},{"location":"registration_and_access/vpn/#instructions-for-connecting","title":"Instructions for Connecting","text":"GUICLI <p>You can connect to the UArizona VPN services with the software Cisco Secure Client. It is available for Windows, Mac, and Linux distributions. On Linux distributions you might have a better experience with OpenConnect VPN (see CLI). Follow the UITS Knowledge Base guide for Windows, Mac, or Linux, to install Cisco Secure Client on your computer. The guide also shows how you can connect to the UA SSL VPN.</p> <p>Follow the steps below to connect to UA HPC VPN (the screenshots are from a Mac, but the experience is similar across OSs):</p> <ol> <li>Open Cisco Secure Client</li> <li>Enter <code>vpn.hpc.arizona.edu</code> in the address bar and click Connect.</li> <li>In the window that launches, enter your UArizona NetID as your Username and click OK.</li> <li>In the next window, enter your UArizona NetID password and click OK.</li> <li>In the window that launches, enter the NetID+ method you selected when you enrolled.</li> <li>Lastly, review the notice box and click Accept.</li> </ol> <p>Use of sudo</p> <p>Do not run any <code>sudo</code> commands on the HPC clusters when following the instructions below. These are strictly meant for your personal machines.</p> <p>You can connect to UArizona VPN services from the command line with OpenConnect VPN. To do this, you will need <code>sudo</code> privileges.</p> <p>OpenConnect VPN is available for Windows, Mac and Linux distributions, however installation on Windows can be difficult. On Windows, you might have a better experience with Cisco Secure Client (see GUI). You can find more information on platforms supported by OpenConnect from the project website. Select your operating system from the list below to view installation instructions:</p> MacLinux <p>Install with the Homebrew package manager: <code>brew install openconnect</code></p> <p>Follow the instructions from Open Build Service for your distribution.</p> <p>Once you have OpenConnect installed, you can connect to UArizona VPNs using the following:</p> UA SSL VPNUA HPC VPN <ol> <li>Open your terminal</li> <li>Enter <code>sudo openconnect vpn.arizona.edu</code></li> <li>A prompt will appear asking you to choose a VPN <code>GROUP</code>. Enter <code>1</code>.</li> <li>A prompt will appear asking you for your Username. Enter your UArizona NetID.</li> <li>A prompt will appear asking for your Password. Enter your UArizona NetID password. </li> <li>A second prompt will appear asking for your Password. Enter the NetID+ method you selected when you enrolled. <ol> <li>If you selected the Push method, then enter <code>push</code>.</li> <li>If you selected the SMS method, enter <code>sms</code>. If you do this, it will show that the login has failed and will ask you to reenter your Username, Password, and NetID+ method. For Username and Password do the same as before. For NetID+ method, enter the SMS passcode you received.</li> <li>If you selected the Passcode method, then enter your passcode. </li> </ol> </li> </ol> <ol> <li>Open your terminal</li> <li>Enter <code>sudo openconnect vpn.hpc.arizona.edu</code></li> <li>A prompt will appear asking you for your Username. Enter your UArizona NetID.</li> <li>A prompt will appear asking for your Password. Enter your UArizona NetID password. </li> <li>A second prompt will appear asking for your Password. Enter the NetID+ method you selected when you enrolled. <ol> <li>If you selected the Push method, then enter <code>push</code>.</li> <li>If you selected the SMS method, enter <code>sms</code>. If you do this, the prompt will appear again. Enter the SMS passcode that you received. </li> <li>If you selected the Passcode method, then enter your passcode.         </li> </ol> </li> </ol>"},{"location":"resources/allocations/","title":"Time Allocations","text":""},{"location":"resources/allocations/#group-allocations","title":"Group Allocations","text":"<p>All University of Arizona Principal Investigators (PIs; typically faculty) that register for access to UArizona High Performance Computing (HPC) services receive free standard allocations on the HPC machines which is shared among all members of their team and refreshed on a monthly basis. All PIs receive a standard allocation in addition to the windfall partition. A breakdown of the allocations available on the system and their usage is shown below. </p> <p>High Priority</p> <p>Please note that the High Priority partition is only available to PI groups who participated in the buy-in process for Puma. PIs will be notified when another buy-in session is available</p> <p>Qualified Hours</p> <p>Qualified Hours are only available to groups which have been awarded a special project. See Policies for information on how to apply. </p> StandardWindfallHigh PriorityQualified <p>Every group receives a free allocation of standard hours that refreshes on the first day of each month. </p> Puma Ocelote ElGato Standard CPU Hours 150,000 100,000 7,000 <p>In batch jobs, standard hours can be used with the directives</p> <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=standard\n</code></pre> <p>Windfall is a partition available to jobs that enables them to run without consuming your allocation, but it also reduces their priority. This means windfall jobs are slower to start than other partitions. In addition to lower priority, windfall jobs are preemptible, meaning standard and high-priority jobs can interrupt a running windfall job, effectively placing it back in the queue. The purpose of windfall is to ensure that the clusters are busy at all times, and to allow researchers additional compute while increasing the efficiency of the system.</p> <p>In batch jobs, the windfall partition can be used with the directive:</p> <pre><code>#SBATCH --partition=windfall\n</code></pre> <p>note that the <code>--account</code> flag is not used in this case.</p> <p>High priority allocations provide access to an additional pool of purchased compute nodes and increase the priority of jobs such that they start faster than standard jobs. Please check with your PI to ensure that your group has access before including these directives in your jobs.</p> <p>In batch jobs, standard hours can be used with the directives:</p> <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=high_priority\n#SBATCH --qos=user_qos_&lt;PI GROUP&gt;\n</code></pre> <p>Groups with an upcoming deadline (e.g., conference, paper submission, graduation) are eligible to apply for a Special Project allocation once per year. Special projects provide an additional pool of standard hours, known as \"qualified hours\" to the group for a limited amount of time. See the linked page for more information.</p> <p>In batch jobs, standard hours can be used with the directive: <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=standard\n#SBATCH --qos=qual_qos_&lt;PI GROUP&gt;\n</code></pre></p> <p>See: interactive jobs, batch jobs, or Open OnDemand for more information on the specific syntax for using hours in different jobs.</p>"},{"location":"resources/allocations/#how-allocations-are-charged","title":"How Allocations are Charged","text":"<p>The number of CPU hours a job consumes is determined by the number of CPUs it is allocated multiplied by its requested walltime. When a job is submitted, the CPU hours it requires are automatically deducted from the account. If the job ends early, the unused hours are automatically refunded.</p> <p>For example, a job requesting 50 CPUs for 10 hours will be charged 500 CPU hours. When the job is submitted, all 500 CPU hours are deducted from the user's account, shown as <code>encumbered</code>. However, if the job only runs for 8 hours and then completes, the unused 100 CPU hours would be refunded.</p> <pre><code>graph LR\n  A[Request 50 CPUs&lt;br&gt;for 10 hours] --&gt; B[500 CPU hours&lt;br&gt;charged];\n  B --&gt; C[Job starts];\n  C --&gt; D[Job completes&lt;br&gt;after 8 hours];\n  D --&gt; E[100 CPU hours&lt;br&gt;refunded];</code></pre> <p>This accounting is the same regardless of which type of node you request. Standard, GPU, and high memory nodes are all charged using the same model and use the same allocation pool. If you find you are being charged for more CPUs that you are specifying in your submission script, it may be an issue with your job's memory request.</p> <p>Allocations are refreshed on the first day of each month. Unused hours from the previous month do not roll over.</p>"},{"location":"resources/allocations/#how-to-find-your-remaining-allocation","title":"How to Find Your Remaining Allocation","text":"<p>To view your allocation's used, unused, and encumbered hours, use the command <code>va</code> in a terminal. For example: <pre><code>(elgato) [user@gpu5 ~]$ va\nWindfall: Unlimited\n\nPI: parent_974 Total time: 7000:00:00\n    Total used*: 1306:39:00\n    Total encumbered: 92:49:00\n    Total remaining: 5600:32:00\n    Group: group1 Time used: 862:08:00 Time encumbered: 92:49:00\n    Group: group2 Time used: 0:00:00 Time encumbered: 0:00:00\n\n*Usage includes all subgroups, some of which may not be displayed here\n</code></pre></p> <p>Note that if your PI has created multiple groups, each of these groups consumes CPU hours from the same allocation. You can see the total allocation pool and the usage for each group you are a member of in the output of <code>va</code>. </p>"},{"location":"resources/compute_resources/","title":"Compute Resources","text":""},{"location":"resources/compute_resources/#compute-resources-available-by-cluster","title":"Compute Resources Available by Cluster","text":"<p>Below is a list of the node types and physical hardware that are available on each of our three clusters. These can be used as a reference when submitting jobs to the system to ensure you are targeting the correct machines and getting the computational resources you need.</p> <p>Node Types</p> Node Type Description Standard CPU Node This is the general purpose node, designed to be used by the majority of jobs. High Memory CPU Node Similar to the standard nodes, but with significantly more RAM. There a only a few of them and they should only be requested for jobs that are known to require more RAM than is provided by standard CPU nodes. GPU Node Similar to the standard node, but with one or more GPUs available. The number of GPUs available per node is cluster-dependent. <p>Available Hardware by Cluster and Node Type</p> PumaOceloteElGato <p>Resources Available</p> Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 192 standard108 buy-in 94 5 GB 470 GB - - - - High Memory 3 standard2 buy-in 94 32 GB 3008 GB - - - - GPU 8 standard7 buy-in 94 5 GB 470 GB 4 32 GB (v100s)20 GB (MIGs) 128 GB 32 standard28 buy-in Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 400 28 6 GB 168 GB - - - - High Memory 1 48 41 GB 1968 GB - - - - GPU 46 28 8 GB 224 GB 1 16 GB 16 GB 46 Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node Standard 130 16 4 GB 64 GB"},{"location":"resources/compute_resources/#gpu-nodes","title":"GPU Nodes","text":"PumaOceloteElGato <p> Puma has a different arrangement for GPU nodes than Ocelote. Whereas Ocelote has one GPU per node, Puma has four. This has a financial advantage for providing GPU's with lower overall cost, and a technical advantage of allowing jobs that can use multiple GPU's to run faster than spanning multiple nodes.</p> <p>Puma's GPU nodes have four Nvidia V100S model GPUs. They are provisioned with 32 GB memory compared to 16 GB on the P100's.  </p> <p>In addition to the V100 nodes, one node has four A100s, each subdivided into three smaller virtual GPUs. See the MIG (Multi-instance GPU) Resources section below for details. </p> <p> Ocelote has 46 compute nodes with Nvidia P100 GPUs that are available to researchers on campus. The limitation is a maximum of 10 concurrent jobs. Previously, one node with a V100 was available, but it has since been replaced with a P100. Tasks which require multiple GPUs must either request multiple nodes on Ocelote, or use Puma's GPU nodes.</p> <p>ElGato has no GPU nodes. During the quarterly maintenance cycle on April 27, 2022 the ElGato K20s and Ocelote K80s were removed after support was ended by Nvidia.</p>"},{"location":"resources/compute_resources/#mig-multi-instance-gpu-resources","title":"MIG (Multi-Instance GPU) Resources","text":"<p>Overview</p> <p>The Four A100 GPUs on Puma Node r5u13n1 are each subdivided into three smaller virtual GPUs using the Nvidia MIG (Multi-Instance GPU) method.  Each of these MIG slices allows the use of 20 GB of GPU memory. The vast majority of jobs run on Puma in 2023 used less than this amount of GPU memory. The 12 MIG GPUs increase overall GPU availability on Puma by freeing the 32 GB V100 GPUs for users requiring larger amounts of GPU memory.</p> <p>Jobs requesting MIG resources will ideally be scheduled more quickly than those requesting the standard V100 GPUs, so MIG resources should be preferred when sufficient.</p> <p>A limitation is that only one MIG slice can be addressed by a single application, so MIG slices are not appropriate for jobs utilizing multiple GPUs.</p> <p>Using MIG Resources </p> <p>The addition of the MIG devices to the Slurm queues will have a number of impacts, and some users may need to make changes to submissions to ensure proper functioning of analyses. For more information on requesting GPU resources, see our Batch Jobs, Interactive Jobs, and/or Open OnDemand Jobs guides. </p>"},{"location":"resources/compute_resources/#system-technical-specifications","title":"System Technical Specifications","text":"ElGato Ocelote Puma Model IBM System X iDataPlex dx360 M4 Lenovo NeXtScale nx360 M5 Penguin Altus XE2242 Year Purchased 2013 2016 (2018 P100 nodes) 2020 Node Count 118 373 CPU-only46 GPU1 High Memory 300 CPU-only15 GPU5 High Memory Total System Memory 26 TB 82.6 TB 128 TB Processors 2x Xeon E5-2650v2 8-core (Ivy Bridge) 2x Xeon E5-2695v3 14-core (Haswell)2x Xeon E5-2695v4 14-core (Broadwell)4x Xeon E7-4850v2 12-core (Ivy Bridge) 2x AMD EPYC 7642 48-core (Rome) Cores/Node (Schedulable) 16 28 (48 - High-memory node) 94 Total Cores 1888 11696<sup>1</sup> 30720<sup>1</sup> Processor Speed 2.66 GHz 2.3 GHz (2.4GHz - Broadwell CPUs) 2.4 GHz Memory/Node 256 GB - GPU nodes64 GB - CPU-only nodes 192 GB(2 TB - High-memory node) 512 GB(3 TB - High-memory nodes) Accelerators 46 NVIDIA P100 (16GB) 29 NVIDIA V100S /tmp<sup>2</sup> ~840 GB spinning ~840 GB spinning ~1440 TB NVMe HPL Rmax (TFlop/s) 46 382 OS CentOS 7 CentOS 7 CentOS 7 Interconnect FDR Inifinband FDR Infiniband for node-node10 Gb Ethernet node-storage 1x 25 Gb/s Ethernet RDMA (RoCEv2)1x 25 Gb/s Ethernet to storage <ol> <li> <p>Includes high-memory and GPU node CPUs\u00a0\u21a9\u21a9</p> </li> <li> <p>/tmp is scratch space and is part of the root filesystem\u00a0\u21a9</p> </li> </ol>"},{"location":"resources/data_center/","title":"Research Data Center","text":""},{"location":"resources/data_center/#central-computing-facilities","title":"Central Computing Facilities","text":"<p>The University of Arizona (UArizona) has two data center facilities available to assist researchers on campus:</p> <ul> <li> <p>Research Data Center (RDC): 1200 ft<sup>2</sup> raised floor data center designed for water-cooled racks dedicated to centrally managed research computing systems</p> </li> <li> <p>Co-location Data Center:  1900 ft<sup>2</sup> of raised floor data center space for air-cooled research co-located equipment</p> </li> </ul> <p>These campus data centers are managed by the UArizona\u2019s central computing organization, University Information Technology Services (UITS). Other than installation costs no bandwidth or other recurring charges will be levied for co-location of research systems in these facilities.</p>"},{"location":"resources/data_center/#power-and-cooling","title":"Power and Cooling","text":"<p>The UITS data centers are both located in the Computer Center with 1192 kW of battery backup and a 1750 kW generator for backup power.</p> <p>Cooling in the RDC is both in-rack cooling with chilled water heat exchangers and Computer Room Air Conditioning (CRAC) units. The co-location Data Center is cooled with chilled water CRAC units and dual cool CRACs. Both data centers are equipped with 18\u201d raised floors that allow for full coverage of cooling to all the equipment, and leak detection systems in the subfloor.</p>"},{"location":"resources/data_center/#fire-suppression","title":"Fire Suppression","text":"<p>The fire suppression system is a multi-tiered defense with clean agent compressed gas, dry pipe pre-action sprinkler and EPO (Emergency Power Off) systems zoned to deploy in affected areas.  For prevention storage of combustible materials such as cardboard, flammable liquids and other hazardous materials is prohibited within the data centers.</p>"},{"location":"resources/data_center/#security","title":"Security","text":"<p>UITS data centers have badge swipe access with two-factor authentication and video surveillance in data center and surrounding building. The data centers are monitored by a co-located 24/7 Operations and dedicated infrastructure team. With automated environmental and system monitoring to assist with issue triaging and escalation. All personnel with swipe access to the data centers have undergone background checks and are required to be US Citizens.</p>"},{"location":"resources/data_center/#network-and-connectivity","title":"Network and Connectivity","text":"<p>In addition to direct connections to commodity Internet carriers, the UArizona connection to Internet2 is through the Sun Corridor Network \u2013 an Arizona regional network established through a collaborative effort sponsored by the Arizona Board of Regents\u2019 (ABOR) three state universities \u2013 Arizona State University (ASU), Northern Arizona University (NAU), and the University of Arizona (UArizona). The Sun Corridor Network provides advanced networking services beyond those available from the individual Arizona Universities and builds an environment essential to leading-edge education, research, and the sharing of digital communications resources, network services, and applications among eligible members.</p> <p>The UArizona manages and operates the Sun Corridor Network. The current connection from UArizona to Sun Corridor is dual 10 Gb, while Sun Corridor is connected to Internet2 via dual 100 Gb connections in Tucson and Phoenix. Network traffic to Internet2 is automatically routed via the Internet2 infrastructure; no action or configuration by the user is required to take advantage of Internet2 connectivity.</p> <p>The UArizona\u2019s Research Data Center has 40 Gb/s connections to the UArizona core with all the servers connected by 1 Gb/s or 10 Gb/s connections.  In-rack switching is enabled with Cisco FEX switches used in a top of rack configuration in both data centers with servers connected to two different switches for (N + 1) redundancy.</p> <p>In addition to direct connectivity to the campus network at the building level, researchers have an opportunity to use a Science DMZ for fast and high volume data transfers to outside collaborating institutions. The Science DMZ is deployed at the University of Arizona network perimeter, outside border firewalls, and is directly connected to Sun Corridor via 10 Gb link. It is secured via static access lists deployed at the Sun Corridor router without impact to performance. There are two high-performance Data Transfer Nodes (DTNs) deployed in the Science DMZ. DTN\u2019s are dedicated servers with hardware and operating system optimized for high speed transfer. We recommend using these DTNs for large transfers</p>"},{"location":"resources/secure_hpc/access/","title":"Access","text":"<p>Warning</p> <p>You must be connected to the Soteria VPN to access the system.</p>"},{"location":"resources/secure_hpc/access/#command-line-access","title":"Command Line Access","text":"<p>Soteria command line access is available with SSH using the hostname <code>shell.cougar.hpc.arizona.edu</code></p> <pre><code>$ ssh your_netid@shell.cougar.hpc.arizona.edu\n\nAuthorized uses only. All activity may be monitored and reported.\nLast login: Tue Nov 29 06:18:33 2022 from ans-02.hpc.arizona.edu\nAuthorized uses only. All activity may be monitored and reported.\nnetid@taub:~ $\n</code></pre> <p>Taub is a login node and will provide the same functionality and have the same policies as the other HPC clusters. Modules are available on Soteria's compute nodes but not on the login node. The command <code>interactive</code> is available to request a session on a compute node and jobs may be submitted using the standard <code>sbatch</code>. More details on Slurm commands can be found in Running Jobs</p>"},{"location":"resources/secure_hpc/access/#graphical-interface","title":"Graphical Interface","text":"<p>Similar to the other HPC clusters, we offer the service Open OnDemand to provide web browser access to Soteria. This can be used to navigate, view, and edit files as well as gain access to graphical applications.</p> <p>In your favorite browser, go to: https://ondemand-hipaa.hpc.arizona.edu</p> <p>The applications currently available are  RStudio, Matlab and Python 3.9 (Jupyter). </p>"},{"location":"resources/secure_hpc/overview/","title":"Secure HPC Overview","text":"<p>Warning</p> <p>Currently Soteria is in a pilot test mode and is not generally available. </p> <p>Research Technologies in partnership with the Data Science Institute is providing a secure research enclave that is HIPAA compliant. It is called Soteria. In Greek mythology, Soteria (Greek: \u03a3\u03c9\u03c4\u03b7\u03c1\u03af\u03b1) was the goddess or spirit (daimon) of safety and salvation, deliverance, and preservation from harm.</p>"},{"location":"resources/secure_hpc/overview/#available-resources","title":"Available Resources","text":"<p>This small cluster has four standard compute nodes. Each has 94 cores and 470 GB memory available. The two GPU nodes have the same resources but there are also four V100 GPU's in each. You can use the regular parts of the documentation to learn how to use Slurm with these nodes.</p> Node Type Node Names Standard Nodes r1u26n1,r1u27n1,r1u28n1,r1u29n1 GPU Nodes r1u30n1,r1u32n1"},{"location":"resources/secure_hpc/overview/#allocations-and-storage","title":"Allocations and Storage","text":"<p>For the purpose of this early testing, the allocations of time and space will be similar to HPC. The time allocation will be 100,000 hours.</p> <p>Your account will come with space in <code>/home</code> and <code>/groups</code> where you can put your data.  Currently those directories are not subject to a quota limit.</p>"},{"location":"resources/secure_hpc/prerequisites/","title":"Prerequisites to Access","text":"<p>To gain access, you will need to submit a Soteria request form. Once your form has been reviewed and approved, you will receive an email with the subject UA Soteria Access Request Approved. This email will contain the next steps to take which are detailed below:</p>"},{"location":"resources/secure_hpc/prerequisites/#complete-required-training-in-edge-learning","title":"Complete Required Training in Edge Learning","text":"<p>The CRRSP (regulated research) team will register you for the required trainings listed below (courses can also be found here: https://uaccess.arizona.edu):</p> <ol> <li>HIPAA Essentials</li> <li>Information Security: Insider Threat Awareness</li> <li>Information Security Awareness Certification</li> </ol>"},{"location":"resources/secure_hpc/prerequisites/#assignment-to-the-soteria-vpn","title":"Assignment to the Soteria VPN","text":"<p>Once you have completed your required training, the CRRSP team will notify you via email when you have been assigned access to the Soteria VPN. This VPN is an important part of our HIPAA compliance and differentiates Soteria usage from the standard HPC clusters. Soteria access cannot be established when not connected to the VPN. For VPN access, use: <code>vpn.arizona.edu/soteria</code>.</p> <p></p>"},{"location":"resources/secure_hpc/prerequisites/#additional-requirements","title":"Additional Requirements","text":"<p>The computer you will use to access Soteria services must meet the following requirements:</p> <ol> <li>The Operating System and applications must be updated with the latest patches.</li> <li>You must have a strong password to log into the computer (at least 8 characters and a mix of character types). </li> <li>This must not be a shared computer with other users.</li> <li>Up to date anti-virus software.</li> </ol>"},{"location":"resources/secure_hpc/storage_and_transfers/","title":"Storage and Transfers","text":""},{"location":"resources/secure_hpc/storage_and_transfers/#storage-available","title":"Storage Available","text":"<p>All users are granted a home directory. Additional communal space can be found in a groups directory allocated to each PI.</p> <p>Your files can be accessed on the filexfer nodes(1):</p> <ol> <li>hostname: <code>filexfer.hpc.arizona.edu</code></li> </ol> <ul> <li> <code>/hipaa/groups/ <li><code>/hipaa/home/uxx/ <p>When connected to a Soteria login/compute node, you can find these under:</p> <ul> <li><code>/groups/&lt;pi_netid&gt;</code></li> <li><code>/home/uxx/&lt;your_netid&gt;</code></li> </ul>"},{"location":"resources/secure_hpc/storage_and_transfers/#storage-quotas","title":"Storage Quotas","text":"<p>Currently, directories are not subject to a quota limit.</p>"},{"location":"resources/secure_hpc/storage_and_transfers/#transferring-data","title":"Transferring Data","text":"<p>Globus can be used for moving data in and out of the Soteria environment. For more information on using Globus, see our Globus documentation</p> <p>Soteria's endpoint is: UA HPC HIPAA Filesystems</p>"},{"location":"running_jobs/batch_jobs/array_jobs/","title":"Array Jobs","text":""},{"location":"running_jobs/batch_jobs/array_jobs/#what-are-job-arrays","title":"What Are Job Arrays?","text":"<p>In Slurm, job arrays are a powerful feature allowing you to submit and manage multiple similar jobs with a single submission script. Instead of submitting individual jobs, you define an array of jobs with similar characteristics.</p> <p>Job Arrays are a method to manage and execute multiple similar jobs as a single entity. You define a range of tasks or jobs within the array, each with its own unique identifier. These are submitted the same way as a regular batch job: <code>sbatch myscript.slurm</code>. Slurm then handles the scheduling and execution of these tasks independently, based on available resources.</p>"},{"location":"running_jobs/batch_jobs/array_jobs/#why-use-job-arrays","title":"Why Use Job Arrays?","text":"<ol> <li> <p>Workflow Efficiency </p> <p>Job arrays streamline the process of managing multiple jobs with similar configurations, reducing manual effort and potential errors in job submission.</p> </li> <li> <p>Resource Utilization</p> <p>By grouping similar tasks into a single job array, you can optimize resource utilization on the cluster. Slurm can efficiently allocate resources to individual tasks within the array based on availability. When multiple independent jobs are submitted without using an array, it can cause the scheduler to sl</p> </li> <li> <p>Scheduler Efficiency</p> <p>Submitting multiple individual jobs in loops can significantly slow down the SLURM scheduler for all users. Job arrays help alleviate this issue by reducing the number of job submissions, leading to improved scheduler performance and responsiveness.</p> </li> <li> <p>Scalability</p> <p>Job arrays are particularly useful for parallel and repetitive tasks, such as parameter sweeps, Monte Carlo simulations, or running the same code with different inputs. They provide a scalable approach to handling large numbers of tasks.</p> </li> <li> <p>Simplified Management</p> <p>With job arrays, you only need to manage a single submission script for a group of tasks, making it easier to track, monitor, and troubleshoot your jobs.</p> </li> </ol>"},{"location":"running_jobs/batch_jobs/array_jobs/#how-to-use-job-arrays","title":"How to Use Job Arrays","text":"<p>To utilize job arrays in Slurm, you define an array job with a specific range of task IDs using the <code>--array</code> batch directive in your submission script. Each task within the array can have its own unique parameters, input files, or commands. These are controlled by the Slurm environment variable <code>$SLURM_ARRAY_TASK_ID</code> which is set to a unique integer value for each subjob in the array.</p> <p>Example</p> <p>Tip</p> <p>The resource requirements defined in an array job are applied to each job in the array</p> <p>Say you wanted to submit three jobs to analyze input files <code>1.txt</code>, <code>2.txt</code>, and <code>3.txt</code>. In this case, we'd set <code>--array=1-3</code> to tell the scheduler we want to submit three jobs with indices 1, 2, and 3. The directives for this batch script might look like:</p> <pre><code>#!/bin/bash\n#SBATCH --account=&lt;GROUP_NAME&gt;\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=01:00:00\n#SBATCH --array=1-3\n</code></pre> <p>Next, we'll need to control what each script does using the environment variable <code>$SLURM_ARRAY_TASK_ID</code>. In this case, we can use the command <code>&lt;command&gt;</code> to analyze these scripts using:</p> <pre><code>&lt;command&gt; ${SLURM_ARRAY_TASK_ID}.txt\n</code></pre> <p>In each subjob, <code>${SLURM_ARRAY_TASK_ID}.txt</code> will be interpreted as <code>1.txt</code>, <code>2.txt</code>, and <code>3.txt</code> where the index matches the array ID.</p> <p>Submitting Your Array Job</p> <p>All you'll need to do to submit your jobs is use the standard <code>sbatch script.slurm</code> syntax once and the scheduler takes care of the rest. No need for loops! </p>"},{"location":"running_jobs/batch_jobs/array_jobs/#monitoring-array-jobs","title":"Monitoring Array Jobs","text":"<p>When you submit a job array, Slurm will assign a single job ID to the array. When you are tracking your jobs, if run a standard <code>squeue</code> you will only see the parent job ID. If you use the command <code>squeue -r &lt;jobid&gt;</code>, that will return a list of all subjobs in the array where each job ID is formatted as <code>&lt;parent_job_id&gt;_&lt;job_array_index&gt;</code>. </p>"},{"location":"running_jobs/batch_jobs/array_jobs/#output-files","title":"Output Files","text":"<p>Each subjob in an array produces its own output file. By default, these are formatted as <code>slurm-&lt;parent_job_id&gt;_&lt;job_array_index&gt;.out</code>. Be careful if you set your own custom output filenames.  If the output filenames are not distinguished from one another using an array task ID, your output files will overwrite one another. </p>"},{"location":"running_jobs/batch_jobs/batch_directives/","title":"Batch Directives","text":"<p>Tip</p> <p>For a full list of directives, see Slurm's official documentation.</p> <p>The first section of a batch script (after the shebang) always contains the Slurm Directives, which specify the resource requests for your job. The scheduler parses these in order to allocate CPUs, memory, walltime, etc. to your job request.</p>"},{"location":"running_jobs/batch_jobs/batch_directives/#allocations-and-partitions","title":"Allocations and Partitions","text":"<p>There are four available partitions, or queues, on the UArizona HPC which determine the priority of your jobs. With the exception of Windfall, these consume your monthly allocation. See our allocations documentation for more detailed information on each. The syntax to request each of the following is shown below:</p> Partition Request Syntax Comments Standard <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=standard</code></pre> Windfall <pre><code>#SBATCH --partition=windfall</code></pre> Unlimited access. Preemptible. Do not include an <code>--account</code> flag when requesting this partition. High Priority <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=high_priority#SBATCH --qos=user_qos_&lt;PI GROUP&gt;</code></pre> Only available to buy-in groups. Qualified <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=standard#SBATCH --qos=qual_qos_&lt;PI GROUP&gt;</code></pre> Available to groups with an activate special project."},{"location":"running_jobs/batch_jobs/batch_directives/#nodes","title":"Nodes","text":"Mutli-Node Programs <p>In order for your job to make use of more than one node, it must be able to make use of something like MPI. </p> <p>If your application is not MPI-enabled, always set <code>--nodes=1</code></p> <p>The term node refers to the number of physical computers allocated to your job. The syntax to allocate <code>&lt;N&gt;</code> nodes to a job is:</p> <pre><code>#SBATCH --nodes=&lt;N&gt;\n</code></pre>"},{"location":"running_jobs/batch_jobs/batch_directives/#cpus","title":"CPUs","text":"<p>Each job must specify the requested number of CPUs with the <code>--ntasks</code> directive.  This can be done in one of two ways:</p> <ol> <li> <p>If your application is making use of MPI or is executing simultaneous distinct processes, you can request <code>&lt;N&gt;</code> CPUs with</p> <pre><code>#SBATCH --ntasks=&lt;N&gt;\n</code></pre> </li> <li> <p>If you are using a multithreaded application, then you can request <code>&lt;N&gt;</code> CPUs with:     <pre><code>#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=&lt;N&gt;\n</code></pre></p> </li> </ol>"},{"location":"running_jobs/batch_jobs/batch_directives/#memory-and-high-memory-nodes","title":"Memory and High Memory Nodes","text":"<p>Memory and CPUs are connected</p> <p>More detailed information on memory and CPU requests can be found on our CPUs and Memory page.</p> <p>Include Units</p> <p>If you exclude <code>gb</code> from your memory request, Slurm will default to <code>mb</code>.</p> <p>Memory is an optional flag. By default, the scheduler will allocate you the standard CPU/memory ratio available on the cluster. </p> <p>Memory can either be requested with the <code>--mem</code> or <code>--mem-per-cpu</code> flags. The <code>--mem</code> flag indicates the amount of Memory per node to allocate to your job. If you are running multi-node MPI jobs with this flag, the total amount of memory you will receive will be <code>mem</code>\\(\\times\\)<code>nodes</code></p> <p>The general syntax for requesting <code>&lt;N&gt;</code> GB of memory per node is <pre><code>#SBATCH --mem=&lt;N&gt;gb\n</code></pre> or, to request <code>&lt;N&gt;</code> GB of memory per CPU: <pre><code>#SBATCH --mem-per-cpu=&lt;N&gt;gb\n</code></pre></p> <p>High Memory Node Requests</p> <p>To request a high memory node, you will need the additional flag <code>--constraint=high_mem</code>. It is recommended to use the exact directives below to avoid unexpected behavior.</p> Cluster Command Ocelote <pre><code>#SBATCH --mem-per-cpu=41gb#SBATCH --constraint=high_mem</code></pre> Puma <pre><code>#SBATCH --mem-per-cpu=32gb#SBATCH --constraint=high_mem</code></pre>"},{"location":"running_jobs/batch_jobs/batch_directives/#time","title":"Time","text":"<p>The syntax for requesting time for your job is <code>HHH:MM:SS</code> or <code>DD-HHH:MM:SS</code>. The maximum amount of time that can be requested is 10 days for a batch job. More details in Job Limits.</p> <pre><code>#SBATCH --time=HHH:MM:SS\n</code></pre>"},{"location":"running_jobs/batch_jobs/batch_directives/#gpus","title":"GPUs","text":"<p>GPUs are an optional resource that may be requested with the <code>--gres</code> directive. For an overview of the specific GPU resources available on each cluster, see our resources page. </p> Cluster Directive Target Puma <pre><code>#SBATCH --gres=gpu:1</code></pre> Request a single GPU. This will either target one Volta GPU (v100) or one A100 MIG slice, depending on availability. Only one GPU should be selected with this method to avoid being allocated multiple MIG slices. <pre><code>#SBATCH --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb</code></pre> Target one A100 MIG slice. <pre><code>#SBATCH --gres=gpu:volta:N</code>\n    Request <code>N</code> V100 GPUs where 1\u2264<code>N</code>\u22644\n  \n  \n    Ocelote\n    <pre><code>#SBATCH --gres=gpu:1</code></pre>\n    Request one Pascal GPU (p100)"},{"location":"running_jobs/batch_jobs/batch_directives/#job-arrays","title":"Job Arrays","text":"<p>Array jobs in Slurm allow users to submit multiple similar tasks as a single job. Each task within the array can have its own unique input parameters, making it ideal for running batch jobs with varied inputs or executing repetitive tasks efficiently. The flag for submitting array jobs is:</p>\n<p><pre><code>#SBATCH --array=&lt;N&gt;-&lt;M&gt;\n</code></pre>\nwhere <code>&lt;N&gt;</code> and <code>&lt;M&gt;</code> are integers. </p>\n<p>For detailed information on job arrays, see our job array tutorial.</p>"},{"location":"running_jobs/batch_jobs/batch_directives/#output-filenames","title":"Output Filenames","text":"<p>The default output filename for a slurm job is <code>slurm-&lt;jobid&gt;.out</code>. If desired, this can be customized using the directives\n<pre><code>#SBATCH -o output_filename.out\n#SBATCH -e output_filename.err\n</code></pre></p>\n<p>Filenames take patterns that allow for job information substitution. A list of filename patterns is shown below. </p>\n\n\n\nVariable\nMeaning\nExample Slurm Directive(s)\nSample Output\n\n\n\n\n<code>%A</code>\nA job array's main job ID\n<pre><code>#SBATCH --array=1-2#SBATCH -o %A.out#SBATCH --open-mode=append</code></pre>\n<code>12345.out</code>\n\n\n<code>%a</code>\nA job array's index number\n<pre><code>#SBATCH --array=1-2#SBATCH -o %A_%a.out</code></pre>\n<code>12345_1.out</code><code>12345_2.out</code>\n\n\n<code>%J</code>\nJob ID plus stepid\n<pre><code>#SBATCH -o %J.out</code></pre>\n<code>12345.out</code>\n\n\n<code>%j</code>\nJob ID\n<pre><code>#SBATCH -o %j.out</code></pre>\n<code>12345.out</code>\n\n\n<code>%N</code>\nHostname of the first compute node allocated to the job\n<pre><code>#SBATCH -o %N.out</code></pre>\n<code>r1u11n1.out</code>\n\n\n<code>%u</code>\nUsername\n<pre><code>#SBATCH -o %u.out</code></pre>\n<code>netid.out</code>\n\n\n<code>%x</code>\nJob name\n<pre><code>#SBATCH --job-name=JobName#SBATCH -o %x.out</code></pre>\n<code>JobName.out</code>"},{"location":"running_jobs/batch_jobs/batch_directives/#additional-directives","title":"Additional Directives","text":"Command\nPurpose\n\n\n\n\n<pre><code>#SBATCH --job-name=JobName</code></pre>\nOptional: Specify a name for your job. This will not automatically affect the output filename.\n\n\n<pre><code>#SBATCH -e output_filename.err#SBATCH -o output_filename.out</code></pre>\nOptional: Specify output filename(s). If <code>-e</code> is missing, stdout and stderr will be combined.\n\n\n<pre><code>#SBATCH --open-mode=append</code></pre>\nOptional: Append your job's output to the specified output filename(s).\n\n\n<pre><code>#SBATCH --mail-type=BEGIN|END|FAIL|ALL</code></pre>\nOptional: Request email notifications. Beware of mail bombing yourself.\n\n\n<pre><code>#SBATCH --mail-user=email@address.xyz</code></pre>\nOptional: Specify email address. If this is missing, notifications will go to your UArizona email address by default.\n\n\n<pre><code>#SBATCH --export=VAR</code>\nOptional: Export a comma-delimited list of environment variables to a job.\n\n\n<pre><code>#SBATCH --export=all</code>\nOptional: Export your working environment to your job. This is the default.\n\n\n<pre><code>#SBATCH --export=none</code>\nOptional: Do not export working environment to your job."},{"location":"running_jobs/batch_jobs/batch_directives/#examples-and-explanations","title":"Examples and Explanations","text":"<p>The below examples are complete sections of Slurm directives that will produce valid requests. Other directives can be added (like output files), but they are not strictly necessary to submit a valid request. For simplicity, the Puma cluster is assumed when discussing memory and GPU resources. Note that these examples do not include the shebang <code>#!bin/bash</code> statement, which should be at the top of every slurm script. Also, note that the order of directives does not matter.</p>\nSingle CPUSingle NodeSingle GPU NodeMulti-NodeHigh-Memory Node\n\n\n<pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=01:00:00\n</code></pre>\n<p>This example requests one CPU on one node for one hour. Easy!</p>\n\n\n<pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=10\n#SBATCH --time=01:00:00\n</code></pre>\n<p>10 CPUs are now requested. The default value of <code>mem-per-cpu</code> is assumed, therefore giving this job 50 GB of total memory. Specifying this value by including <code>#SBATCH --mem-per-cpu=5gb</code> will not change the behavior of the above request.</p>\n<p>The example below will produce an equivalent request as above:</p>\n<p><pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --mem=50gb\n#SBATCH --time=01:00:00\n</code></pre>\nOn Puma, up to 94 CPUs or 470 GB of memory can be requested. </p>\n\n\n<pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=10\n#SBATCH --time=01:00:00\n#SBATCH --gres=gpu:1\n</code></pre>\n<p>Note the <code>gres=gpu:1</code> option.</p>\n\n\n<p>When requesting a multi-node job, up to 94 <code>--ntasks-per-node</code> can be requested on Puma. The numbers below are chosen for illustrative purposes and can be replaced with your choice, up to system limitations. It should be noted that there is no advantage to requesting multiple nodes when the total number of CPUs needed is less than or equal to the number of CPUs on one node.  </p>\n<pre><code>#SBATCH --job-name=Multi-Node-MPI-Job\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --ntasks=30\n#SBATCH --nodes=3\n#SBATCH --ntasks-per-node=10\n#SBATCH --time=01:00:00   \n</code></pre>\n\n\n<p>When requesting a high memory node, include both the <code>--mem-per-cpu</code> and <code>--constraint</code> directives.</p>\n<pre><code>#SBATCH --job-name=High-Mem-Job\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=94\n#SBATCH --mem-per-cpu=32gb\n#SBATCH --constraint=hi_mem\n#SBATCH --time=01:00:00   \n</code></pre>\n\n\n\n\n\n<ol>\n<li>\n<p>Groups and users are subject to limitations on resource usage. For more information, see job limits.\u00a0\u21a9</p>\n</li>\n</ol>"},{"location":"running_jobs/batch_jobs/environment_variables/","title":"Slurm Environment Variables","text":"<p>Every Slurm job has environment variables that are set by default. These can be used in a job to control job behavior, for example setting the names of output files or directories, setting CPU count for multithreaded processes, controlling input parameters, etc. </p> Variable Purpose Example Value <code>$SLURM_ARRAY_JOB_ID</code> Job array's parent ID <code>399124</code> <code>$SLURM_ARRAY_TASK_COUNT</code> Total number of subjobs in an array <code>4</code> <code>$SLURM_ARRAY_TASK_ID</code> Job index number (unique for each job in an array) <code>1</code> <code>$SLURM_ARRAY_TASK_MAX</code> Maximum index for the job array <code>7</code> <code>$SLURM_ARRAY_TASK_MIN</code> Minimum index for the job array <code>1</code> <code>$SLURM_ARRAY_TASK_STEP</code> Job array's index step size <code>2</code> <code>$SLURM_CLUSTER_NAME</code> Which cluster your job is running on <code>elgato</code> <code>$SLURM_CONF</code> Points to the Slurm configuration file <code>/var/spool/slurm/d/conf-cache/slurm.conf</code> <code>$SLURM_CPUS_ON_NODE</code> Number of CPUs allocated to target node <code>3</code> <code>$SLURM_GPUS_ON_NODE</code> Number of GPUs allocated to the target node <code>1</code> <code>$SLURM_GPUS_PER_NODE</code> Number of GPUs per node. Only set if <code>--gpus-per-node</code> is specified <code>1</code> <code>$SLURM_JOB_ACCOUNT</code> Account being charged <code>groupname</code> <code>$SLURM_JOB_GPUS</code> The global GPU IDs of the GPUs allocated to the job. Only set in batch and interactive jobs. <code>0</code> <code>$SLURM_JOB_ID</code> Your Slurm Job ID <code>399072</code> <code>$SLURM_JOB_CPUS_PER_NODE</code> Number of CPUs per node. This can be a list if there is more than one node allocated to the job. The list has the same order as <code>SLURM_JOB_NODELIST</code> <code>3,1</code> <code>$SLURM_JOB_NAME</code> The job's name <code>interactive</code> <code>$SLURM_JOB_NODELIST</code> The nodes that have been assigned to your job <code>gpu[73-74]</code> <code>$SLURM_JOB_NUM_NODES</code> The number of nodes allocated to the job <code>2</code> <code>$SLURM_JOB_PARTITION</code> The job's partition <code>standard</code> <code>$SLURM_JOB_QOS</code> The job's QOS/Partition <code>qos_standard_part</code> <code>$SLURM_JOB_USER</code> The username of the person who submitted the job <code>netid</code> <code>$SLURM_JOBID</code> Same as <code>SLURM_JOB_ID</code>, your Slurm Job ID <code>399072</code> <code>$SLURM_MEM_PER_CPU</code> The memory/CPU ratio allocated to the job <code>4096</code> <code>$SLURM_NNODES</code> Same as <code>SLURM_JOB_NUM_NODES</code> \u2013 the number of nodes allocated to the job <code>2</code> <code>$SLURM_NODELIST</code> Same as <code>SLURM_JOB_NODELIST</code>, The nodes that have been assigned to your job <code>gpu[73-74]</code> <code>$SLURM_NPROCS</code> The number of tasks allocated to your job <code>4</code> <code>$SLURM_NTASKS</code> Same as <code>SLURM_NPROCS</code>, the number of tasks allocated to your job <code>4</code> <code>$SLURM_SUBMIT_DIR</code> The directory where <code>sbatch</code> was used to submit the job <code>/home/u00/netid</code> <code>$SLURM_SUBMIT_HOST</code> The hostname where <code>sbatch</code> was used to submit the job <code>wentletrap.hpc.arizona.edu</code> <code>$SLURM_TASKS_PER_NODE</code> The number of tasks to be initiated on each node. This can be a list if there is more than one node allocated to the job. The list has the same order as <code>SLURM_JOB_NODELIST</code> <code>3,1</code> <code>$SLURM_WORKING_CLUSTER</code> Valid for interactive jobs, will be set with remote sibling cluster's IP address, port and RPC version so that any sruns will know which cluster to communicate with. <code>elgato:foo:0000:0000:000</code>"},{"location":"running_jobs/batch_jobs/intro/","title":"Intro to Batch","text":""},{"location":"running_jobs/batch_jobs/intro/#introduction-to-batch-jobs","title":"Introduction to Batch Jobs","text":""},{"location":"running_jobs/batch_jobs/intro/#what-are-batch-jobs","title":"What are batch jobs?","text":"<p>Some jobs don't need a GUI, may take a long time to run, and/or do not need user input. In these cases, batch jobs are useful because they allow a user to request resources for a job, then wait until it completes automatically without any further input. The user can even fully log off of HPC, and the submitted jobs will continue to run.  The program on HPC that takes requests and assigns resources at ideal times to optimize cluster usage is called a scheduler. All three clusters, Puma, Ocelote, and El Gato, use Slurm for resource management and job scheduling.</p> <p>Contrast with graphical and interactive jobs</p> <p>The main difference between batch and GUI jobs is that batch jobs are only text-based and give no graphical feedback during runtime. While there is a method to submit jobs using a GUI, strictly speaking, batch jobs are of a different nature than GUI jobs.</p> <p>Batch jobs are also different from interactive jobs, because while both use the command line interface and the Slurm scheduler, there is no feedback or interactivity with batch jobs. The script is run exactly as submitted with no way to change it once it is submitted, though it can be canceled. Each batch job is assigned a unique job ID that can be used to trace it.</p>"},{"location":"running_jobs/batch_jobs/intro/#batch-workflow","title":"Batch workflow","text":"<p>The general process for submitting a batch job is as follows:</p> <ol> <li> <p>Write your analysis. </p> <p>This requires an executable program as well as some input files or options. This varies widely between different types of analyses, you will need to determine what needs to be done for your particular analysis.</p> </li> <li> <p>Write your Slurm batch script. </p> <p>This tells the scheduler what resources you want for your job and how to run it. The batch script is written in bash, and normal bash commands can be used within the batch script.</p> </li> <li> <p>Submit your request. </p> <p>This is usually as simple as running <code>sbatch my-script.slurm</code>.</p> </li> <li> <p>Wait. </p> <p>Now the scheduler has your request. It will compare your job to all currently waiting jobs and determine when to run it. Jobs that request more resources generally wait longer in the queue, but there is no concrete rule that determines how long a given job will wait. Typical wait times vary by cluster and activity. Generally, jobs submitted to El Gato will start much sooner than jobs submitted to Puma, with Ocelote falling in between. To check on the activity of a given cluster, use <code>nodes-busy</code> for a detailed report or <code>cluster-busy</code> for an overview. To give some expectation, small jobs on Puma may start within 5 minutes, but large multinode jobs may wait days to begin.</p> </li> </ol>"},{"location":"running_jobs/batch_jobs/intro/#batch-script-structure","title":"Batch Script Structure","text":"<p>A batch script is a text file that is written with three sections:</p> <pre>#!/bin/bash</pre> <pre>#SBATCH --option=value</pre> <pre>[code here]</pre> <ol> <li>The \"shebang\" will always be the line <code>#!/bin/bash</code>. This tells the system to interpret your file as a bash script. Our HPC systems use bash for all our environments, so it should be used in your scripts to get the most consistent, predictable results.</li> <li>The directives section will have multiple lines, all of which start with <code>#SBATCH</code>. These lines are interpreted as directives by the scheduler and are how you request resources on the compute nodes, set your output filenames, set your job name, request emails, etc. A list of directives is shown in Batch Directives.</li> <li>The code section in your script is a set of bash commands that tells the system how to run your analyses. This includes any module load commands you'd need to run to access your software, software commands to execute your analyses, directory changes, etc. </li> </ol> <p>An example batch script might look like the following:</p> <pre><code>#!/bin/bash# --------------------------------------------------------------\n### Directives Section: Requests resources to run your job.\n# --------------------------------------------------------------\n#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:01:00# --------------------------------------------------------------\n### Code Section: Executes bash commands to run your job\n# --------------------------------------------------------------\nmodule load python/3.9\ncd ~/hello_world\npython3 -c \"print('hello world')\"\n### sleep is used for demonstration purposes\nsleep 30</code></pre>"},{"location":"running_jobs/batch_jobs/submitting_jobs/","title":"Submitting Batch Jobs","text":""},{"location":"running_jobs/batch_jobs/submitting_jobs/#submitting-a-job","title":"Submitting a Job","text":"<p>To submit a batch job to the scheduler, use the command <code>sbatch</code>. This will place your job in line for execution and will return a job ID that you can use to track and monitor your job. </p> <p>As an example:</p> <pre><code>[netid@gpu66 hello_world]$ sbatch hello_world.slurm\nSubmitted batch job 807387\n[netid@gpu66 hello_world]$ squeue --job 807387\n             JOBID PARTITION     NAME     USER ST       TIME  NODES\n            807387  standard hello_wo    netid PD       0:06      1 \n</code></pre> <p>The command <code>squeue</code> gives us detailed information about our batch jobs while they're in queue or running. Under <code>ST</code> you can check the state of your job. In this case, it's pending (<code>PD</code>) which means it's waiting in line with other jobs. Once the job starts running, it's state will change to <code>R</code>, and when the job has completed running, <code>squeue</code> will return a blank line. </p>"},{"location":"running_jobs/batch_jobs/submitting_jobs/#submitting-multiple-jobs","title":"Submitting Multiple Jobs","text":"<p>Frequently, users need to submit multiple, related jobs. It may be tempting to do this using a bash loop, but there are several drawbacks to this method, primarily that it can affect the performance of the job scheduling software.  </p> <p>Use Arrays instead of Loops for large numbers of jobs</p> <p>Users submitting large numbers (&gt; 100s) of jobs using loops will be contacted and asked to adjust their workflows. Requests that persistently affect the performance of the job scheduler will be cancelled by HPC infrastructure.</p> <p>The best way to submit related jobs is to use job arrays. Jobs arrays allow users to submit multiple related jobs using a single script and single <code>sbatch</code> command. Each task within the array can have its own unique input parameters, making it ideal for running batch jobs with varied inputs or executing repetitive tasks efficiently. See Array Jobs for specifics on how to submit these sorts of workflows.</p>"},{"location":"running_jobs/batch_jobs/submitting_jobs/#output-files","title":"Output Files","text":"<p>Once your job completes, you should see an output file in the directory where you submitted the batch script. This output file captures anything that would have been printed to the terminal if you had run it interactively. By default, output filenames will be <code>slurm-&lt;jobid&gt;.out</code>(1). In the example above, this translates to filename <code>slurm-807387.out</code>. </p> <ol> <li>Custom output filenames can be set with batch directives.</li> </ol>"},{"location":"running_jobs/cpus_and_memory/","title":"CPUs and Memory","text":""},{"location":"running_jobs/cpus_and_memory/#cpu-and-memory-correlation","title":"CPU and Memory Correlation","text":"<p>Before submitting your job to the scheduler, it's important to know that the number of CPUs you allocate to your job determines the amount of memory you receive. </p> <p>Each cluster has a fixed amount of memory per CPU based on the node type. Accepted values by cluster and node type are listed below:</p> Cluster Standard Node High-Memory Node GPU Node Puma 5 GB 32 GB 5 GB Ocelote 6 GB 41 GB 8 GB El Gato 4 GB - - <p>For example, using the table above we can see on Puma standard nodes you get 5 GB for each CPU you request. This means a standard job using 4 CPUs gets 5 GB/CPU \u00d7 4 CPUs = 20 GB of total memory.</p> <p>The video below shows the relationship between memory and CPUs, specifically looking at one of our Puma nodes. </p> <p> <p></p>"},{"location":"running_jobs/cpus_and_memory/#determining-job-resources","title":"Determining Job Resources","text":"<p>The following flowchart describes the process of determining the amount of memory and number of CPUs to allocate to a job. For simplicity, this shorthand will be used:</p> <ul> <li>N: number of CPUs desired</li> <li>M: total memory desired</li> <li>MpC: the default value of memory per CPU for fixed cluster and node type as listed in the table above</li> </ul> <p>If a decimal value is encountered, round up in all cases. </p> <pre><code>graph LR\n   A[Is my job CPU&lt;br&gt;or Memory limited?] \n   B[\"`set CPUs=**N**`\"]\n   C[\"`do not specify\n   memory or mem/cpu`\"]\n   D[\"`Set CPUs=**M**/**MpC**`\"]\n   E[\"`Set mem/CPU=**MpC**`\"]\n   Z[Done]\n   A --&gt;|CPU| B --&gt; C --&gt; Z\n   A --&gt;|mem| D -.-&gt; E --&gt; Z\n   D --&gt; Z</code></pre> <p>The dotted line above indicates that setting mem/CPU in your job is not strictly necessary. If you are requesting a standard node, this value is set for you by the scheduler. The only times you will need to set this value is:</p> <ol> <li>If you're requsting a non-standard node (e.g. a high memory or Ocelote GPU node)</li> <li>If you're requesting an OnDemand application session. There is a field where you will fill in your mem/CPU requirement. </li> </ol> <p>Note that there is no deterministic method of finding the exact amount of memory needed by a job in advance. A general rule of thumb is to overestimate it slightly and then scale down based on previous runs. Significant overestimation, however, can lead to inefficiency of system resources and unnecessary expenditure of CPU time allocations. </p>"},{"location":"running_jobs/cpus_and_memory/#things-to-watch-out-for","title":"Things to Watch out for","text":"<p>Be careful when requesting memory and memory per CPU. Note that if you request invalid mem/cpu values, unpredictable results may occur:</p> <ul> <li> <p>Memory and CPU Mismatches</p> <p>In batch scripts, if you request <code>--memory</code> instead of <code>--mem-per-cpu</code>, the scheduler will automatically increase your CPU allocation to align with your memory requirements in case of a mismatch. For instance, if you request one CPU and 50 GB of total memory, the scheduler will adjust your resource requirements by allocating 10 CPUs to match your memory request.</p> </li> <li> <p>Invalid Mem/CPU Options</p> <p>If you request a mem/CPU value that isn't valid, the scheduler won't outright reject your job. Instead, it will attempt to accommodate your request. This could involve your job being moved to a high memory node if you ask for more than the standard ratio. However, high memory nodes usually have considerably longer wait times compared to standard nodes, potentially resulting in a longer queue time than anticipated.</p> <p>Alternatively, you might end up with less memory allocated than you expected. For instance, there aren't any machines with a memory ratio exceeding 41 GB/CPU. Therefore, if you request 100 GB/CPU, your job will still be constrained by the physical limits of available memory.</p> </li> </ul>"},{"location":"running_jobs/example_batch_jobs/","title":"Index","text":"<p>foo</p>"},{"location":"running_jobs/example_batch_jobs/array_and_gnu_parallel/","title":"Array Job with GNU Parallel","text":"<p>Click here to download example files</p> <p>This script combines the methods of the basic array job and the basic parallel job to execute a large number of jobs.</p>"},{"location":"running_jobs/example_batch_jobs/array_and_gnu_parallel/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>Sometimes you need to run a lot of jobs. More than can be reasonably accomplished using arrays since submitting thousands of jobs can be a problem for the system, and GNU Parallel can be challenging to make work in a multi-node environment. In this case, we can combine the forces of GNU Parallel and array jobs to distribute a chunk of tasks across multiple nodes where GNU Parallel will execute them</p>"},{"location":"running_jobs/example_batch_jobs/array_and_gnu_parallel/#example","title":"Example","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Sample_Array_With_GNU_Parallel\n#SBATCH --ntasks=94\n#SBATCH --nodes=1                    \n#SBATCH --time=00:05:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-2\n\nmodule load parallel\nBLOCK_SIZE=200\nseq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE)) | parallel echo \"JOB ID: $SLURM_JOB_ID HOST NODE: $HOSTNAME EXAMPLE COMMAND: ./executable input_{}\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_and_gnu_parallel/#script-breakdown","title":"Script Breakdown","text":"<p>Like with the basic parallel example, GNU Parallel is accessible as a module. The general goal here for illustration purposes is to set up a \"block size\". This is the number of tasks GNU Parallel will be executing in each subjob</p> <pre><code>BLOCK_SIZE=200\n</code></pre> <p>In this case, we're asking for 200 tasks per subjob and since we're submitting an array job, that totals 400 tasks. The array indices then used to differentiate tasks. <code>seq n m</code> generates a sequence of integers from <code>n</code> to <code>m</code> (inclusive).</p> <p><code>SLURM_ARRAY_TASK_ID</code> in this case is either 1 or 2, depending on the subjob, so combined with <code>BLOCK_SIZE</code>:</p> <p>Subjob 1: </p> <pre><code>seq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE))\n# Doing the math --&gt; seq 1*200-200+1 1*200 --&gt; seq 1 200\n</code></pre> <p>Subjob 2:  <pre><code>seq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE))\n# Doing the math --&gt; seq 2*200-200+1 2*200 --&gt; seq 201 400\n</code></pre></p>"},{"location":"running_jobs/example_batch_jobs/array_and_gnu_parallel/#script-submission-command","title":"Script Submission Command","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-and-Parallel.slurm \nSubmitted batch job 1693973\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_and_gnu_parallel/#output-files","title":"Output Files","text":"<pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1693973_1.out  slurm-1693973_2.out\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_and_gnu_parallel/#file-contents","title":"File Contents","text":"<p>A total of 400 tasks were executed using an array job with two subjobs on two nodes, r2u07n1 and r2u13n2. To view the full contents of the output file, download the example above. </p> <pre><code>(puma) [netid@junonia ~]$ head *.out\n==&gt; slurm-1693973_1.out &lt;==\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_1\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_2\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_3\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_4\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_5\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_6\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_7\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_8\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_9\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_10\n\n==&gt; slurm-1693973_2.out &lt;==\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_201\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_202\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_203\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_204\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_205\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_206\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_207\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_208\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_209\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_210\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/","title":"Array Job With Text Filenames","text":"<p>Click here to download example files</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>If you want to run multiple jobs where each opens a different file to analyze but the naming scheme isn't conducive to automating the process using simple array indices (i.e., 1.txt, 2.txt, ...)</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#example","title":"Example","text":""},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#submission-script","title":"Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Array-Read-Filenames\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-4\n\nCurrentFile=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\"\necho \"JOB NAME: $SLURM_JOB_NAME, JOB ID: $SLURM_JOB_ID, EXAMPLE COMMAND: ./executable -o output${SLURM_ARRAY_TASK_ID} ${CurrentFile}\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#input-file","title":"Input File","text":"<p>For this example, you'll want to have a file called InputFiles in your working directory. This will contain one filename per line. Contents: <pre><code>SRR2309587.fastq\nSRR3050489.fastq\nSRR305356.fastq\nSRR305p0982.fastq\n</code></pre></p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#script-breakdown","title":"Script Breakdown","text":"<p>For each of the four subjobs, we'll make use of <code>SLURM_ARRAY_TASK_ID</code> to pull the line number (line numbers 1 to 4) from InputFiles:</p> <pre><code>CurrentFile=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\"\n</code></pre> <p>We will print a sample command that includes our filename to verify that everything is working as expected for demonstration purposes:</p> <pre><code>echo \"JOB NAME: $SLURM_JOB_NAME, JOB ID: $SLURM_JOB_ID, EXAMPLE COMMAND: ./executable -o output${SLURM_ARRAY_TASK_ID} ${CurrentFile}\"\n</code></pre> <p>To generate your own InputFile, you can either manually add your filenames or can automate the process, for example if you have all your files in a single location:</p> <pre><code>$ ls *fastq &gt; InputFiles\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#script-submission-command","title":"Script Submission Command:","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-Read-Filenames.slurm \nSubmitted batch job 1694071\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#output-files","title":"Output Files","text":"<p>Each of the subjobs in the array will output its own file of the form <code>slurm-&lt;job_id&gt;_&lt;array_id&gt;.out</code> as seen below:</p> <pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1694071_1.out  slurm-1694071_2.out  slurm-1694071_3.out\nslurm-1694071_4.out\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#file-contents","title":"File Contents:","text":"<pre><code>(puma) [netid@junonia ~]$ cat *.out | grep fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694072, EXAMPLE COMMAND: ./executable -o output1 SRR2309587.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694073, EXAMPLE COMMAND: ./executable -o output2 SRR3050489.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694074, EXAMPLE COMMAND: ./executable -o output3 SRR305356.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694071, EXAMPLE COMMAND: ./executable -o output4 SRR305p0982.fastq\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/","title":"Array Jobs With Multiple Input Parameters","text":"<p>Click here to download example files</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#example","title":"Example","text":"<p>This script demonstrates how to feed parameters to different subjobs in an array by pulling them from an input file, e.g.:</p> <ul> <li>Job 1: <code>./executable job1_variable1 job1_variable2 job1_variable3</code></li> <li>Job 2: <code>./executable job2_variable1 job2_variable2 job2_variable3</code> etc.</li> </ul>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#submission-script","title":"Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Array-Read-Parameters\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-10\n\nread first_parameter second_parameter third_parameter &lt;&lt;&lt; $( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputParameters )\necho \"Job ID: $SLURM_JOB_ID ; Host Node : $HOSTNAME ; Sample Command : ./executable $first_parameter $second_parameter $third_parameter\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#input-file","title":"Input file","text":"<pre><code>job1_param1 job1_param2 job1_param3\njob2_param1 job2_param2 job2_param3\njob3_param1 job3_param2 job3_param3\njob4_param1 job4_param2 job4_param3\njob5_param1 job5_param2 job5_param3\njob6_param1 job6_param2 job6_param3\njob7_param1 job7_param2 job7_param3\njob8_param1 job8_param2 job8_param3\njob9_param1 job9_param2 job9_param3\njob10_param1 job10_param2 job10_param3\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#script-breakdown","title":"Script Breakdown","text":"<p>The line number that corresponds with the job's <code>SLURM_ARRAY_TASK_ID</code> is read in and parsed to extract the input parameters. The parameters here should be space-delimited (of course, you can modify your script to change these specifications). There are three parameters per line that are assigned to the variables on the left:</p> <pre><code>read first_parameter second_parameter third_parameter &lt;&lt;&lt; $( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputParameters )\n</code></pre> <p>A sample command is printed along with job information for demonstration purposes:</p> <pre><code>echo \"Job ID: $SLURM_JOB_ID ; Host Node : $HOSTNAME ; Sample Command : ./executable $first_parameter $second_parameter $third_parameter\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#script-submission-command","title":"Script Submission Command","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-Read-Parameters.slurm \nSubmitted batch job 1694093\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#output-files","title":"Output Files","text":"<pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1694093_10.out  slurm-1694093_1.out  slurm-1694093_2.out\nslurm-1694093_3.out   slurm-1694093_4.out  slurm-1694093_5.out\nslurm-1694093_6.out   slurm-1694093_7.out  slurm-1694093_8.out\nslurm-1694093_9.out\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#file-contents","title":"File Contents","text":"<pre><code>(puma) [netid@junonia ~]$ cat *.out | grep param\nJob ID: 1694093 ; Host Node : r1u03n2 ; Sample Command : ./executable job10_param1 job10_param2 job10_param3\nJob ID: 1694094 ; Host Node : r1u03n1 ; Sample Command : ./executable job1_param1 job1_param2 job1_param3\nJob ID: 1694095 ; Host Node : r1u03n1 ; Sample Command : ./executable job2_param1 job2_param2 job2_param3\nJob ID: 1694096 ; Host Node : r1u03n1 ; Sample Command : ./executable job3_param1 job3_param2 job3_param3\nJob ID: 1694097 ; Host Node : r1u03n1 ; Sample Command : ./executable job4_param1 job4_param2 job4_param3\nJob ID: 1694098 ; Host Node : r1u03n1 ; Sample Command : ./executable job5_param1 job5_param2 job5_param3\nJob ID: 1694099 ; Host Node : r1u03n1 ; Sample Command : ./executable job6_param1 job6_param2 job6_param3\nJob ID: 1694100 ; Host Node : r1u03n1 ; Sample Command : ./executable job7_param1 job7_param2 job7_param3\nJob ID: 1694101 ; Host Node : r1u03n2 ; Sample Command : ./executable job8_param1 job8_param2 job8_param3\nJob ID: 1694102 ; Host Node : r1u03n2 ; Sample Command : ./executable job9_param1 job9_param2 job9_param3\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/","title":"Basic Array Job","text":"<p>Click to download example files</p> <p>Array jobs are used to execute the same script multiple times with different input.</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>To execute multiple analyses, a user may be tempted to submit jobs with a scripted loop, e.g.: <pre><code>for i in $( seq 1 10 ); do sbatch script.slurm &lt;submission options&gt; ; done\n</code></pre></p> <p>For a large number of analyses, this isn't a good solution because it submits too many jobs too quickly and overloads the scheduler. Instead, an array job can be used to achieve the same ends.</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#example-submission-script","title":"Example Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array 1-5\n\necho \"./sample_command input_file_${SLURM_ARRAY_TASK_ID}.in\"\n ```\n\n## Script Breakdown\n\nWhat differentiates the script above from standard submissions is the ```--array``` directive. This is what tells Slurm  that you're submitting an array. Following this flag, you will specify the number of jobs you wish to run. In this case, we're running 5:\n\n```bash\n#SBATCH --array 1-5\n</code></pre> <p>Each job in the array has its own associated environment variable <code>SLURM_ARRARY_TASK_ID</code> that can be used to differentiate subjobs. To demonstrate how we can use each of these to read in different input files, we'll print a sample command:</p> <pre><code>echo \"./sample_command input_file_${SLURM_ARRAY_TASK_ID}.in\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#script-submission","title":"Script Submission","text":"<pre><code>(ocelote) [netid@junonia ~]$ sbatch basic_array_job.slurm \nSubmitted batch job 73958\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#output-files","title":"Output Files","text":"<p>Each of the subjobs in the array will produce its own output file of the form <code>slurm_jobid_arrayid.out</code> as seen below:</p> <pre><code>(ocelote) [netid@junonia ~]$ ls\nslurm-73958_1.out  slurm-73958_2.out      slurm-73958_3.out  slurm-73958_4.out\nslurm-73958_5.out  basic_array_job.slurm\n</code></pre> <p>For more information on naming Bash files, see our online documentation</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#file-contents","title":"File Contents:","text":"<p>Below is a concatenation of the job's output files. Notice how the array indices function to differentiate the input files in the sample command:</p> <pre><code>(ocelote) [netid@junonia ~]$ cat slurm-73958_* | grep sample\n./sample_command input_file_1.in\n./sample_command input_file_2.in\n./sample_command input_file_3.in\n./sample_command input_file_4.in\n./sample_command input_file_5.in\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/basic_gnu_parallel_job/","title":"Basic Parallel Job","text":"<p>Click here to download example files</p> <p>This example demonstrates how to parallelize multiple tasks within one job using gnu parallel</p>"},{"location":"running_jobs/example_batch_jobs/basic_gnu_parallel_job/#example","title":"Example","text":"<pre><code>#!/bin/bash\n#SBATCH --ntasks=28\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n\nmodule load parallel\nseq 1 100 | parallel 'DATE=$( date +\"%T\" ) &amp;&amp; sleep 0.{} &amp;&amp; echo \"Host: $HOSTNAME ; Date: $DATE; {}\"'\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/basic_gnu_parallel_job/#script-breakdown","title":"Script Breakdown","text":"<p>In this case, we'll make use of a full node with GNU Parallel which is available as a module:</p> <pre><code>module load parallel\n</code></pre> <p>The meat of the command lies here:</p> <pre><code>seq 1 100 | parallel 'DATE=$( date +\"%T\" ) &amp;&amp; sleep 0.{} &amp;&amp; echo \"Host: $HOSTNAME ; Date: $DATE; {}\"'\n</code></pre> <p><code>seq 1 100</code> generates a list between 1 and 100 (inclusive), and we pipe that into a parallel command which will generate one task per element (so 100 tasks).</p> <p>GNU Parallel will find the space on our node as it works through the relevant tasks. </p> <p>Inside the command: <code>DATE=$( date + \"%T\" )</code> sets <code>DATE</code> so we can visualize tasks and when they're being executed</p> <p><code>sleep 0.{}</code> forces each task to sleep for <code>0.n</code> seconds, where <code>n</code> is the input integer from the <code>seq</code> command. This means, for example, the 2nd task will wait longer than the 10th task, as can be seen in the output file. This is used to demonstrate that these tasks are being executed in parallel. </p> <p><code>echo \"HOST: $HOSTNAME ; Date: $DATE; {}\"</code> prints out information about the task. <code>{}</code> is piped input which, in this case, is an integer generated by <code>seq</code> between 1 and 100. </p>"},{"location":"running_jobs/example_batch_jobs/basic_gnu_parallel_job/#submitting-the-script","title":"Submitting the Script","text":"<pre><code> (ocelote) [netid@junonia ~]$ sbatch basic-parallel-job.slurm \nSubmitted batch job 74027\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/basic_gnu_parallel_job/#output-files","title":"Output Files","text":"<p>Since this isn't an array job, there will only be one output file:</p> <pre><code>(ocelote) [netid@junonia ~]$ ls\nslurm-74027.out  basic-parallel-job.slurm\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/basic_gnu_parallel_job/#file-contents","title":"File Contents","text":"<p>For the full file contents, download the example above <pre><code>(ocelote) [netid@junonia ~]$ head slurm-74027.out \nHost: i10n18 ; Date: 16:45:55; 1\nHost: i10n18 ; Date: 16:45:55; 10\nHost: i10n18 ; Date: 16:45:55; 11\nHost: i10n18 ; Date: 16:45:55; 12\nHost: i10n18 ; Date: 16:45:55; 13\nHost: i10n18 ; Date: 16:45:55; 14\nHost: i10n18 ; Date: 16:45:55; 15\nHost: i10n18 ; Date: 16:45:55; 16\nHost: i10n18 ; Date: 16:45:55; 17\nHost: i10n18 ; Date: 16:45:55; 2\n</code></pre></p>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/","title":"SLURM Job Dependencies Example","text":"<p>Click here to download the example</p>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#overview","title":"Overview","text":"<p>Sometimes projects need to be split up into multiple parts where each step is dependent on the step (or several steps) that came before. SLURM dependencies are a way to automate this process. </p> <p>In this example, we'll create a number of three-dimensional plots using Python and will combine them into a gif as the last step. A job dependency is a good solution in this case since the job that creates the gif is dependent on all the images being present.</p>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#data-structure","title":"Data structure","text":"<p>We'll try to keep things in order by partitioning our data, output, and images in distinct directories. These directories and files can be downloaded by clicking the button at the top of the page.</p> <pre><code>(elgato) [user@wentletrap volcano]$ tree\n.\n\u251c\u2500\u2500 create_gif.slurm\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.csv\n\u251c\u2500\u2500 generate_frames.slurm\n\u251c\u2500\u2500 images\n\u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 archives\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurm_files\n\u251c\u2500\u2500 submit-gif-job\n\u2514\u2500\u2500 volcano.py\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#scripts","title":"Scripts","text":""},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#python-script","title":"Python script","text":"<p>The Python example script was pulled and modified from the Python graph gallery and the CSV file used to generate the image was downloaded from: https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv</p> <p>Below you'll notice one modification to the original script: <code>n = int(sys.argv[1])</code>. We're going to execute this in an array job and will be importing the array indices (integers <code>n</code> where <code>1 \u2264 n \u2264 360</code>) into this script as arguments so that we can manipulate the viewing angle (<code>ax.view_init(30, 45 + n)</code>). Each frame will be slightly different and, when combined into a gif, will allow us to execute a full rotation of the 3D volcano plot. </p> <pre><code>#!/usr/bin/env python3\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport sys\n\nn = int(sys.argv[1]) # &lt;- We'll import an array index to rotate our image\n\n# Original example from: https://www.python-graph-gallery.com/3d/\n# CSV available from   : https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv\ndata = pd.read_csv(\"data/volcano.csv\")\n\n# Transform it to a long format\ndf=data.unstack().reset_index()\ndf.columns=[\"X\",\"Y\",\"Z\"]\n\n# And transform the old column name in something numeric\ndf['X']=pd.Categorical(df['X'])\ndf['X']=df['X'].cat.codes\n\n# Make the plot\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.set_axis_off()\nax.plot_trisurf(df['Y'], df['X'], df['Z'], cmap=plt.cm.viridis, linewidth=0.2)\nax.view_init(30, 45 + n)\nplt.savefig('images/image%s.png'%n,format='png',transparent=False)\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#slurm-script-to-generate-images","title":"Slurm Script to Generate Images","text":"<p>This is the job where we will generate all of our images. In each step, we will pass our array index to our python script to determine the viewing angle of our plot. </p> <p>Script: <code>generate_frames.slurm</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:05:00\n#SBATCH --job-name=generate_frames\n#SBATCH -o output/slurm_files/%x-%A.out\n#SBATCH -e output/slurm_files/%x-%A.err\n#SBATCH --open-mode=append\n#SBATCH --array=1-360\n\npython3 volcano.py $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#slurm-script-to-combine-frames-into-gif","title":"SLURM script to combine frames into gif","text":"<p>Once each frame has been generated, we'll use ffmpeg to combine our images into a gif and will clean up our workspace. The bash script shown in the next section is what will ensure that this script isn't run until the array has completed. </p> <p>Script: <code>create_gif.slurm</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:10:00\n#SBATCH --job-name=make_gif\n#SBATCH -o output/slurm_files/%x-%j.out\n#SBATCH -e output/slurm_files/%x-%j.err\n\nmodule load ffmpeg \nffmpeg -framerate 25 -i $PWD/images/image%d.png -r 30  -b 5000k volcano.mp4\nffmpeg -i volcano.mp4 -loop 0 -vf scale=400:240 volcano.gif\nrm volcano.mp4\ncd images\nDATE_FORMAT=$(date +%m-%d-%Y.%H:%M:%S)\ntar czvf volcano-images-${DATE_FORMAT}.tar.gz image*.png\nmv *tar.gz ../output/archives\nrm -rf ./*.png\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#script-to-automate-job-submissions","title":"Script to Automate Job Submissions","text":"<p>This simple bash script is what implements the SLURM job dependency magic. Each step is described in detail below.</p> <p>Script: <code>submit-gif-job</code></p> <pre><code>#!/bin/bash\n\nprintf \"Submitting job to generate images\\n\"\njobid=$(sbatch --parsable generate_frames.slurm)\n\nprintf \"Job submitted with ID: $jobid\\n\\n\"\n\nprintf \"Submitting job dependency. Combines images into a gif\\n\"\nsbatch --dependency=afterany:$jobid create_gif.slurm \n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#step-by-step","title":"Step-by-step:","text":"<p>1) <code>jobid=$(sbatch --parsable generate_frames.slurm)</code></p> <p>In this case, we're capturing the job ID output from our array submission. Typically, when you submit a SLURM job without arguments, you get back something that looks like:  <pre><code>(elgato) [user@wentletrap ~]$ sbatch example.slurm \nSubmitted batch job 448243\n</code></pre> The parsable option is what reduces this to simply the job ID and allows us to easily capture it: <pre><code>(elgato) [user@wentletrap ~]$ sbatch --parsable example.slurm \n448244\n</code></pre> As a general comment, when you run something like: <pre><code>VAR=$(command)\n</code></pre> You are running <code>command</code> and setting the variable <code>VAR</code> to the output. In the specifc case of our bash script, we've set the bash variable <code>jobid</code> to the output of our <code>sbatch --parsable</code> command. </p> <p>2) <code>sbatch --dependency=afterany:$jobid create_gif.slurm</code></p> <p>Now that we have the Job ID, we'll submit the next job with a dependency flag: <code>--dependency=afterany:$jobid</code>. </p> <p>The <code>dependency</code> option tells the scheduler that this job should not be run until the job with Job ID <code>$jobid</code> has completed. The <code>afterany</code> specifies that the exit status of the previous job does not matter. Other options are <code>afterok</code> (meaning only execute the dependent job if the previous job ended successfully) or <code>afternotok</code> (meaning only execute if the previous job terminated abnormally, e.g. was cancelled or failed). You might consider setting up multiple job dependencies that depend on the previous job's exit status. </p>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#submitting-the-jobs","title":"Submitting the Jobs","text":"<p>Once we've gotten everything set up, it's time to execute our workflow. We can check our jobs once we've run our bash script. In this case, while the array job used to generate the different image frames is running, the <code>make_gif</code> job will sit in queue with the reason <code>(Dependency)</code> indicating that it is waiting to run until its dependency has been satisfied. </p> <pre><code>(elgato) [user@wentletrap volcano]$ bash submit-gif-job \nSubmitting job to generate images\nJob submitted with ID: 447878\n\nSubmitting job dependency. Combines images into a gif file\nSubmitted batch job 447879\n\n(elgato) [user@wentletrap volcano]$ squeue --user user\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n    447878_[9-360]  standard generate     user PD       0:00      1 (None)\n            447879  standard make_gif     user PD       0:00      1 (Dependency)\n          447878_1  standard generate     user  R       0:02      1 cpu16\n          447878_2  standard generate     user  R       0:02      1 cpu37\n          447878_3  standard generate     user  R       0:02      1 cpu37\n          447878_4  standard generate     user  R       0:02      1 cpu37\n          447878_5  standard generate     user  R       0:02      1 cpu37\n          447878_6  standard generate     user  R       0:02      1 cpu37\n          447878_7  standard generate     user  R       0:02      1 cpu37\n          447878_8  standard generate     user  R       0:02      1 cpu37\n</code></pre> <p>Once the job has completed, you should see something that looks like the following structure with output files: <pre><code>(elgato) [user@wentletrap volcano]$ tree\n.\n\u251c\u2500\u2500 create_gif.slurm\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.csv\n\u251c\u2500\u2500 generate_frames.slurm\n\u251c\u2500\u2500 images\n\u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 archives\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano-images-10-25-2022.12:52:19.tar.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.gif\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurm_files\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 generate_frames-447878.err\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 generate_frames-447878.out\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 make_gif-447879.err\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 make_gif-447879.out\n\u251c\u2500\u2500 submit-gif-job\n\u2514\u2500\u2500 volcano.py\n\n6 directories, 11 files\n</code></pre></p>"},{"location":"running_jobs/example_batch_jobs/general_examples/job_dependencies/#output","title":"Output","text":"<p>If everything is successful, there should be a gif of a rotating volcano under <code>./output/gifs/volcano.gif</code></p> <p></p> <p> </p>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/","title":"Using a Pipefail","text":"<p>This script uses a pipefail to kill a SLURM job in the event of a failure at any point in the pipeline rather than continuing on to the next step.</p>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/#example-submission-script","title":"Example Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --account=YOUR_GROUP\n#SBATCH --partition=standard\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --time=00:01:00\n\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import foo\"\n\nset -oe pipefail # &lt;- kills the batch script on the next error it encounters\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import bar\"\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import baz\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/#job-submission","title":"Job Submission","text":"<pre><code>[user@cpu37 fail_test]$ sbatch pipefail.slurm\nSubmitted batch job 361837\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/#output","title":"Output","text":"<p>Notice that before the pipefail is set, the script moves on to subsequent commands (trying to import bar). After the pipefail is set, the job exits when it can't import bar and never tries to import baz. </p> <pre><code>[user@cpu37 fail_test]$ cat slurm-361837.out\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'foo'\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'bar'\n</code></pre>"},{"location":"running_jobs/interactive_jobs/","title":"Interactive Jobs","text":""},{"location":"running_jobs/interactive_jobs/#overview","title":"Overview","text":"<p>Interactive sessions are a way to gain access to a compute node from the command line. This is useful for checking and using available software modules, testing submission scripts, debugging code, compiling software, and running programs in real time. </p> <p>The term \"interactive session\" in this context refers to jobs run from within the command line on a terminal client. Opening a terminal in an interactive graphical desktop is also equivalent, but these sessions are fixed to the resources allocated to that OOD session. As you'll see below, one has more control over their resources when requesting an interactive session via SSH in a terminal client.</p>"},{"location":"running_jobs/interactive_jobs/#clusters","title":"Clusters","text":"<p>An interactive session can be requested on any of our three clusters: El Gato, Ocelote, and Puma. Since the request to start an interactive session is processed by Slurm, these jobs will be subject to the same wait times as batch jobs. Since Puma is typically busy with high traffic throughput, it is not recommended to request an interactive session on this cluster unless specific resources are needed and longer wait times are acceptable to the user. </p>"},{"location":"running_jobs/interactive_jobs/#how-to-request-an-interactive-session","title":"How to Request an Interactive Session","text":""},{"location":"running_jobs/interactive_jobs/#the-interactive-command","title":"The <code>interactive</code> Command","text":"<p>We have a built-in shortcut command that will allow you to quickly and easily request a session by simply entering: <code>interactive</code></p> <p>The <code>interactive</code> command is essentially a convenient wrapper for a Slurm command called <code>salloc</code>. This can be thought of as similar to the <code>sbatch</code> command, but for interactive jobs rather than batch jobs. When you request a session using interactive, the full <code>salloc</code> command being executed will be displayed for reference.</p> <pre><code>(ocelote) [netid@junonia ~]$ interactive\nRun \"interactive -h for help customizing interactive use\"\nSubmitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=1 --time=01:00:00 --account=windfall --partition=windfall\nsalloc: Pending job allocation 531843\nsalloc: job 531843 queued and waiting for resources\nsalloc: job 531843 has been allocated resources\nsalloc: Granted job allocation 531843\nsalloc: Waiting for resource configuration\nsalloc: Nodes i16n1 are ready for job\n[netid@i16n1 ~]$\n</code></pre> <p>Notice in the example above how the command prompt changes once your session starts. When you're on a login node, your prompt will show <code>junonia</code> or <code>wentletrap</code>. Once you're in an interactive session, you'll see the name of the compute node you're connected to. </p> <p>Customizing Your Resources</p> <p>The command <code>interactive</code> when run without any arguments will allocate a windfall session using one CPU for one hour which isn't ideal for most use cases. You can modify this by including additional flags. To see the available options, you can use the help flag <code>-h</code></p> <p><pre><code>(ocelote) [netid@junonia ~]$ interactive -h\nUsage: /usr/local/bin/interactive [-x] [-g] [-N nodes] [-m memory per core] [-n total number of tasks] [-Q optional qos] [-t hh::mm:ss] [-a account to charge]\n</code></pre> The values shown in the output can be combined and each mean the following:</p> Flag Explanation Example <code>-a</code> This is followed by your group's name will switch you to using the standard partition. This is highly recommended to keep your sessions from being interrupted and to help them start faster <code>-a my_group</code> <code>-t</code> The amount of time to reserve for your job in the format <code>hhh:mm:ss</code> <code>-t 05:00:00</code> <code>-n</code> Total number of tasks (CPUs) to allocate to your job. By default, this will be on a single node <code>-n 16</code> <code>-N</code> Total number of nodes (physical computers) to allocate to your job <code>-N 2</code> <code>-m</code> Total amount of memory per CPU. See CPUs and Memory for more information and potential complications <code>-m 5gb</code> <code>-Q</code> Used to access high priority or qualified hours. Only for groups with buy-in/special project hours High Priority: <code>-Q user_qos_&lt;PI NETID&gt;</code>Qualified: <code>-Q qual_qos_&lt;PI NETID&gt;</code> <code>-g</code> Request one GPU. This flag takes no arguments. On Puma, you may be allocated either a v100 or a MIG slice. If you want more control over your resources, you can use <code>salloc</code> directly using GPU batch directives <code>-q</code> <code>-x</code> Enable X11 forwarding. This flag takes no arguments. <code>-x</code> <p>You may also create your own salloc commands using any desired Slurm directives for maximum customization.</p>"},{"location":"running_jobs/interactive_jobs/#the-salloc-command","title":"The <code>salloc</code> Command","text":"<p>If <code>interactive</code> is insufficient to meet you resource requirements (e.g., if you need to request more than one GPU or a GPU MIG slice), you can use the Slurm command <code>salloc</code> to further customize your job. </p> <p>The command <code>salloc</code> expects Slurm directives as input arguments that it uses to customize your interactive session. For comprehensive documentation on using <code>salloc</code>, see Slurm's official documentation.</p> <p>Single CPU Example</p> <p><pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=interactive\n</code></pre> Single Node, Multi-CPU Example</p> <pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=standard --nodes=1 --ntasks=16 --time=1:00:00 --job-name=interactive\n</code></pre> <p>Multi-GPU Example (Puma) <pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=multi-gpu --gres=gpu:volta:2\n</code></pre></p> <p>GPU MIG Slice Example <pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=mig-gpu --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb\n</code></pre></p>"},{"location":"running_jobs/job_limits/","title":"Job Limits","text":"<p>There are two main types of limits imposed on all jobs: (1) those due to hardware, and (2) those due to policy.</p>"},{"location":"running_jobs/job_limits/#hardware-limits","title":"Hardware Limits","text":"<p>See our page on Compute Resources for more information. Note that each cluster has:</p> <ul> <li>a fixed number of nodes</li> <li>a fixed number of CPUs per node</li> <li>a limited number of GPU nodes</li> <li>a limited number of high-memory nodes</li> <li>a fixed amount of memory (RAM) available per CPU</li> </ul>"},{"location":"running_jobs/job_limits/#policy-limits","title":"Policy Limits","text":"<p>To ensure fair use of the system for all researchers, limits have been placed on the maximum simultaneous usage of resources on a per-group, per-user, and per-job basis. These limits can be listed by using the <code>job-limits</code> command in a terminal session. For convenience, the output of this command for each cluster is listed below. Note that you will see the current usage of your group and personal jobs, so the output may vary.</p> PumaOceloteEl Gato <pre><code>                              Group Limits: &lt;your-group&gt;\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |   Job Number\n         | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit\n--------------------------------------------------------------------------------\nStandard |    -/16998G    |     -/3290     |    -/gres/gpu=4    |      -/-\n--------------------------------------------------------------------------------\n\n\n                              User Limits: &lt;your-net-id&gt;\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |  Job Number*\n         | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit\n--------------------------------------------------------------------------------\nWindfall |      -/-       |     -/6000     |        -/-         |     -/1000\nStandard |    -/16998G    |     -/3290     |    -/gres/gpu=4    |\n--------------------------------------------------------------------------------\n*Max jobs across all groups and partitions.\n\n\n                             Individual Job Limits\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |      Time\n--------------------------------------------------------------------------------\nWindfall |     16998G     |      6000      |                    |  10-00:00:00\nStandard |     16998G     |      3290      |     gres/gpu=4     |  10-00:00:00\n--------------------------------------------------------------------------------\n</code></pre> <pre><code>                              Group Limits: &lt;your-group&gt;\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |   Job Number\n         | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit\n--------------------------------------------------------------------------------\nStandard |     -/10T      |     -/1024     |   -/gres/gpu=10    |      -/-\n--------------------------------------------------------------------------------\n\n\n                              User Limits: &lt;your-net-id&gt;\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |  Job Number*\n         | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit\n--------------------------------------------------------------------------------\nWindfall |      -/-       |     -/6000     |        -/-         |     -/1000\nStandard |     -/10T      |     -/1024     |   -/gres/gpu=10    |\n--------------------------------------------------------------------------------\n*Max jobs across all groups and partitions.\n\n\n                             Individual Job Limits\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |      Time\n--------------------------------------------------------------------------------\nWindfall |     8064G      |      6000      |                    |  10-00:00:00\nStandard |     8064G      |      1024      |    gres/gpu=10     |  10-00:00:00\n--------------------------------------------------------------------------------\n</code></pre> <pre><code>                              Group Limits: &lt;your-group&gt;\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |   Job Number\n         | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit\n--------------------------------------------------------------------------------\nStandard |     -/10T      |     -/1024     |        -/-         |      -/-\n--------------------------------------------------------------------------------\n\n\n                              User Limits: &lt;your-net-id&gt;\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |  Job Number*\n         | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit\n--------------------------------------------------------------------------------\nWindfall |      -/-       |     -/6000     |        -/-         |     -/1000\nStandard |     -/10T      |     -/1024     |        -/-         |\n--------------------------------------------------------------------------------\n*Max jobs across all groups and partitions.\n\n\n                             Individual Job Limits\n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |      Time\n--------------------------------------------------------------------------------\nWindfall |     8064G      |      6000      |                    |  10-00:00:00\nStandard |     8064G      |      1024      |                    |  10-00:00:00\n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"running_jobs/monitoring_jobs_and_resources/","title":"Monitoring Jobs and Resources","text":""},{"location":"running_jobs/monitoring_jobs_and_resources/#monitoring-jobs","title":"Monitoring Jobs","text":"<p>Tip</p> <p>Some of these functions are specific to the UA HPC, and may not work if invoked on other systems. </p> <p>Every job has a unique ID associated with it that can be used to track its status, view resource allocations, and resource usage. Below is a list of some helpful commands you can use for job monitoring.</p> Command Purpose Example <code>squeue --job=&lt;jobid&gt;</code> Retrieves a running or pending job's status. <code>squeue --job=12345</code> <code>squeue --me</code> Retrieves all your running and pending jobs <code>scontrol show jobs &lt;jobid&gt;</code> Retrieve detailed information on a running or pending job <code>scontrol show job 12345</code> <code>scancel &lt;jobid&gt;</code> Cancel a running or pending job <code>scancel 12345</code> <code>job-history &lt;jobid&gt;</code> Retrieves a running or completed job's history in a user-friendly format <code>job-history 12345</code> <code>seff &lt;jobid&gt;</code> Retrieves a completed job's memory and CPU efficiency <code>seff 12345</code> <code>past-jobs</code> Retrieves past jobs run by user. Can be used with option <code>-d &lt;n&gt;</code> to search for jobs run in the past <code>&lt;n&gt;</code> days <code>past-jobs -d 5</code> <code>job-limits &lt;group_name&gt;</code> View your group's job resource limits and current usage. <code>job-limits mygroup</code>"},{"location":"running_jobs/monitoring_jobs_and_resources/#slurm-reason-codes","title":"Slurm Reason Codes","text":"<p>Sometimes, if you check a pending job there is a message under the field <code>Reason</code> indicating why your job may not be running. Some of these codes are non-intuitive so a human-readable translation is provided below:</p> Reason Explanation <code>AssocGrpCpuLimit</code> Your job is not running because your group CPU limit has been reached<sup>1</sup> <code>AssocGrpMemLimit</code> Your job is not running because your group memory limit has been reached<sup>1</sup> <code>AssocGrpCPUMinutesLimit</code> Either your group is out of CPU hours or your job will exhaust your group's CPU hours. <code>AssocGrpGRES</code> Your job is not running because your group GPU limit has been reached<sup>1</sup> <code>Dependency</code> Your job depends on the completion of another job. It will wait in queue until the target job completes. <code>QOSGrpCPUMinutesLimit</code> This message indicates that your high priority or qualified hours allocation has been exhausted for the month. <code>QOSMaxWallDurationPerJobLimit</code> Your job's time limit exceeds the max allowable and will never run<sup>1</sup> <code>Nodes_required_for_job_are_DOWN,_DRAINED_or_reserved_or_jobs_in_higher_priority_partitions</code> This very long message simply means your job is waiting in queue until there is enough space for it to run <code>Priority</code> Your job is waiting in queue until there is enough space for it to run. <code>QOSMaxCpuPerUserLimit</code> Your job is not running because your per-user CPU limit has been reached<sup>1</sup> <code>ReqNodeNotAvail, Reserved for maintenance</code> Your job's time limit overlaps with an upcoming maintenance window. Run \"uptime_remaining\" to see when the system will go offline. If you remove and resubmit your job with a shorter walltime that does not overlap with maintenance, it will likely run. Otherwise, it will remain pending until after the maintenance window. <code>Resources</code> Your job is waiting in queue until the required resources are available."},{"location":"running_jobs/monitoring_jobs_and_resources/#monitoring-system-resources","title":"Monitoring System Resources","text":"<p>We have a number of system commands that can be used to view the availability of resources on each cluster. This may be helpful in determining which cluster to use for your analyses</p> Command Purpose <code>nodes-busy</code> Display a visualization of each node on a cluster and overall usage. Use <code>nodes-busy --help</code> for more detailed options. <code>cluster-busy</code> Display a visual overview of each cluster's usage <code>system-busy</code> Display a text-based summary of a cluster's usage <ol> <li> <p>For more information on resource limitations, see: Job Limits.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"running_jobs/open_on_demand/","title":"Open OnDemand","text":"<p>Open OnDemand (OOD), which is an NSF-funded open-source HPC portal, is available for users and provides web browser access for graphically interfacing with HPC. This service is available at https://ood.hpc.arizona.edu/. </p>"},{"location":"running_jobs/open_on_demand/#why-use-open-ondemand","title":"Why use Open OnDemand?","text":"<p>Since there are many ways to access HPC, including the command line terminal, why use Open OnDemand? Here are a few of the main reasons:</p> <ul> <li> <p>GUI software available</p> <p>Many commonly used research software packages, such as RStudio and ANSYS, have graphical interfaces that streamline analysis workflows. Open OnDemand allows for easy access to these applications without the complications of server setup or image/port forwarding. </p> </li> <li> <p>User friendly </p> <p>Open OnDemand includes an interactive desktop application which mimics what you might find on your local workstation. This environment may be more intuitive to navigate when getting familiarized with the HPC.</p> </li> <li> <p>Standardized batch access </p> <p>Open OnDemand includes forms to submit batch jobs which include all the relevant parameters in one place. </p> </li> </ul>"},{"location":"running_jobs/open_on_demand/#list-of-available-features-in-open-ondemand","title":"List of Available Features in Open OnDemand","text":"Basic Functions GUI Software Servers Batch/Slurm File Browser Abaqus Jupyter Notebook Job Composer Interactive Desktop ANSYS R Studio Job Viewer Mathematica Shell MATLAB Stata VS Code"},{"location":"running_jobs/open_on_demand/#command-line-access","title":"Command Line Access","text":"<p>Need command line access to a terminal on HPC? No problem! Simply select the Clusters dropdown menu to connect to one of HPC's login nodes. This is also detailed under System Access</p> <p></p>"},{"location":"running_jobs/open_on_demand/#file-browser","title":"File Browser","text":"<p>The file browser provides easy access to your <code>/home</code>, <code>/xdisk</code>, and <code>/groups</code> directories and allows you to view, edit, copy, and rename your files. You may also transfer small files (under 64 MB) between HPC and your local workstation using this interface. For larger transfers, see our section on Transferring Data for more efficient methods. </p> <p>Access</p> <p>In the browser at the top of the screen, select the Files dropdown</p> <p></p> <p>You will be able to select your <code>/home</code> directory, <code>/groups</code>, or <code>/xdisk</code>. If you select <code>/groups</code> or <code>/xdisk</code>, enter your PI's NetID in the Filter field to find your shared group space.</p> <p></p> <p>Editing Files</p> <p>First, navigate to the file you wish to edit. Then, click the vertical ellipses on the right-hand side and select Edit</p> <p></p> <p>This will open a file editor in your browser where you may select your color theme, text size, and syntax highlighting.</p> <p></p>"},{"location":"running_jobs/open_on_demand/#job-viewer-and-composer","title":"Job Viewer and Composer","text":"<p>Job Viewer</p> <p>The Job Viewer allows you to check the status and time remaining of your running jobs. You can also cancel your jobs using this interface. Note: be careful looking at All Jobs since this will likely timeout trying to organize them all. To use the Job Viewer, navigate to the Jobs dropdown and select Active Jobs</p> <p></p> <p>This will open a new page listing all your running and pending jobs. You may delete them by clicking the red trash icon under Actions, or view more information about individual jobs using the dropdown on the left next to the ID.</p> <p></p> <p>Job Composer</p> <p>The Job Composer lets you create and run a Slurm script on any of our three clusters. It should be noted that the Job Composer creates a special string of directories in your <code>/home</code> starting with <code>ondemand/</code> which is where both your submission scripts and output files will be stored. Make note of the path to your files on the right-hand side of the Job Composer screen under Script location.</p> <p></p>"},{"location":"running_jobs/open_on_demand/#interactive-graphical-applications","title":"Interactive Graphical Applications","text":"<p>Open OnDemand provides access to graphical interfaces for some popular software. These can be found under Interactive Apps through the Open OnDemand web browser. The process of starting and accessing these jobs is the same regardless of which application you select.</p> <p>Web Form</p> <p></p> <p>First, select the desired application from Interactive Apps. This will take you to a form where you will enter your job information. This includes the entries in the following table:</p> Field Description Example Cluster Select which cluster to submit the job request to. Puma Run Time The maximum number of hours the job can run. Please note that the maximum possible run time is 10 days (240 hours). 4 Core Count on a single node The number of CPUs needed. This affects the amount of memory your job is allocated. The maximum that can be requested is dependent on which cluster you choose. 16 Memory per core The amount of memory needed per core in GB. The amount that can be requested is dependent on which cluster you choose and your desired node type. For more information, see our CPUs and Memory page. 5 GPUs required The number and type of GPUs needed for your job, if any. One A100 20GB GPU PI Group Your accounting group. If you do not know your group name, you can either check in the user portal, or can run <code>va</code> on the command line. If the group you entered does not exist, you will receive an error <code>sg: group 'group_name' does not exist</code> your-group Queue The queue, or partition, to use. Standard is the most common. If your group has buy-in hours, you may use High Priority. Standard <p>Once you've entered all your details, click Launch at the bottom of the page. This will take you to a tile with information about your job including job ID and session ID. This information can used for debugging purposes. </p> <p>When you first submit your job, it will show as having a status of \"Queued\". Once your job reaches the front of the queue, it will show a status of \"Starting\". When your session is ready, you can launch the application using Connect at the bottom of the tile.</p>"},{"location":"running_jobs/open_on_demand/#applications-available","title":"Applications Available","text":"Virtual DesktopJupyter NotebooksRstudioMatlabAnsysAbaqus <p>One nice feature of Open OnDemand is the ability to interact with HPC using a virtual Desktop environment. This provides a user-friendly way to run applications, perform file management, and navigate through your directories as though you were working with a local computer. Additionally, it eliminates the need to use X11 forwarding when working with GUI applications allowing an easy way to interact with software such as Matlab, VisIt, or Anaconda.</p> <p></p> <p>Tip</p> <p>To access your own python packages in Jupyter, you can create custom kernels either using a python module or using anaconda.</p> <p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.</p> <p>When you start a Jupyter notebook, by default your working directory will be your home. If you would like to change this so that your session starts in a different location, you'll need to add a line to the hidden file <code>~/.bashrc</code>. To do this, open your <code>~/.bashrc</code> in a text editor and add the following, substituting your desired path in for <code>&lt;/path/to/directory&gt;</code>:</p> <pre><code>export NOTEBOOK_ROOT=&lt;/path/to/directory&gt;\n</code></pre> <p></p> <p>RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. For an overview of the RStudio IDE, see: https://www.rstudio.com/products/RStudio/.</p> <p>For information on using R on HPC, see our online documentation on Using R Packages.</p> <p></p> <p>A GUI for multiple versions of Matlab is available. You can select which version to use in the web form when specifying your resources.</p> <p></p> <p>Multiple versions of the engineering application Ansys are available. You can specify which version to use in the web form when specifying your resources. To receive Ansys-specific support, see: Community and External Resources</p> <p></p> <p>A GUI for Abaqus is available.</p>"},{"location":"running_jobs/overview/","title":"Overview","text":"<p>HPC operates as a shared system with resources in high demand. Computational tasks must be executed as jobs on dedicated compute resources. These resources are granted to each user for a limited time per session, and sessions are organized by Slurm, an open source, fault-tolerant, and highly scalable cluster manager and task scheduler. Users can interact with Slurm from one of the login nodes to start an interactive job or submit a batch job. </p>"},{"location":"running_jobs/overview/#materials-to-get-started","title":"Materials to Get Started","text":"<p>If you are new to the UArizona HPC system, or to HPC systems in general, we recommend reviewing our quick start guide before getting into the details of running jobs. You may also want to take a look at our workshops which cover topics including introduction to HPC, parallel computing, containers, among other topics. </p>"},{"location":"running_jobs/overview/#best-practices","title":"Best Practices","text":"<p>Do not run jobs on Login Nodes</p> <p>A login node serves as a staging area where you can perform housekeeping work, edit scripts, and submit job requests for execution on one/some of the cluster\u2019s compute nodes. It is important to know that the login nodes are not the location where scripts are run. Heavy computation on the login nodes slows the system down for all users and will not give you the resources or performance you need. Additionally, modules are not available on the login nodes. </p> <p>Tasks run on the login nodes that impact usability will be identified and cancelled by HPC infrastructure without warning. </p> <p>When creating a job request, please keep the following in mind:</p> <ol> <li> <p>Don't ask for more resources than you really need.</p> <p>The scheduler will have an easier time finding a slot for the two hours you need rather than the 48 hours you request.  When you run a job it will report back on the time used which you can use as a reference for future jobs. However don't cut the time too tight.  If something like shared I/O activity slows it down and you run out of time, the job will fail.</p> <p>Additionally, please do not request more CPUs than you plan to use. Increasing the number of CPUs without a clear plan of how your software is going to use them will not result in faster computation. Unused CPUs in a job represent wasted system resources, and will cost more CPU-hours to your allocation than you actually needed to spend. </p> </li> <li> <p>Test your submission scripts.</p> <p>Start small. You can use an interactive session to help build your script and run tests in real time.</p> </li> <li> <p>Respect memory limits. </p> <p>If your application needs more memory than is available, your job could fail and leave the node in a state that requires manual intervention.</p> </li> <li> <p>Do not run loops automating a large number of job submissions. </p> <p>Executing large numbers of job submissions in rapid succession (e.g. in a scripted loop) can overload the system's scheduler and cause problems with overall system performance. Small numbers of automated jobs may be acceptable (e.g. less than 100), but a better alternative in almost all cases is to use job arrays instead.</p> </li> <li> <p>Hyperthreading is turned off. </p> <p>Running multiple threads per core is generally not productive.  MKL is an exception to that if it is relevant to your workflow. Instead, running one thread per core and using multiple cores (i.e. \"multiprocessing\" rather than \"multithreading\") is a suggested alternative.</p> </li> <li> <p>Open OnDemand Usage</p> <p>Please be mindful of other users' needs and avoid monopolizing resources for extended periods when they are not actively being utilized. In practice, this means actively terminating sessions when you are finished rather than leaving the session open. Closing the browser tab does not terminate the session. This ensures fair access for all members of our community and promotes a collaborative environment, and ensures you are only charged for the time you actually used.</p> </li> </ol>"},{"location":"running_jobs/overview/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Below is a FAQ that includes answers to common questions and misconceptions about running jobs on HPC. Some are general information about HPC systems, but some contain information specific to the UA HPC. We recommend reviewing this FAQ whether you are a new user getting started on our system, or an experienced user returning to our documentation.</p> What is a Job? <p>A single instance of allocated computing resources is known as a Job. Jobs can be in the format of a graphical Open OnDemand application instance, interactive terminal session, or batch submission (a script scheduled for  execution at a later time), and require an active allocation of CPU-time under an associated PI account. See Time Allocations for more information.      </p> How much am I charged for a job? <p>Each PI group is given a free monthly allocation of CPU hours on each cluster (see Time Allocations for more information). The amount charged to this allocation is equal to the number of CPUs used for a job multiplied by its total run time in hours.       </p> How many jobs can I run? <p>Each user can submit a maximum of 1000 jobs per cluster, and there are further limits on memory, CPUs, and GPUs concurrently used per group. More information is available at Job Limits. If you intend to submit a large number of jobs, especially if they are similar, they should be submitted as array jobs.      </p> How much memory can I request? <p>The total amount of memory assigned to a job depends on the number of CPUs assigned to a job. Memory is physically mounted to the CPUs, so there is a fixed amount available to the job equal to the memory of a single CPU multiplied by the number of CPUs.      </p> What compute resources are available on each cluster? <p>We have three clusters available for use: Puma, Ocelote, and El Gato. See Compute Resources for details on each.      </p> I submitted a job request, but it isn't running yet. Why? <p>Job requests go to our task scheduler, Slurm, which manages all incoming job requests and determines the optimal order to run them in order to maximize the efficiency of the system. The amount of wait time for your job depends on the number and size of jobs before your job in the queue; the amount of compute resources requested by your job; and the requested wall time for your job. Variation in wait times is a natural consequence of this system. Typically, our most advanced cluster, Puma, experiences longer wait times due to increased usage. Smaller or interactive jobs are recommended to run on Ocelote or El Gato.       </p> Why is my job running slower on the HPC than on my laptop?? <p> The most common reasons for this are (1) not requesting sufficient CPUs on the HPC, and (2) running software that is not utilizing those CPUs. Beyond these, the reason tends to be software-specific. A common misconception is that all software automatically utilizes every CPU that is allocated to a particular job. While some programs are configured to do this, many are not, especially when running scripting languages like Python. In these cases, the script will need to be updated with the relevant libraries in order to take advantage of parallelization. See Parallelization for more info.       </p> I need some software to run my analysis, can you install it for me? <p>First, check our list of installed software by accessing an interactive terminal session, then using module commands to check for your software. If you do not see your desired software package there, then there are two options.      <ol> <li> You can attempt to install the software for yourself in one of your own directories</li> <li> Or you can request HPC Consult to install the software as a system module.</li> </ol> </p> <p> Many programs can be installed with option 1 by users without root privileges, especially packages that are available via package managers like Pip, Conda, and CRAN. Most software like this does not make sense to install as a system module since it is generally easy to install on a per-user basis. Other software that is not available through package managers might be able to be installed by downloading and compiling the source code in one of your folders. The complexity of this process varies greatly from software to software, so if you run into any trouble while doing this, feel free to contact HPC Consulting for support.       </p> <p> For information on what's appropriate to install as a system module, see our software policies guide.       </p>"},{"location":"running_jobs/parallelization/","title":"Parallelization","text":""},{"location":"running_jobs/parallelization/#overview","title":"Overview","text":"<p>You might be surprised to learn that if you move code from a local computer to a supercomputer, it will not automatically run faster and may even run slower. This is because the power of a supercomputer comes from the volume of resources available (compute nodes, CPUs, GPUs, etc.) and not the clockspeed of the processors themselves. Performance boosts come from optimizing your code to make use of the additional processors available on HPC, a practice known as parallelization.</p> <p>Parallelization enables jobs to 'divide-and-conquer' independent tasks within a process when multiple threads are available. In practice, this typically means running a job with multiple CPUs on the HPC. On your local machine, running apps like your web browser is natively parallelized, meaning you don't have to worry about having so many tabs open. However, on the HPC, parallelization must almost always be explicitly configured and called from your job. This process is highly software-dependent, so please research the proper method for running your program of choice in parallel.</p>"},{"location":"running_jobs/parallelization/#scaling","title":"Scaling","text":"<p>A classic example of the advantages of parallel computing is multiplying an \\(N \\times N\\) matrix by a scalar, which takes \\(N^2\\) floating-point operations (flops). On one CPU, this will take an amount of time \\(t = N^2 / f\\) where \\(f\\) is the clock frequency of the CPU. On an integer \\(M\\) number of CPUs, this computation will instead take \\(t' = \\frac{t}{M}\\). Most analyses involve computations which are not always as independent as matrix multiplication, leading to less than perfect speedup times. </p> <p>Amdahl's Law (strong scaling)</p> <p>The behavior of predicted speedup time for an analysis of fixed size is known as Strong Scaling. It depends on the fraction of code in a particular program which is parallelizable. </p> <p>Gustafson's Law (weak scaling)</p> <p>When scaling up an analysis by a factor of N, and running it on N times as many processors, the theoretical limit is an equivalent runtime. In practice, the runtime is typically longer.</p> <p>Most advancements in research are made from increasing the individual problem size, rather than decreasing time to execution, and therefore benefits more from so-called \"weak\" scaling. </p>"},{"location":"running_jobs/parallelization/#single-versus-multi-node-parallelism","title":"Single versus Multi-Node Parallelism","text":"<p>Single Node</p> <p>Each node on HPC has multiple CPUs. These can be utilized simultaneously in shared-memory parallelism. </p> <p>Multi-Node</p> <p>Multiple nodes can be accessed simultaneously on HPC, but memory is distributed rather than shared. In this case, additional software is needed in order to facilitate communication between processes, such as OpenMPI or Intel MPI. </p> <p>Please be aware of the type of parallelism used in your program. Some software is configured only for shared-memory paralleism and will not be able to use processors on multiple nodes. </p>"},{"location":"running_jobs/parallelization/#implementation","title":"Implementation","text":"<p>Some software is configured to be parallel out of the box, and other software needs explicit configuration. Check with your provider to determine whether parallelism is available, and which kind. For example, Python is natively serial, but libraries are available to enable either shared-memory or distributed-memory parallelism.</p> <p>Providing a detailed guide on how to code parallel processing for all software available on the HPC is beyond the scope of this documentation. Instead, please refer to the following online guides:</p> <p>Python</p> <p><code>multiprocessing</code> - enables shared-memory parallelism. Reference. </p> <p><code>import mpi4py</code> - enables distributed-memory parallelism. Reference.</p> <p>R</p> <p>R provides the <code>parallel</code> library for multiprocessing. Reference.</p> <p>MATLAB</p> <p>MATLAB provides the Parallel Computing Toolbox. Reference.</p>"},{"location":"software/common_datasets/","title":"Common Datasets","text":"<p>We host several large community datasets.  It is beneficial to you and us.  For you, it saves all that time downloading and filling up your storage allocation.  And for us it reduces the occurrence of the same data in many places. We do not currently update them on any particular cadence. You can request updates if you feel those would be useful to the community.</p> <p>These datasets and databases are available on the compute nodes under <code>/contrib/datasets</code> in read-only mode. </p>"},{"location":"software/common_datasets/#alphafold-2","title":"AlphaFold 2","text":"<p>AlphaFold is an AI system developed by the Google DeepMind project to predict the 3D structure of a protein from its amino acid sequence. AlphaFold needs multiple datasets to run, the combined size of which is around 2.62 TB. You can find these datasets under <code>/contrib/datasets/alphafold</code>.</p> <p>We also host containers at <code>/contrib/singularity/alphafold</code> that you can use with the provided datasets to predict protein structures with AlphaFold. You can access the container by loading the alphafold module from an interactive session or a batch submission script. When you load the alphafold module, it defines the following additional environment variables that you can use to easily access the container and the datasets:</p> <ul> <li><code>ALPHAFOLD_DIR</code> which points to <code>/contrib/singularity/alphafold</code></li> <li><code>ALPHAFOLD_DATADIR</code> which points to <code>/contrib/datasets/alphafold</code></li> </ul> <p>The following batch submission script shows how you can use the container with the provided datasets.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=alphafold-run\n#SBATCH --time=04:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=standard\n#SBATCH --account=&lt;pi-account&gt;\n\n# Uncomment the following two lines to make predictions on proteins that would be too long to fit into GPU memory.\n# export APPTAINERENV_TF_FORCE_UNIFIED_MEMORY=1 \n# export APPTAINERENV_XLA_PYTHON_CLIENT_MEM_FRACTION=4.0\n\nmodule load alphafold\n\nalphafold --nv \\\n          --use_gpu_relax \\\n          --uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n          --uniref30_database_path=/data/uniref30/UniRef30_2021_03 \\\n          --mgnify_database_path=/data/mgnify/mgy_clusters_2022_05.fa  \\\n          --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt  \\\n          --pdb70_database_path=/data/pdb70/pdb70  \\\n          --template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n          --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n          --model_preset=monomer \\\n          --max_template_date=2023-08-02 \\\n          --db_preset=full_dbs \\\n          --output_dir=results \\\n          --fasta_paths=input.fasta\n</code></pre>"},{"location":"software/common_datasets/#llama-2","title":"Llama 2","text":"<p>The Meta Large Language Model project has several datasets that we make available under <code>/contrib/datasets/llama2</code>.</p>"},{"location":"software/common_datasets/#ncbi-blast","title":"NCBI Blast","text":"<p>The Blast databases are provided in support of the Blast module under <code>/contrib/datasets/blast</code>.</p>"},{"location":"software/containers/building_containers/","title":"Building Containers","text":"<p>Warning</p> <p>The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation. </p> <p>Tip</p> <p>For detailed information on Apptainer recipes, see Apptainer's official documentation.</p> <p>Apptainer Build is a tool that allows you to create containers. With Apptainer Build, you can package your application and its dependencies into a single unit, making it easier to deploy and share across different computing environments. Two useful options are to build your container by bootstraping off a container hosted locally on HPC or bootstrapping off an existing container hosted on Dockerhub. We'll cover both cases below.</p>"},{"location":"software/containers/building_containers/#bootstrapping-off-a-local-image","title":"Bootstrapping off a Local Image","text":"<p>One common case users run into is using a python container hosted on HPC but needing additional packages installed in the image. To do this, it's possible to bootstrap off the local image and pip-install a new package in a section called <code>%post</code> which is executed during build time. </p> <p>For example, say we want to use the HPC container nvidia-tensorflow-2.6.0.sif located in <code>/contrib/singularity/nvidia/</code> but we need it to have the package astropy installed which is currently missing. We can create a recipe file that takes this image, bootstraps off it, and pip-installs astropy. Our recipe file would look like the following:</p> <pre><code>Bootstrap: localimage\nFrom: /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif\n\n%post\n  pip install astropy\n</code></pre> <p>We'll call this recipe file something descriptive, e.g. tf2.6-astropy.recipe. Then, to build, all we need to do is use the syntax <code>apptainer build &lt;local_image_name&gt;.sif &lt;recipe_file&gt;</code>. In this case:</p> <pre><code>[netid@cpu43 build_example]$ apptainer build tf2.6-astropy.sif tf2.6-astropy.recipe \nINFO:    User not listed in /etc/subuid, trying root-mapped namespace\nINFO:    The %post section will be run under fakeroot\nINFO:    Starting build...\nINFO:    Verifying bootstrap image /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif\n. . .\nINFO:    Creating SIF file...\nINFO:    Build complete: tf2.6-astropy.sif\n[sarawillis@cpu43 build_example]$ \n</code></pre>"},{"location":"software/containers/building_containers/#bootstrapping-off-a-docker-hub-image","title":"Bootstrapping off a Docker Hub Image","text":"<p>Tip</p> <p>Not sure how to install your own software? Check our our section on User Installations.</p> <p>Bootstrapping off Ubuntu images is a great way to create a very customizable container where you can install your own software. Instead of pulling an Ubuntu image (see: Pulling Containers for a tip on how to find an Ubuntu image), we can bootstrap directly off the image in our recipe file. </p> <p>Let's say as an example, we want to install python 3.11 with a custom library. We can create a recipe file called python3.11_astropy.recipe:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:22.04\n\n%post \n  apt update -y\n  apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev -y \n  wget https://www.python.org/ftp/python/3.11.3/Python-3.11.3.tgz\n  tar -xf Python-3.11.3.tgz\n  cd Python-3.11.3\n  ./configure --enable-optimizations\n  make \n  make altinstall\n  python3.11 -m pip install astropy\n</code></pre> <p>And building using  <pre><code>[netid@cpu38 pull_example]$ apptainer build python3.11_astropy.sif python3.11_astropy.recipe \nINFO:    User not listed in /etc/subuid, trying root-mapped namespace\nINFO:    The %post section will be run under fakeroot\nINFO:    Starting build...\n. . .\nINFO:    Creating SIF file...\nINFO:    Build complete: python3.11_astropy.sif\n[sarawillis@cpu38 pull_example]$ \n</code></pre></p>"},{"location":"software/containers/containers_on_hpc/","title":"Containers on HPC","text":"<p>Tip</p> <p>Apptainer is installed on the operating systems of all HPC compute nodes, so can be easily accessed either from an interactive session or batch script without worrying about software modules. </p>"},{"location":"software/containers/containers_on_hpc/#available-containers","title":"Available Containers","text":"<p>We support the use of HPC and ML/DL containers available on NVIDIA GPU Cloud (NGC). Many of the popular HPC applications including NAMD, LAMMPS and GROMACS containers are optimized for performance and available to run in Apptainer on Ocelote or Puma. The containers and respective README files can be found in <code>/contrib/singularity/nvidia</code>. They are only available from compute nodes, so start an interactive session if you want to view them.</p>  Container  Description <code>nvidia-caffe.20.01-py3.simg</code> Caffe is a deep learning framework made with expression, speed, and modularity in mind. It was originally developed by the Berkeley Vision and Learning Center (BVLC). <code>nvidia-gromacs.2018.2.simg</code> GROMACS is designed to simulate biochemical molecules like proteins, lipids, and nucleic acids <code>nvidia-julia.1.2.0.simg</code> The Julia programming language is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages. <code>nvidia-lammps.24Oct2018.sif</code> The main use case of the Large-scale Atomic / Molecular Massively Parallel Simulator is atom scale particle modeling or, more generically, as a parallel particle simulator at the atomic, meson, or continuum scale <code>nvidia-namd_2.13-multinode.sif</code> NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. <code>nvidia-pytorch.20.01-py3.simg</code> PyTorch is a Python package that provides two high-level features:- Tensor computation (like numpy) with strong GPU acceleration- Deep Neural Networks built on a tape-based autograd system <code>nvidia-rapidsai.sif</code> RAPIDS provides unmatched speed with familiar APIs that match the most popular PyData libraries. Built on state-of-the-art foundations like NVIDIA CUDA and Apache Arrow. <code>nvidia-relion_2.1.b1.simg</code> RELION (REgularized LIkelihood OptimizatioN) implements an empirical Bayesian approach for analysis of electron cryo-microscopy (Cryo-EM). Specifically, RELION provides refinement methods of singular or multiple 3D reconstructions as well as 2D class averages. <code>nvidia-tensorflow_2.0.0-py3.sif</code> TensorFlow is an open source software library for numerical computation using data flow graphs. TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research. <code>nvidia-theano.18.08.simg</code> Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently."},{"location":"software/containers/containers_on_hpc/#sharing-your-containers","title":"Sharing Your Containers","text":"<p>If you have containers that you would like to share with your research group or broader HPC community, you may do so in the space <code>/contrib/singularity/shared</code>.</p>"},{"location":"software/containers/containers_on_hpc/#cache-directory","title":"Cache Directory","text":"<p>To speed up image downloads for faster, less redundant builds and pulls, Apptainer sets a cache directory in your home under <code>~/.apptainer</code>. This directory stores images, metadata, and docker layers that can wind up being reasonably large. If you're struggling with space usage and your home's 50GB quota, one option is to set a new Apptainer cache directory. You can do this by setting the environment variable <code>APPTAINER_CACHEDIR</code> to a new directory. From Apptainer's documentation:</p> <p>If you change the value of <code>APPTAINER_CACHEDIR</code> be sure to choose a location that is:</p> <pre><code>1. Unique to you. Permissions are set on the cache so that private images cached for one user are not exposed to another. This means that ```APPTAINER_CACHEDIR``` cannot be shared.\n2. Located on a filesystem with sufficient space for the number and size of container images anticipated.\n3. Located on a filesystem that supports atomic rename, if possible.\n</code></pre> <p>For example, if you wanted to set your cache directory to your PI's /groups directory under a directory you own, you could use:</p> <pre><code>export APPTAINER_CACHEDIR=/groups/pi/your_netid/.apptainer\n</code></pre> <p>To make the change permanent, add this line to the hidden file in your home directory <code>~/.bashrc</code>.</p>"},{"location":"software/containers/pulling_containers/","title":"Pulling Containers","text":"<p>Warning</p> <p>The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation. </p>"},{"location":"software/containers/pulling_containers/#pulling-docker-containers","title":"Pulling Docker Containers","text":"<p>Apptainer has the ability to convert available docker images into sif format allowing them to be run on HPC. If you find an image on Docker Hub that you would like to use, you can pull it using the <code>apptainer pull command &lt;local_image_name&gt;.sif docker://docker_image</code>. </p> <p>As an example, we could pull an Ubuntu image from Docker Hub with OS 22.04 by searching for Ubuntu, opening the Tags tab, and copying their <code>docker pull</code> command:</p> <p></p> <p>Then, on HPC, we can run:</p> <pre><code>[netid@cpu37 pull_example]$ apptainer pull ubuntu_22.04.sif docker://ubuntu:22.04\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 01007420e9b0 done  \nCopying config 3db8720ecb done  \nWriting manifest to image destination\nStoring signatures\n2024/02/20 09:02:02  info unpack layer: sha256:01007420e9b005dc14a8c8b0f996a2ad8e0d4af6c3d01e62f123be14fe48eec7\nINFO:    Creating SIF file...\n[netid@cpu37 pull_example]$ ls\nubuntu_22.04.sif\n</code></pre>"},{"location":"software/containers/pulling_containers/#pulling-nvidia-images","title":"Pulling Nvidia Images","text":"<p>The NVIDIA GPU Cloud (NGC) provides GPU-accelerated HPC and deep learning containers for scientific computing.  NVIDIA tests HPC container compatibility with the Apptainer runtime through a rigorous QA process. Application-specific information may vary so it is recommended that you follow the container-specific documentation before running with Apptainer. If the container documentation does not include Apptainer information, then the container has not yet been tested under Apptainer. Apptainer can be used to pull, execute, and bootstrap off Docker images. </p> <p>To pull images, you'll need to register with Nvidia. Once you have an account, you can view their images from their catalogue. Click on the name of the software you're interested in to view available versions</p> <p></p> <p>If you click on the Tags tab at the top of the screen, you'll find the different versions that are available for download. For example, if we click on TensorFlow, we can get the pull statement for the latest tag of TensorFlow 2 by clicking the ellipses and selecting Pull Tag.</p> <p></p> <p>This will copy a <code>docker pull</code> statement to your clipboard, in this case:</p> <pre><code>$ docker pull nvcr.io/nvidia/tensorflow:22.02-tf2-py3\n</code></pre> <p>To pull and convert this NGC image to a local Apptainer image file, we'll convert this to:</p> <pre><code>$ apptainer build ~/tensorflow2-22.02-py3.sif docker://nvcr.io/nvidia/tensorflow:22.02-tf2-py3\n</code></pre> <p>The general format for any pull you want to do is:</p> <pre><code>$ apptainer build &lt;local_image_name&gt; docker://nvcr.io/&lt;registry&gt;/&lt;app:tag&gt;\n</code></pre> <p>This Apptainer build command will download the app:tag NGC Docker image, convert it to Apptainer format, and save it to the local filename local_image_name. </p>"},{"location":"software/containers/using_containers/","title":"Using Containers","text":""},{"location":"software/containers/using_containers/#running-apptainer-images","title":"Running Apptainer Images","text":"<p>Apptainer can be used to execute your workflows in various ways: running a prepackaged workflow embedded in the image, executing commands within the container's environment, or starting an interactive instance.</p>"},{"location":"software/containers/using_containers/#apptainer-run","title":"apptainer run","text":"<p><code>apptainer run</code> is used without any arguments and executes a predefined workflow embedded in the image. The general syntax is: </p> <pre><code>apptainer run &lt;img&gt;.sif\n</code></pre> <p>For example:</p> <pre><code>[netid@cpu38 run_example]$ apptainer run lolcow.sif \n _______________________________________\n/ You get along very well with everyone \\\n\\ except animals and people.            /\n ---------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"software/containers/using_containers/#apptainer-exec","title":"apptainer exec","text":"<p>Apptainer exec instantiates your image and invokes a command from inside the container's environment. The general syntax is</p> <pre><code>apptainer exec app.sif &lt;commands&gt; \n</code></pre> <p>For example:</p> <pre><code>[netid@cpu38 run_example]$ singularity exec /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import tensorflow as tf; print(tf.__version__)\"\n2.6.2\n</code></pre>"},{"location":"software/containers/using_containers/#apptainer-shell","title":"apptainer shell","text":"<p>Apptainer shell starts an interactive session within the container's environment. This is optimal for testing, debugging, or using any sort of graphical interface installed in your image. The general syntax is</p> <pre><code>apptainer shell app.sif\n</code></pre> <p>For example:</p> <pre><code>[netid@cpu38 run_example]$ apptainer shell /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif\nSingularity&gt; python3\nPython 3.8.10 (default, Sep 28 2021, 16:10:42) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.6.2'\n</code></pre>"},{"location":"software/containers/using_containers/#apptainer-and-gpus","title":"Apptainer and GPUs","text":"<p>If you're using Apptainer to execute workflows that require a GPU, you'll need to add the additional flag <code>--nv</code> to your Apptainer command. For example:</p> <pre><code>apptainer exec --nv &lt;container&gt; &lt;commands&gt;\n</code></pre>"},{"location":"software/containers/what_are_containers/","title":"What are Containers?","text":"<p>Tip</p> <p>Want to learn more about containers? Check out our Intro to Containers workshop. Additional examples of using containers can be found in our batch jobs examples.</p> <p></p> <p>A container is a packaged unit of software that contains code and all its dependencies including, but not limited to: system tools, libraries, settings, and data. This makes applications and pipelines portable and reproducible, allowing for a consistent environment that can run on multiple platforms.</p> <p>Shipping containers have frequently been used as an analogy because the container is standard, does not care what is put inside, and will be carried on any ship; or in the case of computing containers, it can run on many different systems.</p> <p>Docker is widely used by researchers, however, Docker images require root privileges which means they cannot be run in an HPC environment.</p> <p>Apptainer (formerly Singularity) addresses this by completely containing the authority so that all privileges needed at runtime stay inside the container. This makes it ideal for the shared environment of a supercomputer. Even better, a Docker image can be encapsulated inside an Apptainer image. Some ideal use cases that can be supported by Apptainer on HPC include:</p> <ul> <li>You already use Docker and want to run your jobs on HPC.</li> <li>You want to preserve your environment so a system change will not affect your work.</li> <li>You need newer or different libraries than are offered on the system.</li> <li>Someone else developed a workflow using a different version of Linux.</li> <li>You prefer to use a Linux distribution other than CentOS (e.g. Ubuntu).</li> <li>You want a container with a database server like MariaDB.</li> </ul> <p>The documentation here provides instructions on how to either take a Docker image and run it from Apptainer, or create an image using Apptainer only.</p>"},{"location":"software/modules/","title":"Software Modules","text":"<p>Availability</p> <p>Software modules are not available on the login nodes. You will need to be on a compute node to access them.</p> <p>Software packages are available as modules and are accessible from the compute nodes of any of our three clusters. A software module is a tool used to manage software environments and dependencies. It allows you to easily load and unload different software packages, libraries, and compilers needed for computational tasks without conflicts. This ensures access to many specific tools, and even different versions of the same tools, without affecting the overall system configuration.</p>"},{"location":"software/modules/#module-commands","title":"Module Commands","text":"<p>Default Versions</p> <p>If multiple versions of software are available on the system, the newest is made the default. This means loading a module without specifying the version will select the most recent. We strongly recommend including version information in your module statements. This ensures that you maintain a consistent environment for your analyses in the event of a software upgrade.</p> Command Description <pre><code>module avail</code></pre> Display all the software and versions installed on the system <pre><code>module avail &lt;search_term&gt;</code></pre> Display all installed modules matching <code>&lt;search_term&gt;</code> <pre><code>module list</code></pre> Display the software you have loaded in your environment <pre><code>module whatis &lt;module_name&gt;</code></pre> Displays some descriptive information about a specific module <pre><code>module show &lt;module_name&gt;</code></pre> Displays system variables that are set/modified when loading module <code>&lt;module_name&gt;</code> <pre><code>module load &lt;module_name&gt;</code></pre> Load a software module in your environment <pre><code>module unload &lt;module_name&gt;</code></pre> Unload a specific software package from your environment <pre><code>module swap &lt;module_name&gt;/&lt;version1&gt; &lt;module_name&gt;/&lt;version2&gt;</code></pre> Switch versions of a software module <pre><code>module purge</code></pre> Unload all the software modules from your environment <pre><code>module help</code></pre> Display a help menu for the module command"},{"location":"software/modules/#example","title":"Example","text":"<pre><code>[netid@cpu39 ~]$ module avail python\n\n------------------- /opt/ohpc/pub/modulefiles --------------------\n   python/3.6/3.6.5     python/3.9/3.9.10\n   python/3.8/3.8.2     python/3.11/3.11.4 (D)\n   python/3.8/3.8.12\n[netid@cpu39 ~]$ module load python/3.9\n[netid@cpu39 ~]$ python3 --version\nPython 3.9.10\n[netid@cpu39 ~]$ module swap python/3.9 python/3.11\n\nThe following have been reloaded with a version change:\n  1) python/3.9/3.9.10 =&gt; python/3.11/3.11.4\n\n[netid@cpu39 ~]$ python3 --version\nPython 3.11.4\n</code></pre>"},{"location":"software/modules/#compilers","title":"Compilers","text":"<p>Puma, Ocelote, and El Gato all run CentOS7 and have the following compilers available:</p> Compiler Version Module Command Intel 2020.1 <pre><code>module load intel/2020.1</code></pre> Intel 2020.4 <pre><code>module load intel/2020.4</code></pre> gcc 5.4.0 <pre><code>module load gnu/5.4.0</code></pre> gcc 7.3.0 <pre><code>module load gnu7/7.3.0</code></pre> gcc 8.3.0 <pre><code>module load gnu8/8.3.0 # Loaded by default</code></pre>"},{"location":"software/modules/#installing-additional-software","title":"Installing additional software","text":"<p>You are allowed and encouraged to install software you may need on the HPC system. However, you cannot install software that requires root permission or use a method like \"yum install\" that accesses system paths. For information on installing software on the HPC, see our User Installations page.</p> <p>Sometimes it is beyond the abilities or permissions of a user to install your desired software. In this case, please submit a request for our Consult team to install it for you via the HPC Software Install Request form. There is no expected time frame for how long it takes to install software, as there are many variables that determine this. If you haven't heard back within a week, it may be appropriate to follow up.</p>"},{"location":"software/overview/","title":"Software Overview","text":""},{"location":"software/overview/#overview","title":"Overview","text":"<p>Our HPC systems support over 100 software applications.  The first thing to know is that the compute nodes on each cluster are where you use and manage software.  The operating system image on each node is the same for all three clusters. And the filesystems are mounted to them all, so the available software looks the same and your data is equally accessible.</p> <p>The software is available in three different ways:</p> <ul> <li>Libraries in the operating system (like <code>fftw</code> or <code>screen</code>);</li> <li>Personal software that you build or download and place in your own directory space;</li> <li>and Modules, which are external packages built and maintained by HPC team for system-wide usage.</li> </ul> Module availability <p>Software modules are not available on the login nodes. To access them, you will need to connect to a compute node either via an interactive session or batch job.</p>"},{"location":"software/overview/#policies","title":"Policies","text":""},{"location":"software/overview/#academicfree-software","title":"Academic/Free Software","text":"<p>There is a plethora of software generally available for scientific and research usage.  We will install that software as a module if it meets the following requirements:</p> Requirements Compatible with our module environment Some software is not written with clusters in mind and tries to install into system directories, or needs a custom environment on every compute node. Generally useful Some software has to be configured to the specific compute environment of the user. You are encouraged to use our \"contrib\" environment to install your own. Public license We do not install software if that would be a violation of its licensing. Reasonably well written Some software takes days of effort and still does not work right.  We have limited resources and reserve the right to \"give up\". Sometimes software is written for workstations and does not function correctly in a shared environment. Downloadable Some software requires additional steps to download installation files, such as registering on a website or accepting a license agreement. In these cases we ask researchers to download files and put them in a directory on the HPC storage. When you submit a software installation request let us know that you have already downloaded the files and provide path to the directory where they are located."},{"location":"software/overview/#commercialfee-based-software","title":"Commercial/Fee-based Software","text":"<p>The University of Arizona Research Computing facility has many commercial and freeware packages installed on our supercomputers. Our approach to acquisition of additional software depends upon its cost, licensing restrictions, and user interest.   </p> Audience Single user interest The license for the software is purchased by the user and his/her department or sponsor.  This software is best installed by the user.  There are two main options; the first and easiest, is to install the software locally in the relevant user's account using the example procedure. The second is to use the \"contrib\" environment.  The advantage is that you can share the software built here with other users. This is created through a support ticket where a consultant will create a \"contrib\" group in which you can build software and add users. Group interest If a package is of interest to a group of several users, the best approach at first is for one user to act as a primary sponsor and arrange to split the procurement/licensing costs among the group. We can install the software and manage the user access according to requests from the group. Broad interest The High Performance Computing team will consider acquiring and supporting software packages that have broad interest among our users. Full facility support will depend on the cost of the package and our ability to comply with any restrictive licensing conditions."},{"location":"software/overview/#unsupported-software","title":"Unsupported Software","text":"<p>Unfortunately, our HPC system is not configured to support all software use cases. We have summarized the main scenarios which cause software to be unsupported by our system below. Prior to submitting an installation request, double-check that your software requirements don't fall into one of these categories. While the HPC may not be able to support these cases, it may be possible that other campus resources are able to. We encourage you to contact services listed in our Community Resources page.</p> <p>The below list is not exhaustive and may be expanded as new scenarios are encountered. If you are unsure whether your desired software is supported, feel free to contact our consultants.</p> Software Requirements Examples Non-SSH External Connections Software requiring external communications that utilize protocols other than SSH are not supported. Job Management/Scheduling The UArizona HPC uses Slurm as its task scheduler and resource manager. Software requiring different a scheduler is therefore unsupported. Apache SparkSun Grid Engine Workstation Software Software that is designed to be run on a local Linux workstation often requires root privileges and display management that may not be compatible with a shared, remote system like HPC Persistent Databases/Servers The UArizona HPC is not configured to support databases, server instances, or other persistent software-specific daemons. SQL databasesApplication deploymentsSAS server Windows Applications While there are plenty of excellent Windows software suites available for scientific computing, they unfortunately cannot be run on HPC. There are, however, national resources available that may support your application. One example is JetStream2 available through an ACCESS Allocation. See our Community Resources page linked at the top of this page for more details. ArcGIS"},{"location":"software/overview/#federal-regulations","title":"Federal Regulations","text":"<p>By policy, it is prohibited to use any of the facility's resources in any manner that violates the US Export Administration Regulations (EAR) or the International Trafficking in Arms Regulations (ITAR). It is relevant in this regard to be aware that the facility employs analysts who are foreign, nonresident, nationals and who have root-access privileges to all files and data. Specifically, you must agree not to use any software or data on facility systems that are restricted under EAR and/or ITAR.</p>"},{"location":"software/popular_software/R/","title":"R","text":"<p>Examples</p> <p>We have examples of using R in batch scripts available in our Job Examples section.</p> <p>R is a popular language for data analysis and visualization. Different versions are available as software modules and we provide the graphical interface RStudio for R through our Open OnDemand web interface.</p> <p>Similar to other languages that use package managers to install libraries contributed by the user community, we recommend you create and manage your own local libraries in your account. This ensures a stable global environment for all users and that you have the most control over your packages' versions and dependencies.</p> <p>We provide instructions below for how to create, use, and switch between libraries as well as some debugging techniques for when package installations fail. We also provide some script examples (click the button in the banner at the top of this page) for submitting R scripts as batch jobs.</p> <p>RStudio is a popular method for running analyses (and for good reason!), but for longer-running jobs (say, many hours or days) or workflows that need more flexibility in their environment (e.g., need access to software installed as system modules such as gdal), we recommend batch submissions.</p>"},{"location":"software/popular_software/R/#creating-a-custom-library","title":"Creating a Custom Library","text":"<p>R packages can be finicky. See Switching Between Custom Libraries and Common Problems below to help with frequent user issues.</p> <p>Creating your first library</p> <ol> <li> <p>Make a local directory to store your packages    <pre><code>mkdir -p ~/R/library_4.2\n</code></pre></p> </li> <li> <p>Tell R where the directory is by creating an environment file<sup>1</sup> <pre><code>echo 'R_LIBS=~/R/library_4.2/' &gt;&gt; ~/.Renviron\n</code></pre></p> </li> <li> <p>That's it! Now you can install packages and they will be stored in the directory you just created. For example, to install and load the package <code>ggplot2</code>:     <pre><code>module load R/4.2\nR\ninstall.packages(\"ggplot2\")\n</code></pre></p> </li> </ol>"},{"location":"software/popular_software/R/#switching-between-custom-libraries","title":"Switching Between Custom Libraries","text":"<p>If you're using different versions of R, we recommend you use different libraries. See Common Problems below for more information. When creating a library, consider including pertinent information in the name such as R version. For example, if you wanted to switch to using R 4.1, you could create a directory called <code>library_4.1</code> using: <pre><code>mkdir -p ~/R/library_4.1\n</code></pre> Then, to use your new library, edit your <code>~/.Renviron</code> file<sup>1</sup> using a text editor such as <code>nano</code>: <pre><code>nano ~/.environ\n</code></pre> Once your text editor opens, rename the <code>R_LIBS</code> you previously had in your file. In this case, this value would look like: <pre><code>R_LIBS=~/R/library_4.1\n</code></pre> To exit <code>nano</code>, use Ctrl+X and save at the prompt. Once your file is saved, you're ready to start installing files into your new library. </p>"},{"location":"software/popular_software/R/#common-problems-and-how-to-debug-them","title":"Common Problems and How to Debug Them","text":"<p>Working on a cluster without root privileges can lead to complications. For general information on package installations, see the r-bloggers documentation. For information on common installation problems on our clusters, see the section below with with suggested solutions:</p> AnacondaA Corrupted EnvironmentLibrary IssuesMixing R VersionsOpen OnDemand RStudio Issues <p>One common reason R packages won't install is an altered environment. This can frequently be caused by the presence of Anaconda (or Miniconda) installed locally or initialized in your account from our system module.</p> <p>When Anaconda is initialized, your <code>.bashrc</code> file is edited so that it becomes the first thing in your <code>PATH</code> variable. This can cause all sorts of mayhem. To get around this, you can either remove anaconda from your <code>PATH</code> and deactivate your environment, or comment out/delete the initialization in your <code>~/.bashrc</code> if you want the change to be permanent.</p> Turn off Auto-activationTemporary RemovalPermanent Removal <p>Anaconda's initialization will tell it to automatically activate itself when you log in (when anaconda is active, you will see a \"(conda)\" preceding your command prompt). To disable this behavior, run the following from the command line in an interactive terminal session:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>This will suppress anaconda's activation until you explicitly call <code>conda activate</code> and is a handy way to have more control over your environment. Once you run this, you will either need to log out and log back in again to make the changes live, or you can follow the instructions in the section below. </p> <p>Sometimes turning off auto-activation won't be enough because Anaconda will still be present in your <code>PATH</code>. In this case, follow the instructions in the tab Temporary Removal or Permanent Removal</p> <p>You can either use the command <code>conda deactivate</code> and then manually edit your <code>PATH</code> variable to remove all instances of anaconda/miniconda or copy the following and run it in your terminal: <pre><code>conda deactivate &gt; /dev/null 2&gt;&amp;1\nIFS=':' read -ra PATHAR &lt;&lt;&lt; \"$PATH\"\nfor i in \"${PATHAR[@]}\"\n    do if [[ $i == *\"conda\"* ]]\n        then echo \"removing $i from PATH\"\n    else NEWPATH=$i:$NEWPATH\n    fi\ndone\nexport PATH=$NEWPATH\nmodule unload gnu8 &amp;&amp; module load gnu8\nunset NEWPATH\necho \"Successfully removed conda\"\n</code></pre></p> <p>Warning</p> <p>Your <code>.bashrc</code> file configures your environment each time you start a new session. Be careful when editing it. You may consider making a backup before editing in case of unwanted changes.</p> <p>Tip</p> <p>Note: this change will remove anaconda from all future terminal sessions but will not make the changes live right away. To make the changes live, either follow the instructions above under Temporary Removal for removing anaconda from your PATH, or log out and back in again.</p> <p>Start by opening the file ~/.bashrc. This can be done using the command <code>nano</code></p> <pre><code>$ nano ~/.bashrc # opens your bashrc file to edit\n</code></pre> <p>Then comment out or delete the following lines and the text in between:</p> <p><pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n...\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> To exit use Ctrl+X, select Y to save, and hit Enter to confirm your filename.</p> <p>If Anaconda is not initialized in your account, there might be other culprits that are corrupting your environment.</p> <p>Look for any of the file types listed below on your account. If you find them, try removing them (make a backup if you need them) and try the installation again.</p> <ul> <li>Saved R sessions. If this is the case, after starting a session, you will get the message \"[Previously saved workspace restored]\". Old sessions are saved as a hidden file <code>.RData</code> in your home directory. </li> <li>Gnu compilers</li> <li>Windows files</li> </ul> <p>Have you set up a custom library? Are you switching between custom libraries? You may want to check that everything is being loaded from the correct location and that there are not multiple or unwanted libraries being used.</p> <p>Double-check that you have an <code>.Renviron</code> file. This is a hidden file located in your home directory and should set the path to your custom R library. If you do not have a custom library name set up, R will create one for you saved as something like: <pre><code>~/R/x86_64-pc-linux-gnu-library\n</code></pre> This directory can lead to unwanted behavior. For example, if you're trying to use a new custom library (such as when switching R version), R will still search x86_64-pc-linux-gnu-library for package dependencies and may cause installs to fail. To fix this, rename these types of folders something unique and descriptive.</p> <p>To set up/switch custom libraries, follow the instructions in the Creating a Custom R Library section above.</p> <p>Because HPC is a cluster where multiple versions of R are available, users should take care to avoid mixing and matching. Because packages often depend on one another, libraries using different versions of R can turn into a tangled mess.  Common errors that can crop up include: \"Error: package or namespace load failed.\"</p> <p>If you're switching R versions and have a custom library defined in your ~/.Renviron file, we recommend creating a new library.</p> <p>RStudio is a great tool! Sometimes though, because it's a different environment than working directly from the terminal, you may run into problems. Specifically, these typically arise for installs or when using packages that rely on software modules.  </p> <p>Package Installations</p> <p>If you're trying to install a package in an OOD RStudio session and you've tried all the troubleshooting advice above without luck, try starting R in the terminal and give the installation another try. You can access an R session in the terminal by first starting an interactive session, then using:</p> <pre><code>$ module load R/&lt;version&gt;\n$ R\n&gt; install.packages(\"package_name\")\n</code></pre> <p>Accessing Modules</p> <p>RStudio does not have access to module load commands. This means that if you have a package that relies on a system module, the easiest option is to work through an interactive terminal session or to submit a batch script.</p> <p>The alternative is to to modify your RStudio environment. For example, the library hdf5r relies on the hdf5 software module. If you try to load hdf5r, you will get an error complaining about a shared object file. To get around this, you will need to manually add that shared object to your environment using dyn.load(). For example:</p> <p><pre><code>&gt; library(\"hdf5r\") # without using dyn.load()\nError: package or namespace load failed for \u2018hdf5r\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\nunable to load shared object '/home/u21/sarawillis/R/lib_4.0/hdf5r/libs/hdf5r.so':\nlibhdf5_hl.so.100: cannot open shared object file: No such file or directory\n&gt; dyn.load(\"/opt/ohpc/pub/libs/gnu8/hdf5/1.10.5/lib/libhdf5_hl.so.100\")\n&gt; library(\"hdf5r\") # success!\n&gt;\n</code></pre> This requires that you know the location of the relevant file(s). These can usually be tracked down by looking at your system path variables (e.g. LD_LIBRARY_PATH) after loading the relevant module in a terminal. It should be noted that modifying your system paths from RStudio will not help since RStudio has its own configuration file that overrides these. </p> <p>Font Issues</p> <p>RStudio uses Singularity under the hood. As a result, there are some environment differences that may affect correct font formatting in images generated in RStudio. If you are experiencing this, add the following line to the hidden file ~/.Renviron in your account (you can create this file if it does not exist):</p> <pre><code>FONTCONFIG_PATH=/opt/ohpc/pub/apps/fontconfig/2.14.2/etc/fonts\n</code></pre>"},{"location":"software/popular_software/R/#using-rstudio","title":"Using RStudio","text":"Open OnDemandSingularity <p>We provide access to the popular development environment RStudio through our Open OnDemand web interface. This is a very handy tool, though it should be noted that it is a less flexible environment than using R from the command line. This is because RStudio sets its own environment which prevents easy access to third party software installed as system modules. These issues can sometimes worked around by following the guide in the debugging section above.</p> <p>In some circumstances, you may want to run RStudio using your own Singularity image. For example, this allows access to different versions of R not provided when using our OOD application. We have some instructions on one way to do this below.</p> <p>First, log into HPC using an Open OnDemand Desktop session and open a terminal. A Desktop session is the easiest solution to access RStudio since it eliminates the need for port forwarding.</p> <p>In the terminal, make an RStudio directory where all of the necessary files will be stored. In this example, we'll be working in our home directory and will pull an RStudio image from Dockerhub to use as a test. If you're interested, you can find different RStudio images under rocker in Dockerhub.</p> <pre><code>mkdir $HOME/RStudio\ncd $HOME/RStudio\nsingularity pull ./geospatial.sif docker://rocker/geospatial.sif\n</code></pre> <p>Next, create the necessary directories RStudio will use to generate temporary files. You will also generate a secure cookie key.</p> <pre><code>TMPDIR=$HOME/RStudio/rstudio-tmp\nmkdir -p $TMPDIR/tmp/rstudio-server\nuuidgen &gt; $TMPDIR/tmp/rstudio-server/secure-cookie-key\nchmod 600 $TMPDIR/tmp/rstudio-server/secure-cookie-key\nmkdir -p $TMPDIR/var/{lib,run}  \n</code></pre> <p>Next, create a file in your RStudio directory called rserver.sh and make it an executable:</p> <p><pre><code>touch rserver.sh\nchmod u+x rserver.sh\n</code></pre> Open the file in your favorite editor and enter the content below. Modify the variables under <code>USER OPTIONS</code> to match your account if necessary. You can change <code>PASSWORD</code> to any password you'd like to use. Once you've entered the contents, save and exit:</p> <pre><code>#!/bin/bash\n\n# --- USER OPTIONS --- #\nWD=$HOME/RStudio\nSIFNAME=geospatial.sif\nPASSWORD=\"PASSWORD\"\n\n# --- SERVER STARTUP EXECUTED BELOW --- #\nNETID=$(whoami)\nTMPDIR=$WD/rstudio-tmp\nSIF=$WD/$SIFNAME\nPASSWORD=$PASSWORD singularity exec -B $TMPDIR/var/lib:/var/lib/rstudio-server -B $TMPDIR/var/run:/var/run/rstudio-server  -B $TMPDIR/tmp:/tmp $SIF rserver --auth-none=0 --auth-pam-helper-path=pam-helper --server-user=$NETID --www-address=127.0.0.1\n</code></pre> <p>Now, in your desktop session's terminal, execute the rserver.sh script using <code>./rserver.sh</code></p> <p></p> <p>Next, open a Firefox window and enter <code>localhost:8787</code> for the URL. In your browser, you will be prompted to log into your RStudio server. Enter your NetID under Username. Under Password, enter the password you defined in the script server.sh.</p> <p></p> <p>This will open your RStudio session:</p> <p></p>"},{"location":"software/popular_software/R/#setting-a-new-user-state-directory","title":"Setting a New User State Directory","text":"<p>When working on a large project in RStudio, it is possible for your R session's data to fill up your home directory resulting in out-of-space errors (e.g. when trying to edit files, create new OOD sessions, etc). With the newest version of RStudio, you can find these saved session files under <code>~/.local/share/rstudio</code>.</p> <p>To preserve space in your home, you can specify a different directory by setting the environment variable <code>RSTUDIO_DATA_HOME</code>. To do this, open the hidden file <code>~/.bashrc</code> and add:</p> <pre><code>export RSTUDIO_DATA_HOME=/path/to/new/directory\n</code></pre> <p>where <code>/path/to/new/directory</code> is the path to a different location where you have a larger space quota. For example, <code>/groups/YOUR_PI/YOUR_NETID/rstudio_sessions</code>.</p>"},{"location":"software/popular_software/R/#setting-your-working-directory-in-rstudio","title":"Setting Your Working Directory in RStudio","text":"Current SessionAll Non-Project Sessions <p>If you'd like to change your working directory in an RStudio session, one option is to use <code>setwd(\"/path/to/directory\")</code> in your terminal. Alternatively, if you'd like to see the contents of your new workspace in your file browser, you can navigate to the Session dropdown tab, navigate to Set Working Directory, and click Choose Directory...</p> <p></p> <p>From there, either navigate to the desired subdirectory, or click the ellipsis <code>...</code> in the upper right to enter the full path to a directory.</p> <p></p> <p>Once you click OK and then Choose in the main file navigation window, R will change its working directory and you should see the contents of your new space under the Files browser in the lower right.</p> <p></p> <p>If you'd like to permanently set a different default working directory for all non-project RStudio sessions, navigate to the Tools dropdown tab and select Global Options...</p> <p></p> <p>This will open a menu where you can set your default working directory under General. Click the Browse... button to open a file navigator</p> <p></p> <p>To select a new working directory, either navigate to the subdirectory of your current working space, or select the ellipsis <code>...</code> in the upper right to allow you to enter the full path. </p> <p></p> <p>The ellipsis option allows for more flexibility such as pointing to an <code>/xdisk</code> or <code>/groups</code> space.</p> <p></p> <p>Next, click OK, then Choose in the Choose Directory window, then Apply in the Global Options menu. This will set your working directory for your current session as well as all future sessions.</p> <p></p>"},{"location":"software/popular_software/R/#popular-packages","title":"Popular Packages","text":"<p>Updates and Version Changes</p> <p>We attempt to keep these instructions reasonably up-to-date. However, given the nature of ongoing software and package updates, there may be discrepancies due to version changes. If you notice any instructions that don't work that we have not caught, contact our consultants and they can help. </p> Seurat and SeuratDiskMonocle3 <p>R Studio Version</p> <p>If you use RStudio for your analyses, make sure that you load the same version of R when working with modules on the command line.</p> <p>To install Seurat and SeuratDisk, you'll need to be in an interactive terminal session and not in an RStudio session. This is because these libraries depend on software modules that RStudio doesn't have access to (see Common Problems \u2192 OOD RStudio Issues above for more information).</p> <p>You will also need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block under Resolving Anaconda Issues \u2192 Temporary Removal above.</p> SeuratSeuratDisk <pre><code>(elgato) [netid@junonia ~]$ interactive -a your_group\n[netid@cpu38 ~]$ module load R/&lt;version&gt;\n[netid@cpu38 ~]$ module load gdal glpk libpng # software modules that are needed for Seurat's dependencies\n[netid@cpu38 ~]$ R\n&gt; install.packages(\"Seurat\")\n</code></pre> <p>If you want to load this software in an RStudio session, you will first need to use the following <code>dyn.load</code> commands. When using the <code>dyn.load</code>s in RStudio, you will need to be careful to run them in the order shown below, otherwise you may wind up with \"Undefined symbol\" errors. If you repeatedly run into library errors working in RStudio, you might consider converting your workflow to a batch script that you can submit through the command line. See the section Example R Scripts above for more information. <pre><code>&gt; dyn.load(\"/opt/ohpc/pub/apps/glpk/5.0/lib/libglpk.so.40\")\n&gt; dyn.load(\"/opt/ohpc/pub/apps/gdal/3.3.2/lib/libgdal.so.29\")\n&gt; dyn.load(\"/opt/ohpc/pub/apps/proj/7.2.1/lib/libproj.so.19\")\n&gt; library(Seurat)\n</code></pre></p> <p>SeuratDisk is similar to Seurat with a few more dependencies. It also includes the line <code>unset CPPFLAGS</code> due to a reported issue with the dependency hdf5r:</p> <pre><code>(elgato) [netid@junonia ~]$ interactive -a your_group\n[netid@cpu1 ~]$ module load R/&lt;version&gt; gdal geos hdf5/1.10.5 libpng/1.6.37 glpk\n[netid@cpu1 ~]$ unset CPPFLAGS\n[netid@cpu1 ~]$ R\n&gt; install.packages(\"Seurat\")\n&gt; install.packages(\"remotes\")\n&gt; remotes::install_github(\"mojaveazure/seurat-disk\")\n</code></pre> <p>Then, to load the software in RStudio:</p> <pre><code>&gt; dyn.load(\"/opt/ohpc/pub/apps/glpk/5.0/lib/libglpk.so.40\")\n&gt; dyn.load(\"/opt/ohpc/pub/apps/proj/7.2.1/lib/libproj.so.19\")\n&gt; dyn.load(\"/opt/ohpc/pub/apps/gdal/3.3.2/lib/libgdal.so.29\")\n&gt; dyn.load(\"/opt/ohpc/pub/libs/gnu8/hdf5/1.10.5/lib/libhdf5_hl.so.100\")\n&gt; library(Seurat)\n&gt; library(SeuratDisk)\n</code></pre> <p>To install Monocle3, you'll need to be in an interactive terminal session and not in an RStudio session. This is because it depends on software modules that RStudio doesn't have access to (see Common Problems \u2192 OOD RStudio Issues above for more information).</p> <p>You will also need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block under Resolving Anaconda Issues \u2192 Temporary Removal above.</p> <p>When using the dyn.loads in RStudio, you will need to be careful to run them in the order shown below, otherwise you may wind up with \"Undefined symbol\" errors. If you repeatedly run into library errors working in RStudio, you might consider converting your workflow to a batch script that you can submit through the command line. </p> <p>Monocle3's documentation includes steps that you can use for a successful installation.</p> <pre><code>(elgato) [netid@junonia ~]$ interactive -a your_group\n[netid@cpu1 ~]$ module load R/&lt;version&gt; gdal\n[netid@cpu1 ~]$ R\n&gt; install.packages(\"BiocManager\")\n&gt; BiocManager::install(c('BiocGenerics', 'DelayedArray', 'DelayedMatrixStats',\n                       'limma', 'lme4', 'S4Vectors', 'SingleCellExperiment',\n                       'SummarizedExperiment', 'batchelor', 'HDF5Array',\n                       'terra', 'ggrastr'))\n&gt; install.packages(\"devtools\")\n&gt; devtools::install_github('cole-trapnell-lab/monocle3')\n</code></pre> <p>Then, to load Monocle3 in RStudio:</p> <pre><code>dyn.load(\"/opt/ohpc/pub/apps/gdal/3.3.2/lib/libgdal.so.29\")\ndyn.load(\"/opt/ohpc/pub/apps/proj/7.2.1/lib/libproj.so.19\")\nlibrary(monocle3)\n</code></pre> <ol> <li> <p>The file <code>~/.Renviron</code> is a \"dot\" file which means it does not show up when you run a standard <code>ls</code>. Files that start with a <code>.</code> are hidden and are typically used for important configuration information. This particular file can be used to control your R environment for each subsequent time you start a session. All the echo command does is append the line <code>R_LIBS=~/R/library</code> to this file.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"software/popular_software/gaussian/","title":"Gaussian","text":""},{"location":"software/popular_software/gaussian/#access","title":"Access","text":"<p>In order to access Gaussian and Gaussview, you will need to belong to a special group called g03.  You can request to be added by submitting a help ticket. This is a constraint in Gaussian that other modules do not have.</p>"},{"location":"software/popular_software/gaussian/#gpu-notes","title":"GPU Notes","text":"<p>When reading these notes, keep in mind that the GPU nodes on Ocelote have one P100 GPU, 28 cores and 256 GB RAM.</p> <ol> <li> <p>Gaussian 16 can use the NVIDIA P100 GPUs installed on Ocelote.  Earlier GPUs do not have the computational capabilities or memory size to run the algorithms in G16.  Allowing larger amounts of memory is even more important when using GPUs than for CPUs, since larger batches of work must be done at the same time in order to use the GPUs efficiently (see below).</p> </li> <li> <p>When using GPUs it is essential to have the GPU controlled by a specific CPU and much preferable if the CPU is physically close to the GPU it is controlling. The hardware arrangement can be checked using the <code>nvidia-smi</code> utility. For example, this output is for a machine with 2 16-core Haswell CPU chips and 4 K80 boards, each of which has two GPUs:</p> <pre><code>GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 CPU Affinity \nGPU0  X    PIX  SOC  SOC  SOC  SOC  SOC  SOC    0-15\nGPU1  PIX   X   SOC  SOC  SOC  SOC  SOC  SOC    0-15\nGPU2  SOC  SOC   X   PIX  PHB  PHB  PHB  PHB   16-31\nGPU3  SOC  SOC  PIX   X   PHB  PHB  PHB  PHB   16-31\nGPU4  SOC  SOC  PHB  PHB   X   PIX  PXB  PXB   16-31\nGPU5  SOC  SOC  PHB  PHB  PIX   X   PXB  PXB   16-31\nGPU6  SOC  SOC  PHB  PHB  PXB  PXB   X   PIX   16-31\nGPU7  SOC  SOC  PHB  PHB  PXB  PXB  PIX   X    16-31\n</code></pre> <p>The important part is the CPU affinity. This shows that GPUs 0 and 1 (on the first K80 card) are connected to the CPUs on chip 0 while GPUs 2-7 (on the other three K80 cards) are connected to the CPUs on chip 1.  So a job which uses all the CPUs (24 CPUs doing parts of the computation and 8 controlling GPUs) would use input</p> <pre><code>%cpu=0-31\n%gpucpu=0-7=0-1,16-21\n</code></pre> <p>or equivalently but more verbosely</p> <pre><code>%cpu=0-31\n%gpucpu=0,1,2,3,4,5,6,7=0,1,16,17,18,19,20,21\n</code></pre> <p>This pins threads 0-31 to CPUs 0-31 and then uses GPU0 controlled by CPU 0, GPU1 controlled by CPU 1, GPU2 controlled by CPU 16, etc.</p> <p>Normally one uses consecutive numbering in the obvious way, but things can be associated differently in special cases. For example, suppose on the other machine one already had one job using 6 CPUs running with <code>%cpu=16-21</code>.  Then if one wanted to use the other 26 CPUs with 8 controlling GPUs one would specify:</p> <pre><code>%cpu=0-15,22-31\n%gpucpu=0-7=0-1,22-27\n</code></pre> <p>This would create 26 threads with GPUs controlled by the threads on CPUs 0,1,22,23,24,25,26, and 27.</p> </li> <li> <p>GPUs are not helpful for small jobs but are effective for larger molecules when doing DFT energies, gradients and frequencies (for both ground and excited states).  They are not used effectively by post-SCF calculations such as MP2 or CCSD.</p> <p>Each GPU is several times faster than a CPU but since on modern machines there are typically many more CPUs than GPUs, it is important to use all the CPUs as well as the GPUs and the speedup from GPUs is reduced because many CPUs are also used effectively (i.e., in a job which uses all the CPUs and all the GPUs).  For example, if the GPU is 5x faster than a CPU, then the speedup from going to 1 CPU to 1 CPU + 1 GPU would be 5x, but the speedup going from 32 CPUs to 32 CPUs + 8 GPUs would be 32 CPUs -&gt; 24 CPUs + 8 GPUs, which would be equivalent to 24 + 5x8 = 64 CPUs, for a speedup of 64/32 or 2x.</p> </li> <li> <p>The GPUs can have up to 16 GB of memory and one typically tries to have most of this available for Gaussian, which requires at least an equal amount of memory for the CPU thread which is running each GPU.  8 or 9 GB works well if there is 12 GB total on each GPU, or 11-12 GB for a 16 GB GPU.  Since the program gives equal shares of memory to each thread, this means that the total memory allowed should be the number of threads times the memory required to use a GPU efficiently.  For example, when using 4 CPUs and two GPUs each of which has 12 GB of memory, one should use 4 x 12 GB of total memory, i.e. </p> <pre><code>%mem=48gb\n%cpu=0-3\n%gpucpu=0-1=0,2\n</code></pre> <p>(or whatever specific CPU and GPU numbers are appropriate to the machine).</p> </li> <li> <p>GPUs on nodes in a cluster can be used. Since the <code>%cpu</code> and <code>%gpucpu</code> specifications are applied to each node in the cluster, the nodes must have identical configurations (number of GPUs and their affinity to CPUs); since most clusters are collections of identical nodes, this is not usually a problem.</p> </li> </ol>"},{"location":"software/popular_software/matlab/","title":"Matlab","text":""},{"location":"software/popular_software/matlab/#overview","title":"Overview","text":"<p>There are four ways to run Matlab:</p> <ol> <li>Using the Matlab graphical application through Open OnDemand.</li> <li>Graphical mode using the Open OnDemand Desktops.</li> <li>The command line version using modules.  This is the most common as you will typically submit a job using SLURM.</li> <li>Python </li> </ol> <p>Like any other application, Matlab has to be loaded as a module before you can use it. To see all the installed versions of the Matlab, use the command <code>module avail matlab</code>.</p>"},{"location":"software/popular_software/matlab/#running-matlab-analyses-in-batch","title":"Running Matlab Analyses in Batch","text":"<p>The typical procedure for performing calculations on UArizona HPC systems is to run your program non-interactively on compute nodes. The easiest way to run Matlab non-interactively is to use input/output redirection. This method uses Linux operators <code>&lt;</code> and <code>&gt;</code> to point Matlab to the input file and tell where to write the output (see the example script). The other method is to invoke Matlab from the Slurm script and execute specified statement using <code>-r</code> option. For details, refer to the manual page for the matlab command.</p> <p>An example batch script might look like the following:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=matlab\n#SBATCH --account=group_name\n#SBATCH --partition=standard\n#SBATCH --ntasks=20\n#SBATCH --nodes\n#SBATCH --mem-per-cpu=5gb\n#SBATCH --time=01:00:00\n\nmodule load matlab/&lt;version&gt;\n\nmatlab -nodisplay -nosplash &lt; script_name.m &gt; output.txt\n</code></pre> <p>The options <code>-nodisplay</code> and <code>-nosplash</code> in the example prevent Matlab from opening graphical elements. To view the full list of options for the <code>matlab</code> command, load the Matlab module and type <code>matlab -h</code> at the prompt. Alternatively, use the link above to see the manual page on the MathWorks website.</p>"},{"location":"software/popular_software/matlab/#parallel-computing-toolbox","title":"Parallel Computing Toolbox","text":""},{"location":"software/popular_software/matlab/#temporary-files","title":"Temporary Files","text":"<p>By default, Matlab PCT will dump files to <code>~/.matlab/MATLAB_VERSION</code>. This causes problems when multiple Matlab PCT jobs are running simultaneously. Users should always define the environment variable <code>MATLAB_PREFDIR</code> so each job uses a unique temporary folder. Files there will be cleaned after the job finishes. For example: <pre><code>export MATLAB_PREFDIR=$(mktemp -d $SLURM_JOBTMP/matlab-XXXX)\n</code></pre></p>"},{"location":"software/popular_software/matlab/#matlab-and-slurm-resource-requests","title":"Matlab and SLURM Resource Requests","text":"<p>If you are trying to run Matlab in parallel interactively, you may encounter the following error:</p> <pre><code>&gt;&gt; Starting parallel pool (parpool) using the 'local' profile ...\nError using parpool (line 149)\nYou requested a minimum of &lt;n&gt; workers, but the cluster \"local\" has the NumWorkers property set to allow a maximum of 1 workers. To run a communicating job on more workers than this\n(up to a maximum of 512 for the Local cluster), increase the value of the NumWorkers property for the cluster. The default value of NumWorkers for a Local cluster is the number of\nphysical cores on the local machine.\n</code></pre> <p>This is caused by an interaction between Slurm and Matlab. To resolve this issue, when requesting <code>&lt;n&gt;</code> cores for your interactive job, you will need to set Slurm's <code>--ntasks</code> directive to 1 and <code>--cpus-per-task</code> to the number of cores you need. For example:</p> <pre><code>$ salloc --nodes=1 --ntasks=1 --cpus-per-task=6 --mem-per-cpu=5GB --time=01:00:00 --job-name=interactive --account=&lt;GROUP&gt; --partition=standard\n</code></pre>"},{"location":"software/popular_software/matlab/#external-resources","title":"External Resources","text":"<p>If you are getting started with Matlab or think there might be a better way, check out the training resources.</p> Resource Link Self-Paced Online Courses https://matlabacademy.mathworks.com/ Matlab Parallel Server https://www.mathworks.com/products/matlab-parallel-server.html#resources Natural Language Processing https://www.mathworks.com/discovery/natural-language-processing.html Matlab Videos https://www.mathworks.com/videos.html"},{"location":"software/popular_software/perl/","title":"Perl","text":""},{"location":"software/popular_software/perl/#accessibility","title":"Accessibility","text":"<p>Perl is installed on the operating system of each compute node: </p> <pre><code>[netid@compute_hostname ~]$ perl --version\n\nThis is perl 5, version 16, subversion 3 (v5.16.3) built for x86_64-linux-thread-multi\n(with 44 registered patches, see perl -V for more detail)\n...\n</code></pre>"},{"location":"software/popular_software/perl/#perl-module-policy","title":"Perl Module Policy","text":"<p>We provide a version of perl through modules or the operating. Installation of additional user libraries can be done in a perl environment using <code>perl-virtualenv</code>.</p> <p>For a helpful Perl tutorial, see: http://www.tutorialspoint.com/perl/perl_modules.htm. Additionally, O'Reilly Media is a well regarded source for Perl </p>"},{"location":"software/popular_software/perl/#installing-perl-packages-using-perl-virtualenv","title":"Installing Perl Packages Using perl-virtualenv","text":"<p>One of the best things about Perl is the number of packages provided by the user community. Installing packages generally requires root access but that is not a viable solution in the HPC environment.</p> <p>An easy solution is to use perl-virtualenv to create a consistent personal Perl environment that will persist for each time you log in. An example of usage:</p> <pre><code>[netid@i0n1 ~]$ perl-virtualenv my_project    # Create virtual environment\nperl path: /usr/bin/perl\nvenv path: /home/uxx/netid/my_project\n[netid@i0n1 ~]$ source my_project/bin/activate # Activate virtual environment\n(my_project)[netid@i0n1 ~]$ cpanm -i Config::Trivial\n--&gt; Working on Config::Trivial\nFetching http://www.cpan.org/authors/id/A/AT/ATRICKETT/Config-Trivial-0.81.tar.gz ... OK\nConfiguring Config-Trivial-0.81 ... OK\n...\n4 distributions installed\n(my_project)[netid@i0n1 ~]$\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/","title":"Anaconda","text":"<p>Known Issues with Anaconda</p> <p>Anaconda is known to cause potential issues on HPC Clusters. If it is possible to design your workflow around the native Python package manager Pip, we highly encourage you to do so. </p> <p>If you decide to use Anaconda on our HPC system, please read this page carefully and make yourself aware of the common problems and how to best avoid them. Thank you.</p>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#overview","title":"Overview","text":"<p>We have several versions of Anaconda installed as system modules for use. You can initialize these in your home directory for access and package management. </p> Version Accessibility 2020.02 <code>module load anaconda/2020.02</code> 2020.11 <code>module load anaconda/2020.11</code> 2022.05 <code>module load anaconda/2022.05</code>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#initializing-anaconda","title":"Initializing Anaconda","text":"<p>Initializing Anaconda in your account only needs to be performed once and is what makes Anaconda available and ready for customization (e.g., installing custom packages) in your account. </p> <p>Faster Reloading</p> <p>Conda will direct you to close and reopen your shell to complete the initialization process. You can skip this by running the command <code>source ~/.bashrc</code> listed in the instructions below. </p> <p>Turn Off Auto-Activate</p> <p>To ensure proper functioning of built-in system functions, turning off auto-activation is highly recommended. Do this by running <code>conda config --set auto_activate_base false</code> in a terminal following initialization.</p> <p>In an interactive session:</p> <pre><code>module load anaconda/&lt;version&gt;\nconda init bash                  \nsource ~/.bashrc  \nconda config --set auto_activate_base false\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#creating-a-conda-environment","title":"Creating a Conda Environment","text":"<p>Once conda has been configured following the steps above, you can create a local environment. This allows you to control the version of Python you want to use, install your own software, and even create custom Juypyter kernels (making your environment accessible in an OnDemand notebook). To do this, you can use the command <code>conda create</code>. For example, in an interactive session:</p> <pre><code>conda activate\nconda create --name py37 python=3.7 # Build a local environment with a specific version of python\nconda activate py37\n</code></pre> <p>To view the environments available to you, use the command</p> <pre><code>conda env list\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#installing-conda-packages-and-other-software","title":"Installing Conda Packages and Other Software","text":"<p>Once you have created a conda environment, you can install the packages you need. To do this, follow the software-specific installation instructions. This may be as simple as running <code>conda install &lt;my_package&gt;</code>, or it may involve installing a handful of dependencies. If the installation instructions ask you to create a new environment, you do not have to repeat this step. </p> <p>Once you have performed the install, you should then be able to access your software within this environment. If you are unable to load your software, check your active environment with</p> <pre><code>conda info\n</code></pre> <p>and the installed packages with </p> <pre><code>conda list\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#custom-jupyter-kernel","title":"Custom Jupyter Kernel","text":"<p>If you want to make one of your conda environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment:</p> <pre><code>conda activate\nconda activate &lt;your_environment&gt;\n</code></pre> <p>Next, pip-install Jupyter and use it to create a custom kernel using the command <code>ipython</code> and replacing <code>&lt;your_environment&gt;</code> with your own environment's name:</p> <pre><code>pip install jupyter\nipython kernel install --name &lt;your_environment&gt; --user\n</code></pre> <p>Once you've configured your kernel, go to Open OnDemand and start a Jupyter notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom name. For example, if you created a conda environment with the kernel name py38, you should see the following:</p> <p></p> <p>Once you've selected your environment, try checking the Python version in your notebook using the <code>sys</code> module. Additionally, for demonstration purposes, we'll check that a custom package installed in py38 (emoji) can be imported and is working. </p> <p></p>"},{"location":"software/popular_software/python_and_anaconda/python/","title":"Python Modules","text":""},{"location":"software/popular_software/python_and_anaconda/python/#overview","title":"Overview","text":"<p>Different versions of Python are available on HPC both as system modules as well as system software on each compute node. Python 2 is available but is no longer supported by the Python Foundation, so we recommend you use Python 3. Python version 3 requires the <code>python3</code> command or <code>pip3 list</code> to differentiate.  It is very different from Python version 2, so do not assume that Python 3 will work for you or that all older modules will work with version 3.</p>"},{"location":"software/popular_software/python_and_anaconda/python/#installation-and-package-policy","title":"Installation and Package Policy","text":"<p>We maintain a two tiered approach to Python packages</p> <ul> <li> <p>Tier 1: We install the basic Python packages that are required by most users (these are mostly libraries rather than packages, such as numpy and scipy). This is done for the versions of Python that we install as modules. Adding some packages might force an upgrade of numpy for example, which might break a user's environment that was dependent on the prior version.</p> </li> <li> <p>Tier 2: For packages that we do not provide we STRONGLY recommend the use of virtualenv, which is detailed below and provides a custom and easy to use personal Python environment.</p> </li> </ul>"},{"location":"software/popular_software/python_and_anaconda/python/#available-python-versions","title":"Available Python Versions","text":"<p>Python 2 is no longer officially supported by the Python Software Foundation.</p> <p>Multiple versions of Python are available on HPC. They are only available on compute nodes and are accessible either using a batch submission or interactive session. </p> Version Accessibility<sup>1</sup> Python 2.7.5 system version (no module) Python 3.6.8 system version (no module) Python 3.6.5 module load python/3.6/3.6.5 Python 3.8.2 module load python/3.8/3.8.2 Python 3.9.10 module load python/3.9/3.9.10 Python 3.11.4 module load python/3.11/3.11.4"},{"location":"software/popular_software/python_and_anaconda/python/#installing-python-packages-using-a-virtual-environment","title":"Installing Python Packages Using a Virtual Environment","text":"Virtual environment tips <ul> <li>Useful overview of virtualenv and venv: InfoWorld Article: Python virtualenv and venv do's and don'ts</li> <li>In the following instructions any module commands have to be run from an interactive session on a compute node</li> </ul> <p>One of the best things about Python is the number of packages provided by the user community. On a personal machine, the most popular method today for managing these packages is the use of a package manager, like <code>pip</code>. Unfortunately, these may require root access preventing you from being able to successfully install the libraries you need.</p> <p>There is an easy solution, however. You can use a virtual environment to create a personal python environment that will persist each time you log in. There is no risk of packages being updated under you for another user and allows greater control over your environment.</p> <p>Virtual Environment Instructions</p> <ol> <li> <p>Set up your virtual environment in your account. This step is done one time only and will be good for all future uses of your Python environment. You will need to be in an interactive session to follow along. </p> <p>Note: In the commands below, <code>/path/to/virtual/env</code> is the path to the directory where all of your environment's executables and packages will be saved. For example, if you use the path <code>~/mypyenv</code>, this will create a directory in your home called <code>mypyenv</code>. Inside will be directories <code>bin</code>, <code>lib</code>, <code>lib64</code>, and <code>include</code>. </p> Python Version \\(\\geq\\) 3.8Python Version \\(&lt;\\) 3.8 <pre><code>module load python/&lt;version&gt;\nvirtualenv --system-site-packages /path/to/virtual/env\n</code></pre> <pre><code>module load python/&lt;version&gt;\npython3 -m venv --system-site-packages /path/to/virtual/env\n</code></pre> </li> <li> <p>To use your new environment, you'll need to activate it. Inside your virtual environment, there's a directory called bin that has a file called activate. Sourcing this will add all of the paths needed to your working environment. To activate, run the following, replacing /path/to/virtual/env with the path specific to your account:</p> <p><pre><code>source /path/to/virtual/env/bin/activate\n</code></pre> 3. Once your environment is active, you can use pip to install your python packages.  You should first upgrade to the latest version of pip. For example, to add the pycurl package to the virtual environment:</p> <p><pre><code>pip install --upgrade pip\npip install pycurl\n</code></pre> 4. That's it! As long as your virtual environment is active, you will have access to the packages installed there. Virtual environments deactivate when you log out, so for each subsequent session or in batch jobs, you will just need to reactivate the environment to get access to your packages: <pre><code>module load python/&lt;version&gt;\nsource /path/to/virtual/env/bin/activate\n</code></pre></p> </li> </ol>"},{"location":"software/popular_software/python_and_anaconda/python/#custom-jupyter-kernel","title":"Custom Jupyter Kernel","text":"<p>Warning</p> <p>The default version of Python available in an OnDemand Jupyter Notebook is 3.8.2. If you would like to create a virtual environment using a standard Python module, you will need to use Python version 3.8.2. If you want to use a different version of python, you can use an Anaconda environment.</p> <p>If you want to make one of your virtual environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment (if you do not have an environment, refer to the sections above on how to do so):</p> <pre><code>module load python/3.8/3.8.2                     \nsource /path/to/your/virtual/environment/bin/activate\n</code></pre> <p>Once your environment is ready to go, pip-install <code>jupyter</code> and create your own custom kernel. The <code>--force-reinstall</code> flag will allow you to install the <code>jupyter</code> package in your local environment and will not affect the system version. This will create a directory in <code>~/.local/share/jupyter/kernels/</code> in your account. In the following commands, replace <code>&lt;your_environment&gt;</code> with the name of your own environment: </p> <pre><code>pip install jupyter --force-reinstall\nipython kernel install --name &lt;your_environment&gt; --user \n</code></pre> <p>Once you've successfully created your kernel, go to Open OnDemand and start a Jupyter Notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom kernel's name. For example, if the custom kernel's name was <code>py38-env</code>:</p> <p></p> <p>Once you've selected your environment, try loading a custom package you've installed to check that everything is working as expected. In this example, we'll check with the non-standard package <code>emoji</code> which has been installed in this environment:</p> <p></p> <ol> <li> <p>Note: The command <code>python</code> defaults to the system 2.7.5 version. To use Python 3, use the command <code>python3</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"software/popular_software/vscode_remote_connection/","title":"VSCode Remote Connection","text":""},{"location":"software/popular_software/vscode_remote_connection/#overview","title":"Overview","text":"<p>Visual Studio Code (VSCode) can be used to edit source code and other files on the HPC systems.  VSCode is available to run directly on HPC through the Open OnDemand system. </p> <p>VSCode can also be run locally on laptop or desktop computers and used to make a remote connection to the HPC systems.  This documentation is intended to detail the steps that must be taken to allow such a connection.  The details of how to make such a connection within the VSCode software itself is beyond the scope of this documentation. Refer to the VSCode documentation here: https://code.visualstudio.com/docs/remote/ssh-tutorial.  </p>"},{"location":"software/popular_software/vscode_remote_connection/#creating-and-using-a-remote-connection","title":"Creating and Using a Remote Connection","text":"<p>Remote VSCode sessions must connect to a compute node.  Briefly, the procedure is as follows:</p> <pre><code>graph LR\nA[Request resources&lt;br&gt;on a compute node] --&gt; B[Connect to the HPC VPN]\nB --&gt; C[Connect VSCode to the&lt;br&gt;allocated compute node]</code></pre> <p>The specific steps are these:</p> <ol> <li> <p>Set up ssh key authentication on the file transfer node (<code>filexfer.hpc.arizona.edu</code>), which will allow VSCode to directly connect to the HPC systems without using passwords or Duo authentication.  Our documentation for setting up ssh keys on the bastion host appears here: SSH Keys.  Follow the procedure documented on that page, but replace <code>hpc.arizona.edu</code> in any commands with <code>filexfer.hpc.arizona.edu</code>.</p> </li> <li> <p>Use the Cisco AnyConnect VPN software to connect to <code>vpn.hpc.arizona.edu</code>.  Cisco AnyConnect is the software that you would also use to connect to the general UArizona VPN.  Information on downloading and connecting Cisco AnyConnect appears here: https://it.arizona.edu/service/ua-virtual-private-network-vpn</p> </li> <li> <p>From the login node, start an interactive session for the length of time that you\u2019d like to connect VSCode. Note that executing <code>elgato</code> or <code>ocelote</code> first will request your session on those less busy clusters and it will likely start faster:</p> <pre><code>elgato\ninteractive -t 4:00:00 -a &lt;group_name&gt;\n</code></pre> <p>After the interactive session starts, type <code>hostname</code>, which will give something like <code>cpu25.elgato.hpc.arizona.edu</code>. This is the name that you will enter in your local VSCode as the remote computer to connect to. Note that each time you start an interactive session you will likely get a different node, and will therefore need to tell VSCode the specific host to connect to each time.</p> </li> </ol>"},{"location":"software/unsupported/","title":"Unsupported Software","text":"<p>Unfortunately, our HPC system is not configured to support all software use cases. We have summarized the main scenarios which cause software to be unsupported by our system below. Prior to submitting an installation request, double-check that your software requirements don't fall into one of these categories. While the HPC may not be able to support these cases, it may be possible that other campus resources are able to. We encourage you to contact services listed in our Community Resources page.</p> <p>The below list is not exhaustive and may be expanded as new scenarios are encountered. If you are unsure whether your desired software is supported, feel free to contact our consultants.</p>"},{"location":"software/unsupported/#external-connections","title":"External Connections","text":"<p>Software requiring external communications that utilize protocols other than SSH are not supported. </p>"},{"location":"software/unsupported/#job-managementscheduling","title":"Job Management/Scheduling","text":"<p>The UArizona HPC uses Slurm as its task scheduler and resource manager. Software requiring different a scheduler is therefore unsupported. </p> <p>Examples</p> <ul> <li>Apache Spark</li> <li>Sun Grid Engine</li> </ul>"},{"location":"software/unsupported/#persistent-databasesservers","title":"Persistent Databases/Servers","text":"<p>The UArizona HPC is not configured to support databases, server instances, or other persistent software-specific daemons. </p> <p>Examples</p> <ul> <li>Persistent SQL databases</li> <li>Application deployments</li> <li>SAS server</li> </ul>"},{"location":"software/unsupported/#windows-applications","title":"Windows Applications","text":"<p>While there are plenty of excellent Windows software suites available for scientific computing, they unfortunately cannot be run on HPC. There are, however, national resources available that may support your application. One example is JetStream2 available through an ACCESS Allocation. See our Community Resources page linked at the top of this page for more details.</p> <p>Examples</p> <ul> <li>ArcGIS</li> </ul>"},{"location":"software/user_installations/","title":"User Installations","text":""},{"location":"software/user_installations/#package-managers","title":"Package managers","text":"<p>Many popular programs, in particular Python and R have built-in package managers that can be used to collect new software from the internet and easily install them to your environment. See the section corresponding to your prefered language on the left under the \"Popular Software\" heading for more detailed instructions.</p>"},{"location":"software/user_installations/#manual-installations","title":"Manual installations","text":"<p>You are encouraged to download and compile(1) any desired software in your own account. Commands like <code>git clone</code> and <code>wget</code> are available to pull repositories from the internet. The installation process may involve changing install locations from system folders (e.g. <code>/usr/bin/xyz</code>) to user folders (e.g. <code>/home/ux/netid/</code> or <code>/groups/pi_netid/netid/</code>), and will vary substantially from software to software, so providing instructions for all cases is beyond the scope of this page. If you encounter difficulties while installing your own software, the HPC Consult team is available for assistance. </p> <ol> <li>on a compute node!</li> </ol> <p>While you cannot add or update system software or libraries using tools that require root privileges such as <code>yum</code>, many software packages can be installed locally without needing to be a superuser. Frequently linux packages make use of the \"<code>configure</code>, <code>make</code>, <code>make install</code>\" method which allows you to customize your installation location. An example of how to do this is shown below. </p> <p>Tip</p> <ul> <li>Software is not available on the login nodes. To install custom software, log into an interactive session.</li> <li>For a typical Linux installation, the default settings may attempt to install files in system locations. This is not permitted, so the installation process (specifically, the <code>./configure</code> step) needs to be changed so that files are installed somewhere you have write access to.  There is frequently done with the syntax <code>--prefix=/path/to/software</code>.</li> </ul> configure/make/make install examplecmake <p>Here is a typical example of installing software on a Linux cluster with the <code>configure</code>, <code>make</code>, <code>make install</code> method. We'll use a simple hello world example downloaded from https://ftp.gnu.org/gnu/hello/</p> <ol> <li> <p>Download and unpack the software</p> <pre><code>[user@cpu39 make_example]$ wget https://ftp.gnu.org/gnu/hello/hello-2.10.tar.gz\n[user@cpu39 make_example]$ tar xzvf hello-2.10.tar.gz \n[user@cpu39 make_example]$ cd hello-2.10\n</code></pre> </li> <li> <p>Configure your software</p> <p>The <code>./configure</code> command generates a Makefile tailored to the specific environment and requirements of the system where the software is being installed. The use of the <code>--prefix</code> option allows users to install the software to a custom directory, circumventing the standard root locations which users do not have permission to modify. In this example, we'll install the software to a directory called hello_world in our home. </p> <pre><code>[user@cpu39 hello-2.10]$ ./configure --prefix=$HOME/hello_install\n</code></pre> </li> <li> <p>Compile and install your software</p> <p>Tip</p> <p>Often, there is an additional option to test your software after compiling it and before installing it. Usually, this is something like <code>make test</code> or <code>make check</code></p> <p>The <code>make</code> command compiles the software according to the instructions provided in the Makefile generated by <code>./configure</code>. Once the software is compiled, <code>make install</code> will install the software in the directory you specified with the <code>--prefix</code> option.</p> <pre><code>[user@cpu39 hello-2.10]$ make\n[user@cpu39 hello-2.10]$ make install\n[user@cpu39 hello-2.10]$ ls $HOME/hello_install \nbin  share\n</code></pre> </li> <li> <p>Modify your environment</p> <p>Different environment variables control where the system looks for executables, libraries, header files, etc. Modifying your environment variables will allow you to use your new software without specifying the full paths. These variables can either be set manually on the command line for each new session, or can be added to your bashrc to make the changes permanent. For more information, see the sections Environment Variables and Hidden Files and Directories in our Bash cheat sheet.</p> <pre><code>[user@cpu39 hello-2.10]$ export PATH=$HOME/hello_install/bin:$PATH\n</code></pre> </li> <li> <p>Use your software</p> <pre><code>[user@cpu39 hello-2.10]$ hello\nHello, world!\n</code></pre> </li> </ol> <p>Here is a typical example of installing software on a Linux cluster with the <code>cmake</code> method. We'll use the software library Eigen as an example. </p> <ol> <li> <p>Download the software</p> <p>In this example, we'll <code>git clone</code> the source code from the Gitlab repository https://gitlab.com/libeigen/eigen</p> <pre><code>[user@cpu39 cmake_example]$ git clone https://gitlab.com/libeigen/eigen.git\n[user@cpu39 cmake_example]$ cd eigen/\n</code></pre> </li> <li> <p>Create and set your build environment</p> <p>Next, create a subdirectory called build where build files will be generated and stored. </p> <pre><code>[user@cpu39 eigen]$ mkdir build\n[user@cpu39 eigen]$ cd build\n</code></pre> <p>Additionally, you'll often need to set some environment variables to point to compilers and libraries. In this case, we'll set <code>CC</code> (which sets your C compiler), <code>CXX</code> (which sets your C++ compiler), and <code>FC</code> (which sets your Fortran compiler).</p> <pre><code>[user@cpu39 build]$ export CC=$(which gcc)\n[user@cpu39 build]$ export CXX=$(which g++)\n[user@cpu39 build]$ export FC=$(which gfortran)\n</code></pre> <p>Tip</p> <p>The command <code>which</code> returns a full filepath to an executable. Running something like <code>export FOO=$(which foo)</code> will take the output of <code>which foo</code> and store it as the environment variable <code>FOO</code></p> </li> <li> <p>Configure your build</p> <p>Use <code>cmake</code> to configure your build. Use <code>-DCMAKE_INSTALL_PREFIX</code> to set a custom installation directory that you have access to. This will prevent the installation process from trying to access default root-owned locations which users don't have permission to modify. The <code>..</code> is a shortcut for the directory one level above which is where the CMakeLists.txt file lives.</p> <pre><code>[user@cpu39 build]$ cmake -DCMAKE_INSTALL_PREFIX=$HOME/eigen_install ..\n</code></pre> </li> <li> <p>Compile and install your software</p> <pre><code>[user@cpu39 build]$ make\n[user@cpu39 build]$ make install\n[user@cpu39 build]$ ls $HOME/eigen_install\ninclude  share\n</code></pre> </li> <li> <p>Modify your environment</p> <p>Different environment variables control where the system looks for executables, libraries, header files, etc. Modifying your environment variables will allow you to use your new software without specifying the full paths. These variables can either be set manually on the command line for each new session, or can be added to your bashrc to make the changes permanent. For more information, see the sections Environment Variables and Hidden Files and Directories in our Bash cheat sheet.</p> <p>In this case, we'll set <code>INCLUDE</code> and <code>CPPFLAGS</code></p> <pre><code>[user@cpu39 build]$ export INCLUDE=$HOME/eigen_install:$INSTALL\n[user@cpu39 build]$ export CPPFLAGS=\"-I$HOME/eigen_install $CPPFLAGS\"\n</code></pre> </li> </ol>"},{"location":"storage_and_transfers/overview/","title":"Overview","text":"<p>In addition to compute resources, the HPC system also provides access to various storage options, and data transfer methods. Data management is a crucial part of every workflow, so we provided tools and recommended practices to help you with this process.</p>"},{"location":"storage_and_transfers/overview/#storage-summary","title":"Storage Summary","text":"<p>The main storage array that is accessible to the HPC is the Primary HPC Storage, which provides 3 shares, all accessible from both login and compute nodes. </p> <ul> <li> <p><code>/home/u&lt;NM&gt;/&lt;netid&gt;/</code> provides 50 GB of storage for the user in their home directory. Note that '<code>u&lt;NM&gt;</code>' refers to the various user parent directories such as <code>u16</code>. To find this directory, simply SSH into a login node, and type <code>pwd</code>, since it is the default directory.</p> </li> <li> <p><code>/groups/&lt;pi-netid&gt;/</code> provides 500 GB of storage to each PI group, to be shared among members.</p> </li> <li> <p><code>/xdisk/&lt;pi-netid/</code> provides up to 20 TB of storage to each PI group, to be shared among members. Xdisk shares are not provided by default and must be requested by the PI through the HPC user portal. Xdisk shares are also subject to a 300 day expiration cycle. </p> </li> </ul> <p>Check your disk usage</p> <p>To check the usage of each of these shares, run the command <code>uquota</code> from a login node. </p> <p>Additional storage options, such as rental, Tier 2 AWS Glacier, Google Drive, and more are detailed in the Storage tab to the left. </p>"},{"location":"storage_and_transfers/overview/#transfer-summary","title":"Transfer Summary","text":"<p>There are many different utilities available to transfer files to and from the HPC filesystem. Details are given under the Transfers tab to the left, and the most popular options are summarized below. </p> <ul> <li> <p>Open OnDemand</p> <p>Use the browser to navigate, upload, and download. File size limit of 64 MB applied to uploads. </p> </li> <li> <p>Globus </p> <p>A powerful browser tool to automate transfers. </p> </li> <li> <p>Command Line Utilities</p> <p>The familiar options of <code>scp</code>, <code>rsync</code> and more are available to initiate transfers through the command line.</p> </li> </ul>"},{"location":"storage_and_transfers/overview/#best-practices","title":"Best Practices","text":"<p>The shared file system on HPC is the location for everything in <code>/home</code>, <code>/groups</code>, and <code>/xdisk</code>. The <code>/tmp</code> directory is also available to users, and refers to the local disk on each node. Your I/O activity can have dramatic activity on other users. Extreme read/write activity can cause bottlenecks and may be cancelled without warning. It is generally best to limit I/O whenever possible to avoid straining the system. The HPC consult team is available to help optimize workflows that may be impacted by I/O. </p> <ul> <li> <p>Be aware of I/O load.</p> <p>Running multiple instances of jobs performing significant I/O activity may be detrimental to the system, especially if these occur within the same subdirectories. It may be best to read in data at the beginning of a workflow, perform the entire analysis, then write at the very end. Reconfiguring your workflow to limit I/O may cost some time up front, but will most likely be made back through faster job completion. </p> <p>If you are running array jobs, please be cognizant of your I/O activity.</p> </li> <li> <p>Use the file transfer nodes for large data transfers</p> <p>Login and compute nodes are not designed for large file transfers and transfers intiated here may result in network problems. The data transfer nodes (DTNs) are specifically set up for moving large amounts of data and are accessible via the hostname <code>filexfer.hpc.arizona.edu</code>.</p> </li> <li> <p>Use /tmp for working space</p> <p>If you have multiple jobs that will use the same data, consider copying it to <code>/tmp</code> and run multiple jobs. This can increase performance and reduce I/O load.</p> </li> <li> <p>Avoid storing many files in a single directory</p> <p>Hundreds of files is probably ok; tens of thousands is not.    </p> </li> <li> <p>Avoid opening and closing files repeatedly in tight loops</p> <p>If possible, open files once at the beginning of your workflow/program, then close them at the end.</p> </li> <li> <p>Watch your quotas</p> <p>You are limited in capacity and file count. Use <code>uquota</code>. In <code>/home</code> the scheduler writes files in a hidden directory assigned to you.</p> </li> <li> <p>Avoid frequent snapshot files</p> <p>This can stress the storage.</p> </li> <li> <p>Limit file copy sessions</p> <p>You share bandwidth with others. Two or three scp sessions are probably ok; &gt; 10 is not.</p> </li> <li> <p>Consolidate files</p> <p>If you are transferring many small files, consider collecting them in a tarball first.</p> </li> <li> <p>Use parallel I/O</p> <p>Some modules enable parallelized file operations, such as <code>phdf5</code></p> </li> </ul>"},{"location":"storage_and_transfers/storage/hpc_storage/","title":"HPC Storage","text":""},{"location":"storage_and_transfers/storage/hpc_storage/#overview","title":"Overview","text":"<p>The University\u2019s Research Data Center provides data storage for active analysis on the high-performance computers (HPCs). Using central computing storage services and resources, University researchers, faculty researchers, and post-doctoral researchers are able to:</p> <ul> <li>Share research data in a collaborative environment with other UArizona affiliates on the HPC system</li> <li>Store large-scale computational research data</li> <li>Request additional storage for further data analysis</li> </ul> <p>The storage is mounted as a filesystem and all the clusters have access to the same filesystems.</p> <p>Every user has access to individual and shared storage on the system where they can host data for active analyses. A summary of these locations is shown below:</p> Path Description Quota Duration <code>/home/uxx/netid</code> An individual storage allocation provided for every HPC user 50 GB Accessible for the duration of user's account <code>/groups/pi_netid</code> A communal storage allocation provided for every research group 500 GB Accessible for the duration of a PI's account <code>/xdisk/pi_netid</code> Temporary communal storage available for every group on request. See xdisk section below for details. 200 GB to 20  TB Up to 300 days <code>/tmp</code> Local storage available on individual compute nodes. \\(&lt;\\) 800 GB to 1.4 TB Only accessible for the duration of a job's run."},{"location":"storage_and_transfers/storage/hpc_storage/#checking-your-storage-usage","title":"Checking Your Storage Usage","text":"Command LineUser Portal <p>To check your storage usage, on a compute node, file transfer node, or login node, use the command <code>uquota</code>. This will show you all the spaces you have access to, their quotas, and current usage. <pre><code>(puma) [netid@junonia ~]$ uquota\n                                        used  soft limit  hard limit\n/groups/pi_netid                            6.6G      500.0G      500.0G\n/home                                      37.1G       50.0G       50.0G\n/xdisk/pi_netid                            12.9G        9.8T        9.8T\n</code></pre></p> <p>You can check your storage allocation through our online user portal by navigating to the Storage tab and clicking Check Disk Quotas:</p> <p></p>"},{"location":"storage_and_transfers/storage/hpc_storage/#xdisk","title":"xdisk","text":""},{"location":"storage_and_transfers/storage/hpc_storage/#what-is-xdisk","title":"What is xdisk?","text":"<p>xdisk is a temporary storage allocation available to all PIs and offers up to 20 TB of usable space for their group for up to 300 days. PIs may only have one active xdisk at a time.</p> <p>A PI can request an allocation either via the command line or through our web portal (no paperwork necessary!). Only faculty members (PIs) may request, alter, or delete an allocation from the command line. Members of their research group may be delegated management rights allowing them to manage a group's xdisk on their PI's behalf through our web portal.</p> <p>Once an xdisk allocation is created, it is immediately available for use. Groups can find their allocations under <code>/xdisk/pi_netid</code>. By default, a subdirectory is created for each group member under <code>/xdisk/pi_netid/netid</code>. If a group member is added after the allocation is created, a directory is not automatically created for them. To add one, reach out to our consultants.</p> <p>Because xdisk allocations are temporary, they will be removed as soon as their time limit is reached. Warnings will be sent to every group member at their netid@arizona.edu addresses beginning one month before the expiration. It is the group's responsibility to renew xdisk allocations or copy files to an alternate storage location prior to the expiration date. Once an xdisk allocation expires, everything in it is permanently deleted. The data is not deleted immediately, but on a regular basis or when the space is needed, so do not rely on it being recovered after expiration.</p> <p>PIs may request a new xdisk allocation immediately after their previous one has expired. This ensures groups will always have access to increased storage on HPC on a rolling basis with the requirement that housekeeping be done once per academic year. </p>"},{"location":"storage_and_transfers/storage/hpc_storage/#requesting-modifying-and-deleting-an-allocation","title":"Requesting, Modifying, and Deleting an Allocation","text":"Requesting an AllocationModifying an AllocationDeleting an Allocation <p>Warning</p> <p>If a group has an active xdisk allocation, a new one cannot be created until the active allocation expires or is deleted.</p> <p>PIs or delegates can request an xdisk allocation at any time through the user portal. Under the Storage tab, select Manage XDISK</p> <p></p> <p>This will open a web form where you can enter your size and duration requirements. The maximum size that can be requested is 20000 GB and the maximum duration is 300 days. If a PI has created multiple research groups, you can specify the desired group ownership for the allocation from the Group dropdown menu. Once you click Ok, your allocation should immediately be available.</p> <p></p> <p>PIs or delegates may manage their xdisk allocation at any time through the user portal. Under the Storage tab, select Manage XDISK</p> <p></p> <p>This will open a form which will allow you to modify the size and duration of your xdisk. Xdisk allocations cannot be increased beyond 20000 GB and the maximum duration of 300 days. Note: the Group field may only be modified at the time of the allocation's creation.</p> <p></p> <p>PIs or delegates may delete their xdisk allocation at any time through the user portal. Under the Storage tab, select Delete XDISK</p> <p></p> <p>Clicking this link will open a window with a prompt. Type confirm and then select Delete XDISK to complete the process.</p> <p></p> <p>If you would like to request a new xdisk, you may do so as soon as the request is processed. Note: sometimes processing the request can take a few minutes, depending on the number of files and the size of the allocation.</p>"},{"location":"storage_and_transfers/storage/hpc_storage/#cli-commands-pis-only","title":"CLI Commands (PIs only)","text":"<p>Warning</p> <p>The xdisk CLI commands are usable by PIs only. Group delegates can manage allocations via the user portal.</p> <p><code>xdisk</code> is a locally written utility for PI's to create, delete, resize, and expire (renew) xdisk allocations. Any PIs who wish to utilize the CLI to manage their allocations can do so using the syntax shown below:</p> xdisk Function Command Examples Display xdisk help <pre><code>xdisk -c help</code></pre> <pre><code>$ xdisk -c help</code></pre> View Current Information <pre><code>xdisk -c query</code></pre> <pre><code>$ xdisk -c queryXDISK on host: ericidle.hpc.arizona.eduCurrent xdisk allocation for &lt;pi_netid&gt;:Disk location: /xdisk/&lt;pi_netid&gt;Allocated size: 200GBCreation date: 3/10/2020 Expiration date: 6/8/2020Max days: 45    Max size: 1000GB</code></pre> Create an xdisk <pre><code>xdisk -c create -m [size in gb] -d [days]</code></pre> <pre><code>$ xdisk -c create -m 300 -d 30Your create request of 300 GB for 30 days was successful.Your space is in /xdisk/&lt;pi_netid&gt;</code></pre> Extend xdisk Expiration Date <pre><code>xdisk -c expire -d [days]</code></pre> <pre><code>$ xdisk -c expire -d 15Your extension of 15 days was successfully processed</code></pre> Resize an xdisk Allocation <pre><code>xdisk -c size -m [size in gb]</code></pre> <pre><code>$ # Assuming an initial xdisk allocation size of 200 gb$ xdisk -c size -m 200XDISK on host: ericidle.hpc.arizona.eduYour resize to 400GB was successful$ xdisk -c size -m -100XDISK on host: ericidle.hpc.arizona.eduYour resize to 300GB was successful</code></pre> Delete an xdisk Allocation <pre><code>xdisk -c delete</code></pre> <pre><code>$ xdisk -c delete`Your delete request has been processed</code></pre>"},{"location":"storage_and_transfers/storage/overview/","title":"Storage Overview","text":""},{"location":"storage_and_transfers/storage/overview/#where-should-i-store-my-data","title":"Where Should I Store My Data?","text":"<ol> <li>Data undergoing active analyses should be stored in HPC's local High Performance Storage.</li> <li>Large amounts of data not requiring immediate access from our HPC compute nodes can be stored at reasonable rates on our Rental Storage. </li> <li>RDAS is a research data service which supports the mounting of SMB shares. The supported operating systems are MacOS, Linux, and Windows. It provides 5TB of free storage. </li> <li>Research data not requiring immediate access should be stored in General Research Data Storage (Tier 2). For example:<ol> <li>Large datasets where only subsets are actively being analyzed</li> <li>Results no longer requiring immediate access</li> <li>Backups (highly encouraged!)</li> </ol> </li> <li>Data that require HIPAA-compliance can be stored on Soteria (currently in the pilot phase).</li> </ol> <p> <pre><code>graph LR\n  A[My data are...] --&gt; B{Controlled?}\n  B--&gt;|Yes| C{HIPAA?};\n  C--&gt;|Yes| D[&lt;a href=\"../../../resources/secure_hpc/overview/\"&gt;Soteria&lt;/a&gt;];\n  C--&gt;|No| E[Unsupported];\n  B--&gt;|No| F{Archival?}\n  F--&gt;|Yes| G[&lt;a href=\"../../storage/tier2_storage/\"&gt;AWS Tier 2&lt;br&gt;Storage&lt;/a&gt;]\n  F--&gt;|No| H{Need&lt;br&gt;HPC&lt;br&gt;compute?}\n  H--&gt;|Yes| I{Under&lt;br&gt;20 TB?}\n  I--&gt;|Yes| J[&lt;a href=\"../../storage/hpc_storage/\"&gt;HPC Storage&lt;/a&gt;]\n  I--&gt;|No| K[&lt;a href=\"../../storage/rental_storage/\"&gt;Rental Storage&lt;/a&gt;]\n  H--&gt;|No| L{Under&lt;br&gt;5 TB?}\n  L--&gt;|No| K\n  L--&gt;|Yes| M[&lt;a href=\"../../storage/rdas_storage/\"&gt;R-DAS Storage&lt;/a&gt;]</code></pre> </p>"},{"location":"storage_and_transfers/storage/overview/#storage-option-summary","title":"Storage Option Summary","text":"Purpose Capacity Cost Restricted Data Access Duration Backup Primary HPC Storage Research data. Supports compute. Directly attached to HPC. <code>/home</code>: 50 GB<code>/groups</code>: 500 GB<code>/xdisk</code>: 20 TB Free Not for restricted data Directly mounted on HPC. Also uses Globus and DTNs. Long term. Aligns with HPC purchase cycle. No R-DAS Research Desktop Attached Storage - SMB shares 5 TB Free Not for restricted data Mounted to workstations as shares Long term No Rental Storage Research data. Large datasets. Typically for staging to HPC Rented per TB per year Rental rate: $47.35 per TB per year Not for restricted data Uses Globus and DTNs. Copy data to Primary Long term. Aligns with HPC purchase cycle No Tier 2 Typically research data. Unused data is archived 15 GB to TBs Tier-based system. First 1 TB of active data and archival data are free. Active data &gt; 1 TB is paid. Not for restricted data Uses Globus and AWS command line interface Typically long term since use of Glacier is free and slow Archival ReData Research data. Managed by UA Libraries Quota system Free Not for restricted data Log in and fill out fields, then upload Longer than 10 years No Soteria HIPAA Secure data enclave Individual requests Free upon qualification Restricted data; HIPAA, ePHI HIPAA training required, followed by request process Long term No Box General Data 50 GB Free Not for restricted data Browser Long term No Google Drive General data 15 GB Free. Google rates for amounts &gt; 15 GB Not for restricted data Browser Unlimited usage expires March 1, 2023 No"},{"location":"storage_and_transfers/storage/overview/#nih-data-management-and-sharing-policy","title":"NIH Data Management and Sharing Policy","text":"<p>The NIH has issued a new data management and sharing policy, effective January 25, 2023. The University Libraries now offers a comprehensive guide for how to navigate these policies and what they mean for you.</p> <p>What's new about the 2023 NIH Data Management and Sharing Policy?   Previously, the NIH only required grants with $500,000 per year or more in direct costs to provide a brief explanation of how and when data resulting from the grant would be shared.   The 2023 policy is entirely new. Beginning in 2023, ALL grant applications or renewals that generate Scientific Data must now include a robust and detailed plan for how you will manage and share data during the entire funded period. This includes information on data storage, access policies/procedures, preservation, metadata standards, distribution approaches, and more. You must provide this information in a data management and sharing plan (DMSP). The DMSP is similar to what other funders call a data management plan (DMP).   The DMSP will be assessed by NIH Program Staff (though peer reviewers will be able to comment on the proposed data management budget). The Institute, Center, or Office (ICO)-approved plan becomes a Term and Condition of the Notice of Award.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/","title":"Research Desktop Attached Storage (R-DAS)","text":""},{"location":"storage_and_transfers/storage/rdas_storage/#overview","title":"Overview","text":"<p>R-DAS not an HPC filesystem</p> <p>R-DAS storage is not mounted on HPC compute or login nodes. Data stored in R-DAS will need to be copied over to the HPC filesystem in order to be accessible to jobs. Instructions on how to access R-DAS from HPC are included below.</p> <p>Group Sharing</p> <p>Faculty members/PIs can share their allocations with group members. To do so, in step 6 in the **Accessing Your R-DAS Allocation\" section below, group members will choose the allocation with their faculty member's/PI's NetID.</p> <p>No Controlled Data</p> <p>This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. </p> <p>On October 16, 2023, we went live with the Research Desktop Attached Storage Array (R-DAS). R-DAS provides up to 5 TB of no-cost storage capacity for each PI group. Our requirement was to enable our users to easily share data with other research group members. You can treat the allocation as a drive mounted on your local computer. R-DAS is intended for storing open research data, but not controlled or regulated data.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#technical-requirements","title":"Technical Requirements","text":"<p>R-DAS is a storage service backed by a Qumulo branded storage array. It supports the mounting of SMB shares for SMB 3.1. The supported operating systems are MacOS (Monterey or higher), Linux (kernel 3.7 or higher), and Windows (Windows 10 or 11).</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#performance","title":"Performance","text":"<p>The storage array is located in the Research Data Center to benefit from the network infrastructure in the Computer Center. The performance you experience will depend on your network connectivity. The best case is likely wired ethernet in a newer building. Off campus usage requires connection to the VPN, and so performance can be variable. Our testing off campus regularly reached 3 MB/s.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#requesting-an-allocation","title":"Requesting an Allocation","text":"<p>PIs can request an allocation on R-DAS from https://portal.hpc.arizona.edu/portal</p> <ol> <li>Go to the Storage tab</li> <li> <p>Select Create Shared Desktop Storage under Research Desktop Storage</p> <p></p> </li> <li> <p>Select Create from the window that opens. </p> <p></p> </li> <li> <p>A window will open with the MOU agreement. Review it and, if it is acceptable to you, select Agree.</p> <p></p> </li> <li> <p>You can now select the View Shared Desktop Storage option from the main Storage page in the user portal</p> <p></p> </li> </ol>"},{"location":"storage_and_transfers/storage/rdas_storage/#accessing-your-r-das-allocation","title":"Accessing Your R-DAS Allocation","text":"<p>Tip</p> <p>UA IP Address Required: To access your R-DAS allocation you need to be connected to either the UA campus network, or the UA SSL VPN. For information about connecting to a VPN, see VPN - Virtual Private Network. If you are accessing your R-DAS allocation from an HPC cluster, then you are already on the UA campus network and do not need to connect to the UA SSL VPN. </p> <p>R-DAS can be accessed from Linux, MacOS, or Windows. The screenshots are intended to be visual aids, but they include information from the consulting team. When you proceed, please enter your own information.</p> <p>Choose your operating system</p> Linux/HPCMac OSWindows <p>No <code>sudo</code> on HPC</p> <p>To connect to R-DAS from HPC, do not attempt to run <code>sudo</code> commands, these are only meant for your personal Linux machines. All required packages are already installed on the HPC clusters.</p> <p>First, install the necessary software packages to access your allocation</p> <p>Choose your distribution</p> Debian/UbuntuFedora/CentOSOther Linux Distributions <pre><code>sudo apt install samba gvfs-backends smbclient\n</code></pre> <pre><code>sudo yum install samba gvfs-samba samba-client \n</code></pre> <p>Please check the documentation of your distribution.</p> <p>Next, access your allocation</p> <p>Choose your connection method</p> GUICLI <p>On a desktop environment, such as MATE, GNOME, KDE, you can mount your R-DAS allocation as a local drive with the corresponding file manager (Caja on MATE, GNOME Files, Dolphin on KDE). On HPC, you can use a virtual desktop.</p> <ol> <li> <p>Open the file manager (Caja, GNOME Files, Dolphin)(1).</p> <ol> <li>On the HPC Interactive Desktop's MATE desktop environment, you can launch Caja by clicking the file drawer like icon in the top bar, or by selecting Applications &gt; System Tools &gt; Caja.</li> </ol> </li> <li> <p>Press Ctrl+L. This makes the location bar editable.</p> </li> <li> <p>Enter <code>smb://rdas.hpc.arizona.edu</code> in the location bar, and press ++Enter++.</p> <p></p> </li> <li> <p>A few moments later a window opens, prompting for your Username (<code>BLUECAT\\</code> followed by your UA NetID) and Password (UA NetID password). After entering the details, select Connect (on other file managers this may be OK). Some file managers, such as Caja and GNOME Files, also have a Domain field, whereas others, like Dolphin, do not. Either way, you do not need to modify its default value.</p> <p></p> </li> <li> <p>Select the allocation named after your group from the list of allocations displayed.</p> <p></p> </li> <li> <p>On some file managers, such as Dolphin, you can right away access your allocation by double clicking on it. On others, such as Caja and GNOME Files, double clicking on it will open another window prompting for your Username (<code>BLUECAT\\</code> followed by your UA NetID) and Password (UA NetID password). Select Connect as user, enter the details, and select Connect. Your allocation will be mounted as a local drive.</p> <p></p> </li> </ol> <p>You can interactively browse your R-DAS allocation with <code>smbclient</code>: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt;\n</code></pre></p> <p>The <code>&lt;share&gt;</code> is the PI group that you belong to, and <code>&lt;username&gt;</code> is your UA NetID. The command will prompt for a password where you will enter your UA NetID password. This will start an <code>smb</code> shell. For example:</p> <pre><code>~ $ smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\sohampal -U BLUECAT\\\\sohampal\nPassword for [BLUECAT\\sohampal]:\nTry \"help\" to get a list of possible commands.\nsmb: \\&gt;\n</code></pre> <p>Try <code>help</code> to get a list of possible commands:</p> <pre><code>smb: \\&gt; help\n?              allinfo        altname        archive        backup        \nblocksize      cancel         case_sensitive cd             chmod         \nchown          close          del            deltree        dir           \ndu             echo           exit           get            getfacl  \n. . .\n</code></pre> <p>Use the <code>-L</code> flag to get the list of shares on the Array. For example:</p> <pre><code>smbclient -L \\\\\\\\rdas.hpc.arizona.edu -U BLUECAT\\\\sohampal\nPassword for [BLUECAT\\sohampal]:\n\nSharename       Type      Comment\n---------       ----      -------\nQ$              Disk      Default root share for SRVSVC.\nipc$            IPC       Named Pipes\nupgrade         Disk      for qumulo upgrades\ntmerritt        Disk      Desktop share for tmerritt created on 09/12/2023 12:24 PM\n. . .\n</code></pre> <p>Any command that you can run interactively from the smb shell, you can also run non-interactively with the <code>-c</code> flag. For example, to list the files and directories in your share, run: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt; -c 'ls'\n</code></pre> You can also combine multiple commands with <code>;</code>. For example to list the contents in a directory in your share, run: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt; -c 'cd &lt;directory&gt;;ls'\n</code></pre> To copy a file from your local system to your R-DAS share use <code>put</code>, and from your R-DAS share to your local system use <code>get</code>: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt; -c 'put &lt;file&gt;'\n</code></pre> To learn more about smbclient, run <code>man smbclient</code>.</p> <p>If you are on a Mac, then you can mount your R-DAS allocation as a local drive with the following steps:</p> <ol> <li>Go to Finder</li> <li>Select Go from the top menu bar.</li> <li>From the drop-down menu, select Connect to Server.</li> <li> <p>In the window that opens, enter <code>smb://rdas.hpc.arizona.edu</code> in the address bar, and select Connect.</p> <p></p> </li> <li> <p>After a few moments a window opens prompting for your Name (UA NetID) and Password (UA NetID password). After entering the details, select Connect.</p> <p></p> </li> <li> <p>A window will open with the list of allocations on the array. Select the allocation named after your group, and then select OK.</p> <p></p> </li> </ol> <p>If you are on Windows, you can mount your R-DAS allocation as a local drive with the following steps:</p> <ol> <li>Open Windows Explorer.</li> <li> <p>Enter <code>\\\\rdas.hpc.arizona.edu</code> in the location bar, and press Enter.</p> <p></p> </li> <li> <p>A few moments later a window will open, prompting for your Username (<code>BLUECAT\\</code> followed by your UA NetID) and Password (UA NetID password). After entering the details, select OK.</p> <p></p> </li> <li> <p>Select the allocation named after your group from the list of allocations displayed. You can directly open the allocation by double-clicking on it, or mount it by right clicking on it and selecting Map network drive.</p> <p></p> </li> </ol>"},{"location":"storage_and_transfers/storage/rental_storage/","title":"Rental Storage","text":""},{"location":"storage_and_transfers/storage/rental_storage/#overview","title":"Overview","text":"<p>Accessibility</p> <p>Your <code>/rental</code> allocation is only mounted on our Data Transfer Nodes and is not directly accessible from the HPC login or compute nodes. </p> <p>No Controlled Data</p> <p>This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. </p> <p>The storage we offer is configured with consideration given to the direct relationship between capacity, performance and cost. We offer a rental storage solution that has less performance than our primary SSD array making it affordable for researchers to rent. This storage array is located in the Research Data Center and is mounted on our data transfer nodes which makes it more accessible than most other options. Data in your rental space will be accessible via the command line and the graphical transfer application Globus. </p>"},{"location":"storage_and_transfers/storage/rental_storage/#pricing","title":"Pricing","text":""},{"location":"storage_and_transfers/storage/rental_storage/#cost-per-year","title":"Cost per Year","text":"<p>The first-year rate is $94.50 per TB, and RII will provide matching funds for first-year allocations to make the actual first-year cost to researchers $47.35. These matching funds will be applied automatically, so in practice you will see the $47.35 rate. The ongoing rate after year one is $47.35 per TB per year.</p>"},{"location":"storage_and_transfers/storage/rental_storage/#billing","title":"Billing","text":"<p>Researchers must provide a KFS account for this service. Charges will be applied at the end of the academic year (June).</p>"},{"location":"storage_and_transfers/storage/rental_storage/#size-modifications","title":"Size Modifications","text":"<p>If the size of your allocation is modified, you will be billed for the maximum amount of space reserved during that fiscal year. </p>"},{"location":"storage_and_transfers/storage/rental_storage/#data-location","title":"Data Location","text":"<p>Danger</p> <p>Your <code>/rental</code> allocation is only mounted on our Data Transfer Nodes and is not directly accessible from the HPC login or compute nodes. </p> <p>Your rental space will be on a storage array in our Research Data Center and mounted on our data transfer nodes (hostname: <code>filexfer.hpc.arizona.edu</code>). Your space will be findable under </p> <pre><code>/rental/&lt;pi_netid&gt;\n</code></pre> <p>Where <code>&lt;pi_netid&gt;</code> is the NetID of the faculty member who requested the allocation.</p>"},{"location":"storage_and_transfers/storage/rental_storage/#data-transfers","title":"Data Transfers","text":"<p>A few data transfer options are Globus, <code>sftp</code>, and <code>scp</code> which will allow you to move data external to the data center to your allocation.</p> <p>For data transfers between HPC storage (<code>/home</code>, <code>/groups</code>, or <code>/xdisk</code>) and your rental allocation, you may also <code>ssh</code> into <code>filexfer.hpc.arizona.edu</code> and use <code>mv</code> or <code>cp</code>. For large copies done using this method, we recommend using a <code>screen</code> session to prevent timeouts. For example: <pre><code>[netid@home ~]$ ssh netid@filexfer.hpc.arizona.edu\nAuthorized uses only. All activity may be monitored and reported.\nLast login: Fri Sep 15 10:53:27 2023\n[netid@sdmz-dtn-3 ~]$ cd /rental/pi/netid/example\n[netid@sdmz-dtn-3 example]$ screen\n[netid@sdmz-dtn-3 example]$ cp -r /xdisk/pi/CONTAINERS/ $PWD/CONTAINERS\n[netid@sdmz-dtn-3 example]$ ls\nCONTAINERS\n[netid@sdmz-dtn-3 example]$ exit # exits screen session\n[netid@sdmz-dtn-3 example]$ exit # exits filexfer node\nlogout\nCome again soon!\nConnection to filexfer.hpc.arizona.edu closed.\n[netid@home ~]$\n</code></pre></p>"},{"location":"storage_and_transfers/storage/rental_storage/#how-to-request-rental-storage","title":"How to Request Rental Storage","text":"<p>Warning</p> <p>Allocations up to 20TB in size can be requested through the user portal. For allocations larger than 20TB, contact our consulting team for help.</p> <p>Tip</p> <p>It can take a few days to process the request is it has to route through the Financial Services Office (FSO). You will receive an email confirmation once it is complete.</p> <ol> <li> <p>PIs or Group Delegates can request rental storage on behalf of their group. To do so, navigate to the User Portal in your browser, then choose the Storage tab</p> <p></p> </li> <li> <p>Select Submit Rental Storage Request under the Rental Storage heading and fill out the form. </p> <p></p> </li> <li> <p>Once your space has been created, you will receive an email notification that it is ready for use.</p> </li> </ol>"},{"location":"storage_and_transfers/storage/rental_storage/#resizing-your-allocation","title":"Resizing Your Allocation","text":"<p>Warning</p> <p>Resizing allocations up to 20TB can be done the user portal. For allocations larger than 20TB, contact our consulting team for help.</p> <p>Your rental allocation can be resized through the user portal by navigating to the Storage tab and selecting Modify Rental Quota under the Rental Storage heading.</p> <p></p>"},{"location":"storage_and_transfers/storage/rental_storage/#checking-your-usage","title":"Checking Your Usage","text":"<p>You can check your allocation's size and current usage either through the user portal or on the command line.</p> User PortalCommand Line <p>In the user portal, navigate to the Storage tab and select Check Rental Quota from under the Rental Storage heading</p> <p></p> <p>From an HPC login node, enter the command <code>uquota</code>, for example: <pre><code>[user@local_machine ~]$$ ssh netid@hpc.arizona.edu\n[netid@gatekeeper ~]$ shell\n(puma) [netid@wentletrap ~]$ uquota\n                                        used  soft limit  hard limit\n/groups/pi                                163.4G      500.0G      500.0G\n/home                                      13.2G       50.0G       50.0G\n/rental/pi                                 11.8G      931.3G      931.3G\n/xdisk/pi                                   9.0T        9.9T        9.9T\n</code></pre></p>"},{"location":"storage_and_transfers/storage/tier2_storage/","title":"Tier 2 AWS Storage","text":""},{"location":"storage_and_transfers/storage/tier2_storage/#overview","title":"Overview","text":"<p>No Controlled Data</p> <p>This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. </p> <p>Research Technologies in partnership with UITS has implemented an AWS rental storage solution. This AWS option is called Tier 2 which differs from Tier 1, the primary storage that is directly connected to the HPC clusters. Tier 1 is very fast, very expensive, and immediately available for active analyses. Tier 2 is intended for data not immediately undergoing active analyses and for backups (highly encouraged!). Researchers can use the software Globus to move data to Tier 2, and can also move data from other sources (called endpoints) like Google Drive. The data in Tier 2 will not be mounted on HPC, and so Globus will be used to move it back to Tier 1 if needed.</p> <p>AWS storage is organized in \"buckets.\" One S3 intelligent tiering bucket is supported per KFS account. A PI could sponsor multiple buckets by submitting separate requests each with a unique KFS number, and then provide permissions as they see fit. </p>"},{"location":"storage_and_transfers/storage/tier2_storage/#data-lifecycle","title":"Data Lifecycle","text":"<p>Avoid large numbers of files</p> <p>Because AWS is set up for automatic archiving, files are moved into tiers where restore requests need to be submitted for each individual file that needs to be downloaded after a period of time. We strongly recommend archiving directories (.zip, .tar.gz files, etc) prior to moving them to AWS. This will significantly speed up your data transfers as well as reduce the complexity of file restorations. If you transfer hundreds or thousands of files to AWS, restore requests may take days or weeks to process. </p> <p>Small files</p> <p>Warning: Very small files (less than 128KB ) are not subject to intelligent tiering and are not migrated to Glacier/Deep Glacier. This means they are permanently stored in the paid storage class. If you have many small files, we recommend making archives of your directories (.tar.gz, .zip, etc) prior to uploading them to AWS. This will also reduce transfer times significantly. </p> <p>Tier 2 AWS buckets use intelligent tiering to determine the archival status of files. When data are first uploaded to a group's bucket, they are in the standard access class. This essentially means they are stored on higher performant storage and are available for immediate download. After three months of inactivity(1), data are automatically migrated to Glacier storage. This is less performant and data are no longer instantly downloadable. Users will need to request a restore before downloading their files. Restore requests can be submitted either in the user portal or using a command line tool available on our compute nodes (more details below).</p> <ol> <li>Activity in this context means the user has interacted with the file in some way, e.g. by downloading. </li> </ol> <p>After three months of inactivity in the Glacier access tier, data are automatically migrated to Deep Glacier. Deep Glacier utilizes very slow storage technology and requires a restore request to be submitted prior to downloading files, similar to Glacier. Deep Glacier restore requests typically take more time than Glacier files. </p> <pre><code>graph LR\n  A[Data uploaded&lt;br&gt;to AWS bucket] --&gt; B[Data stored in&lt;br&gt;standard access tier];\n  B --&gt;|Data downloaded| C[Counter resets];\n  C --&gt; B;\n  B --&gt;|Three months inactivity| D[Glacier access&lt;br&gt;storage tier];\n  D --&gt; |Restore request&lt;br&gt;submitted| C; \n  D --&gt; |Three months inactivity| E[Deep Glacier access&lt;br&gt;storage tier];\n  E --&gt; |Restore request&lt;br&gt;submitted| C;\n</code></pre>"},{"location":"storage_and_transfers/storage/tier2_storage/#pricing","title":"Pricing","text":""},{"location":"storage_and_transfers/storage/tier2_storage/#storage-costs","title":"Storage Costs","text":"<p>Part of this service is paid for by researchers and the rest is either subsidized or covered by UITS. The data stored in S3 will be billed monthly by AWS to the KFS account used when this is set up. Data in archival storage will be stored at no cost to the researcher. You will receive an email with detailed billing information when charges are made to your account.</p> Tier Cost to Researchers Duration Data Retrieval Standard $0 (First TB)$23/TB/Month<sup>1</sup> (data &gt; 1TB) Three months (if data not downloaded). After three months, untouched data automatically migrate to Glacier. Data Retrieval Glacier $0 Three months (if data not downloaded*). After three months, untouched data automatically migrated to Deep Glacier. A restore request must be submitted. Restores may take a few minutes to hours. Data may be transferred once restored. Deep Glacier $0 Unlimited (if data not downloaded) A restore request must be submitted. Restores may take a few hours to days. Data may be transferred once restored."},{"location":"storage_and_transfers/storage/tier2_storage/#data-transfer-costs","title":"Data Transfer Costs","text":"<p>Data movement costs are subsidized by UITS so researchers are not charged any AWS transfer fees.</p>"},{"location":"storage_and_transfers/storage/tier2_storage/#request-a-bucket","title":"Request a Bucket","text":"<p>Who can submit a request?</p> <p>A group's PI is responsible for submitting a storage request unless they have a group delegate. Delegates may perform Tier 2 storage operations on behalf of their PI by clicking Switch Users and entering their PI's NetID in the user portal. PIs may add delegates by entering their group member's NetID in the user portal under Add Delegate.</p> <p>First, log into the User Portal and navigate to the Storage tab at the top of the page. Select Submit Tier 2 Storage Request.</p> <p></p> <p>This will open a web form. Add your KFS number under KFS Number(1) and the email address for the Department's financial contact under Business contact email. There will also be two optional fields: Subaccount and Project. These are used for tagging/reporting purposes in KFS billing. You can safely leave these entries blank if you're not sure what they are. Once you have completed the form, click Send request. The KFS number can be obtained from the same financial contact.</p> <ol> <li>A KFS number is used for accounting purposes and used by your Department's finance specialist. If you do not know your KFS number, contact your department's financial office. </li> </ol> <p></p> <p>Submitting this form will open a ServiceNow ticket. Processing time may take up to a few days. Once your request has been completed, you will receive a confirmation email with a link to subscribe for account alerts (e.g., notifications for a sudden spike in usage). </p> <p></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#checking-your-usage","title":"Checking Your Usage","text":"<p>Tip</p> <p>AWS runs a batch update every night with the results being reported the following day. This means that if you have made any modifications to your allocation, your usage information will not be accurately reflected until the next batch update. </p> User PortalCLI <p>You may check your storage usage at any time in the User Portal. Navigate to the Storage tab, select View Tier 2 Storage, and click Query Usage.</p> <p></p> <p>To view the size and storage classes of individuall objects, you will need to use the CLI interface.</p> <p>A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool (see the next section). This can be accessed using: <pre><code>(elgato) [netid@junonia ~]$ interactive\n[netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer\n</code></pre> For information on usage: <pre><code>tier2-viewer --help\n</code></pre> To play a tutorial in your terminal, use: <pre><code>tier2-viewer --example\n</code></pre></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#generate-access-keys","title":"Generate Access Keys","text":"<p>Access keys will allow you to connect your AWS bucket using tools such as Globus which will enable you to make transfers directly between HPC and your Tier 2 storage allocation. Access keys should be treated as passwords and should only be shared with trusted group members and collaborators. </p> <p>To generate an access key, log into the User Portal, navigate to the Storage tab, and select Regenerate IAM Access Key.</p> <p></p> <p>This will generate a KeyID and Secret Access Key used to establish the connection. Save these keys somewhere safe since once the window is closed, they cannot be retrieved. If you forget your keys, you can regenerate a new pair.</p> <p></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#transferring-files","title":"Transferring Files","text":"<p>The easiest way to transfer files from AWS to HPC is using Globus. We have instructions in our Transferring Files page on how to set up an endpoint to access your AWS bucket as well as how to initiate file transfers.</p> <p>Some other file transfer programs include rclone and Cyberduck.</p>"},{"location":"storage_and_transfers/storage/tier2_storage/#restoring-archived-data","title":"Restoring Archived Data","text":"<p>Data that are not touched for at least 90 and 180 days are automatically retiered to archival storage (Glacier and Deep Glacier, respectively). Files stored in an archival state cannot be transferred out of AWS until they are restored. Restore requests can be submitted either via the User Portal or using a command line utility available on our compute nodes. </p> <p>The time it takes for an object to be retrieved is dependent on its storage class. Objects in Glacier may take a few hours while objects in Deep Glacier may take up to a day or two. Once an object has been restored, it will move back up to the frequent access tier and can be downloaded using any transfer method you prefer.</p> User PortalCLI <p>File count</p> <p>Warning: If you are restoring a directory, the portal will only support restore requests for directories containing up to 50 files. If you need to restore a large directory, use the CLI.</p> <p>In the User Portal, navigate to the Storage tab by clicking Restore Archived Tier 2 Storage Object:</p> <p></p> <p>This will open a box where you can enter the path to a file or directory in your bucket. Enter the path to the object you would like to restore:</p> <p></p> <p>Once you select an object, click Send Request to initiate the retrieval</p> <p></p> <p>A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool (see the next section). This can be accessed using: <pre><code>(elgato) [netid@junonia ~]$ interactive\n[netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer\n</code></pre> For information on usage: <pre><code>tier2-viewer --help\n</code></pre> To play a tutorial in your terminal, use: <pre><code>tier2-viewer --example\n</code></pre> The <code>--restore</code> flag can be used to either restore a file or a full directory. </p> <ol> <li> <p>More up-to-date pricing information can be found on AWS's website.\u00a0\u21a9</p> </li> </ol>"},{"location":"storage_and_transfers/transfers/cyberduck/","title":"Cyberduck","text":"<p>Cyberduck is a graphical file transfer application that can be used to connect to and transfer files between your local computer and various remote servers and cloud storage services. To get started, you can download the application onto your local workstation from their website here: https://cyberduck.io/</p>"},{"location":"storage_and_transfers/transfers/cyberduck/#initiating-transfers","title":"Initiating Transfers","text":"<p>Once you have Cyberduck installed, open the software and select New Browser from the toolbar</p> <p></p> <p>In the window that opens, select Open Connection</p> <p></p> <p>This will give you a number of options to choose from.</p>"},{"location":"storage_and_transfers/transfers/cyberduck/#some-connection-options","title":"Some Connection Options","text":"HPCGoogle Drive <p>To connect to HPC, select SFTP (SSH File Transfer Protocol) from the top dropdown, enter <code>filexfer.hpc.arizona.edu</code> under Server, and your university credentials under Username and Password.</p> <p></p> <p>Once you click Connect, you will be prompted to duo-authenticate</p> <p></p> <p>If your connection is successful, you will see a window open with the contents of your home directory.</p> <p></p> <p>To connect to Google Drive, select the Google Drive option from the dropdown tab and select Connect</p> <p></p> <p>This will open a browser where you will be prompted to log into your Google Drive account.</p> <p></p> <p>Once you have successfully logged in, grant access to Cyberduck where prompted. If this process is successful, you should see a connection window where you can navigate through the contents of your Google Drive.</p> <p></p> <p>To initiate transfers, simply drag and drop your files between the Cyberduck window and your local computer. If you have multiple connections open, you can also initiate transfers between two remotes by dragging and dropping files between two connection windows.</p>"},{"location":"storage_and_transfers/transfers/globus/","title":"Globus","text":""},{"location":"storage_and_transfers/transfers/globus/#overview","title":"Overview","text":"<p>Globus provides a graphical web application that employs GridFTP to transfer data between pre-configured endpoints. GridFTP is an extension of the standard File transfer Protocol (FTP) for high-speed, reliable, and secure data transfer. Because GridFTP provides a more reliable and high performance file transfer (compared to protocols such as SCP or rsync), it enables the transmission of very large files. GridFTP also addresses the problem of incompatibility between storage and access systems. (You can read more about the advantages of GridFTP here).</p> <p>A list of endpoint names managed by HPC are shown below for reference. For more information on usage, see HPC-Managed Globus Endpoints below. </p> Endpoint Name Data Storage UA HPC Filesystems HPC's main storage array (access to home, xdisk, and groups) UA Rental Storage Filesystem HPC rental storage Tier 2 AWS Storage HPC-managed AWS S3 buckets UA HPC HIPAA Filesystems Soteria"},{"location":"storage_and_transfers/transfers/globus/#accessing-globus","title":"Accessing Globus","text":"<p>Globus can be used as a web application. To access it, navigate to https://www.globus.org/. Next, click Log In in the upper right-hand corner</p> <p></p> <p>On the next page, enter The University of Arizona in the search field and click the result.</p> <p></p> <p>This will take you through the standard university WebAuth login process. Once you successfully log in, you will be placed in a File Manager window. The various steps for setting up endpoints, initiating transfers, and viewing a transfer's progress can be found in the sections below.</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#globus-connect-personal","title":"Globus Connect Personal","text":"<p>To transfer files to/from your personal computer with Globus, you'll need to have a local endpoint set up. This can be achieved using Globus Connect Personal. Official documentation on how to install the relevant software and configure a local endpoint can be found in Globus' offical how-to documentation. An overview is shown for Mac, Linux, and Windows below.</p> <p>To start, regardless of operating system, go to https://www.globus.org/, log in, navigate to the Collections tab, and select Get Globus Connect Personal</p> <p></p> <p>From there, choose your operating system to proceed with the download and setup process</p> <p></p> MacWindowsLinux <p>Once you've downloaded the .dmg file, open it and drag/drop the Globus icon into your Applications directory</p> <p></p> <p>Next, open the application. This will prompt you to log in via the university WebAuth process in a browser session. Once you've logged in, enter an identifying label for your local machine and grant Globus access</p> <p></p> <p>This will bring you back to your local Globus Connect Personal installation. You will fill out your local display name for your endpoint and click Save.</p> <p></p> <p>Once your installation is complete, open the .exe file to initiate the install. Click Yes to allow Globus Connect Personal to make changes to your device</p> <p></p> <p>Next, select the install location, and click Install. </p> <p></p> <p>Once the install is complete, make sure the Run Globus Connect Personal box is checked and click Finish.</p> <p></p> <p>Globus Connect Personal will then open and begin the configuration process. Click Log In to continue.</p> <p></p> <p>This will open a web browser where you will go through the typical UArizona WebAuth login process. Once you're logged in, give your local endpoint a descriptive name and click Allow.</p> <p></p> <p>This will bring you back to your local install. Enter a descriptive local name for your endpoint and click Save.</p> <p></p> <p>Once your installation is complete, open a terminal, navigate to your Downloads directory, and unpack the tar archive. Next, change into the unpacked directory and execute the <code>globusconnectpersonal</code> binary:</p> <pre><code>[user@ubuntu ~]$ cd Downloads\n[user@ubuntu Downloads]$ tar xzvf globusconnectpersonal-latest.tgz\n[user@ubuntu Downloads]$ cd globusconnectpersonal-3.2.0\n[user@ubuntu globusconnectpersonal-3.2.0]$ ./globusconnectpersonal\n</code></pre> <p>This will bring up a graphical application. Click Log In to continue</p> <p></p> <p>This will open a web browser where you will need to go through the typical university WebAuth process. Once you're logged in, give your endpoint a name and click Allow.</p> <p></p> <p>This will bring you back to your local installation. Give your machine a descriptive name, then select Save.  </p> <p>Your setup should now be complete and your endpoint will now be usable to initiate transfers. You can find your endpoint by navigating to the Collections tab and checking the box Administered by you. For example:</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#hpc-managed-globus-endpoints","title":"HPC Managed Globus Endpoints","text":"<p>HPC managed endpoints allow you to connect to HPC-affiliated storage to initiate transfers. Transfers can be made between any two endpoints; for example, allowing you to make transfers between your own personal computer and HPC storage, between HPC storage (<code>/home</code>, <code>/groups</code>, or <code>/xdisk</code>) and a rental option (such as Tier 2 AWS buckets or HPC rental storage), or between HPC and another institution's endpoint.</p> <p>Below are a list of HPC managed endpoints and how to configure them:</p> HPC StorageRental StorageTier 2 AWS Storage <p>The endpoint for HPC can be found by searching UA HPC Filesystems under the Collections tab.</p> <p></p> <p>Click the result, then click Open in File Manager to access your HPC files.</p> <p></p> <p>The default location is your /home on HPC. You can navigate through by double-clicking directories, or by entering a full path in the Path search bar and hitting enter. This method can be used to access any /xdisk or /groups directories you have access to.</p> <p></p> <p>The endpoint for rental storage (found on the filexfer nodes under <code>/rental</code>) can be found by searching UA Rental Storage Filesystem under the Collections tab.</p> <p></p> <p>This will open details on the endpoint. Click Open in File Manager to view the contents.</p> <p></p> <p>The root for this endpoint is /rental. Faculty members who have rented storage will have a directory with their NetID in this space. Find the one relevant to you and double-click to access.</p> <p></p> <p>To access a Tier 2 AWS S3 bucket, in the Collections tab, enter UA AWS S3 in the search bar. In the results, you should see the name UA AWS S3 show up with the description Subscribed Mapped Collection. Click the endpoint's name to proceed</p> <p></p> <p>Next, select the Credentials tab and select Add Credential. If you are prompted for Authentication/Consent, click Continue</p> <p></p> <p>If requested, authenticate by selecting your Arizona email address, then Allow. You will then be returned to the Credentials tab. From there, link to your AWS S3 Bucket by entering your public and private keys in the provided fields and click Continue.</p> <p></p> <p>Once you've added your keys, navigate back to the UA AWS S3 collection, go to the Collections tab, and click Add a Guest Collection on the right</p> <p></p> <p>Under Create New Guest Collection, click Browse next to the Directory field to find your group's AWS bucket. You will find it under /ua-rt-t2-faculty_netid/ where faculty_netid is the NetID of the faculty member who requested the bucket. Under Display Name, enter a descriptive name that you can use to identify your bucket. Once you've completed the process, click Create Collection at the bottom of the page.</p> <p>Tip</p> <p>If you encounter Authentication/Consent Required after clicking Browse, click Continue, select your university credentials, and click Allow. That should bring you back to the Browse window.</p> <p></p> <p>To find and use your new collection, navigate to the Collections tab and select the display name you assigned to your bucket. That will open your collection in the File Manager window allowing you to view the contents and initiate transfers.</p> <p></p> <p>If you click the display name, this will open the bucket in the Globus file manager window allowing you to see the contents</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#making-transfers","title":"Making Transfers","text":"<p>Transfers can be made between any two endpoints of your choosing using the File Manager window in the Globus web application. In this example, we'll make a transfer between a Globus Connect Personal endpoint and the primary HPC storage array.</p> <p>To start, go to the File Manager tab in the Globus web application. Make sure you have the dual-panel mode enabled (upper right-hand corner shown with the red arrow below) to allow you to open two endpoints. Start with opening your first endpoint by clicking the Search bar on the left-hand side. </p> <p></p> <p>This will open a window where you can search for your first endpoint. In this example, we'll use UA HPC Filesystems. Click the result.</p> <p></p> <p>Now, on the left-hand side you should see the contents of your home directory on HPC. You can navigate through the various directories by double-clicking the folder icons, or can enter a full path in the Path search bar. To open a second connection, click the Search bar on the right-hand side. </p> <p></p> <p>You can search for your next endpoint in the same way as we searched for UA HPC Filesystems. You can also find recently used endpoints and your collections (e.g., a Google Drive collection or personal endpoint) under the Recent and Your Collections tabs. In this example, we'll go to Your Collections, find a personal endpoint, and click the result. </p> <p></p> <p>Now you should be back in the File Manager window with two endpoints open. A transfer can be made from one endpoint to another by selecting the item(s) you want to transfer, then clicking the Start button.</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#monitoring-your-transfers","title":"Monitoring Your Transfers","text":"<p>When you initiate a transfer following the instructions in the Making Transfers section above, a green box will pop up confirming the request. </p> <p></p> <p>You can get additional information the Activity panel on the left-hand side of the page. This will show you active and past transfers as well as their status. You can view additional details about your transfers by clicking the &gt; shown on the right-hand side next to the target task. You can also cancel a transfer by clicking the \u00d7 on the right. </p> <p></p> <p>Once your transfer has completed, you should receive an email with its status.</p>"},{"location":"storage_and_transfers/transfers/irods/","title":"iRods","text":"<p>CyVerse Support</p> <p>If you are looking for information on how to connect to CyVerse's data store, see their iRODS documentation for a guide.</p> <p>Only use on DTNs</p> <p>iRODS should only be used on the filexfer/data transfer nodes (DTNs), which are equipped to handle large data transfers. Use of iRODS on the compute nodes may result in system issues. To access a DTN, run <code>ssh &lt;your_netid&gt;@filexfer.hpc.arizona.edu</code> from a login node.</p> <p>iRODS 4 is installed as a standard package on the operating systems of the data transfer nodes. You will need to <code>iinit</code> the first time you use the software (see below). </p>"},{"location":"storage_and_transfers/transfers/irods/#initializing-irods","title":"Initializing iRODS","text":"<p>Running <code>iinit</code> for any system using iRODS 4.x, unlike its iRODS3 counterpart, does not help you set up the environment. Instead, you need to run <code>create_irods_env</code> with suitable options for the iRODS host, zone, username, etc manually.</p> For this key Enter this <code>-h</code> <code>&lt;hostname of iRODS server&gt;</code> <code>-p</code> <code>&lt;port number of iRODS server&gt;</code> (1247 is default) <code>-z</code> <code>&lt;Zone name of iRODS zone&gt;</code> <code>-u</code> <code>&lt;user name on the iRODS server&gt;</code> (may not match your netid) <code>-a</code> <code>&lt;authentication method for the iRODS server&gt;</code> (PAM, native,...) <p>For example <pre><code>$ create_irods_env -a native -h someserver.somewhere.net -z MYZONE\n</code></pre></p> <p>will suffice to create an appropriate <code>~/.irods/irods_environment.json</code> file to allow you to run <code>iinit</code>; we took the default <code>-p 1247</code>, <code>-u &lt;your NetId&gt;</code> in the above example by omitting <code>-p</code> and <code>-u</code>.  You only need to do this step ONE time; subsequent times you will just run iinit and it will asked for your password. Note <code>create_irods_env</code> will NOT overwrite or alter an existing <code>~/.irods/irods_environment.json</code> file.</p> <p>Once the  <code>~/.irods/irods_environment.json</code> file is created properly, you should be able to sign in to the iRods server your selected using <code>iinit</code>, viz:</p> <pre><code>$ iinit\nEnter your current ... password:    # enter your iRODS server password here\n</code></pre> <p>At this point you can use other iRods commands such as <code>icp</code> to move files.</p>"},{"location":"storage_and_transfers/transfers/irods/#examples","title":"Examples","text":"<p>Tip</p> <p>In the following examples: * <code>my-files-to-transfer/</code> is the example name of the directory or folder for bulk transfers. * <code>my-file-to-transfer.txt</code> is the example name for single file transfers. * Any filename may be used for the <code>checkpoint-file</code>.</p> <p>Bulk Files Transfer Example <pre><code>iput -P -b -r -T --retries 3 -X checkpoint-file my-files-to-transfer/\n</code></pre></p> <p>Single Large File Transfer Example <pre><code>iput -P -T --retries 3 --lfrestart checkpoint-lf-file my-file-to-transfer.txt\n</code></pre></p>"},{"location":"storage_and_transfers/transfers/open_on_demand/","title":"Open OnDemand","text":"<p>Tip</p> <p>Open OnDemand file transfers are limited to 64 MB. For larger files, see our data transfer overview for more options.</p> <p>A popular method of transfering files to and from the HPC is the Open OnDemand interface, which is accessed through the browser at ood.hpc.arizona.edu. </p> <p>To access the file browser in Open OnDemand, choose your desired share from the \"Files\" dropdown menu.</p> <p></p> <p>From there, you should see a list of folders and files. Click on folders to open them up, or use the file path navigator to ascend the tree. </p> <p></p> <p>Additional actions can be taken using the button ribbon on the top right. </p> <p></p> <p>To change the root directory, use the links on the left hand side of the screen. </p> <p></p> <p>When navigating to your group's share within <code>/groups</code> or <code>/xdisk</code>, use the \"Filter\" box to quickly find your folder from the list.</p> <p></p>"},{"location":"storage_and_transfers/transfers/overview/","title":"Overview","text":"<p>Log In Before Transferring</p> <p>To make transfers to/from HPC, you will need to have logged into your account at least once. If you have not, you may encounter \"directory does not exist\" errors. This is because your home directory is not created until you log in for the first time. See here about System Access</p> <p>Files are transferred to shared data storage and not to the bastion node, login nodes, or compute nodes. Because the storage is shared, your files are accessible on all clusters; Puma, Ocelote and Elgato. When you look at the diagram above, you can intuitively see the efficiency of transferring data without additional hops.  Keeping mind that the data pipes are wider also, allowing for faster data transmission.</p> <p></p>"},{"location":"storage_and_transfers/transfers/overview/#data-transfers-by-size","title":"Data Transfers By Size","text":"<ol> <li>Small Transfers: For small data transfers the web portal offers the most intuitive method.</li> <li>Transfers &lt;100GB: we recommend sftp, scp or rsync using <code>filexfer.hpc.arizona.edu</code>.  </li> <li>Transfers (&gt;100GB), transfers outside the university, and large transfers within HPC: we recommend using Globus (GridFTP).</li> </ol>"},{"location":"storage_and_transfers/transfers/overview/#transfer-software-summary","title":"Transfer Software Summary","text":"Software CLI Interface? GUI Interface? Access to Cloud Services? Notes Google Drive AWS Box Dropbox Globus \u2705 \u2705 \u2705 \u2705 \u274c \u274c SFTP \u2705 \u2705 \u274c \u274c \u274c \u274c SCP \u2705 \u2705 \u274c \u274c \u274c \u274c On Windows, WinSCP is available as a GUI interface rsync \u2705 \u2705 \u274c \u274c \u274c \u274c Grsync is a GUI interface for rsync for multiple platforms. rclone \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 rclone has recently announced they have an experimental GUI. Cyberduck \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 iRODS \u2705 \u2705 \u274c \u274c \u274c \u274c"},{"location":"storage_and_transfers/transfers/overview/#file-transfers-and-ssh-keys","title":"File Transfers and SSH Keys","text":"<p>Several of the file transfer methods listed below use authentication based on the SSH protocol, including scp, sftp and rsync. Therefore, adding your SSH Key to the <code>filexfer.hpc.arizona.edu</code> node can allow one to avoid entering passwords when using those methods. See the documentation for adding SSH Keys.</p>"},{"location":"storage_and_transfers/transfers/rclone/","title":"Rclone","text":"<p>Browser Availability</p> <p>If you encounter issues configuring rclone on HPC due to an internet browser not being available, try using a terminal in an interactive desktop session in Open OnDemand. This will allow you to grant application access graphically. </p> <p>Reference</p> <p>For information on rclone commands, see: https://rclone.org/commands/</p> <p>Rclone is a CLI tool installed on <code>filexfer.hpc.arizona.edu</code> that can be used to transfer files between HPC and Cloud-based storage sites. To use rclone, you will need to start by configuring it. The walkthrough provided by rclone is fairly straightforward and has a large number of options to choose from. A configuration example for Google Drive is shown below. </p>"},{"location":"storage_and_transfers/transfers/rclone/#example-google-drive-configuration","title":"Example Google Drive Configuration","text":"<pre><code>[netid@sdmz-dtn-4 ~]$ rclone config\nName                 Type\n====                 ====\n\n  e) Edit existing remote\n  n) New remote\n  d) Delete remote\n  r) Rename remote\n  c) Copy remote\n  s) Set configuration password\n  q) Quit config\n  e/n/d/r/c/s/q&gt; n\n  name&gt; GoogleDrive\n  Type of storage to configure.\n  Enter a string value. Press Enter for the default (\"\").\n  Choose a number from below, or type in your own value\n  ...\n  12 / Google Drive\n    \\ \"drive\"\n  ...\n  Storage&gt; 12\n  Google Application Client Id\n  client_id&gt;\n  client_secret&gt;\n  scope&gt; 1\n  root_folder_id&gt;\n  service_account_file&gt;\n  Edit advanced config? (y/n)\n  y) Yes\n  n) No\n  y/n&gt; n\n</code></pre> <p>The next prompt will ask if you would like to use auto config, select N here or the configuration will not be successful. You will be given a URL. Copy and paste this into your web browser and follow the prompts to allow rclone to have access to your UArizona Google Drive account. When you are done, it will give you a verification code. Copy and paste this back into the terminal to proceed.</p> <pre><code>Remote config\nUse auto config?\n  * Say Y if not sure\n  * Say N if you are working on a remote or headless machine\ny) Yes\nn) No\ny/n&gt; N\nIf your browser doesn't open automatically go to the following link: &lt;long URL&gt;\nLog in and authorize rclone for access\nEnter verification code&gt; &lt;your verification code here&gt;\nConfigure this as a team drive?\ny) Yes\nn) No\ny/n&gt; n\n--------------------\n[GoogleDrive]\ntype = drive\nscope = drive\ntoken = {\"access_token\": &lt;lots of token data&gt;}\n--------------------\ny) Yes this is OK\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; y\nCurrent remotes:\n\nName                 Type\n====                 ====\nGoogleDrive          drive\n\nYour Google Drive connection is now active and you can use rclone to make transfers. \n</code></pre>"},{"location":"storage_and_transfers/transfers/rsync/","title":"rsync","text":"<p><code>rsync</code> is a fast and extraordinarily versatile file copying tool. It synchronizes files and directories between two different locations (or servers). <code>rsync</code> copies only the differences of files that have actually changed. An important feature of <code>rsync</code> not found in most similar programs/protocols is that the mirroring takes place with only one transmission in each direction. <code>rsync</code> can copy or display directory contents and copy files, optionally using compression and recursion. <code>rsync</code> is similar to scp in that both a source and a destination must be specified, one of which may be remote. </p>"},{"location":"storage_and_transfers/transfers/rsync/#subdirectory-behavior","title":"Subdirectory Behavior","text":"<p>How can we recursively transfers all files from the directory <code>src/directory-name</code> on a local machine into <code>/data/tmp/</code> on a remote machine?</p> <p>A trailing slash on the source changes the behavior of <code>rsync</code>. The inclusion of the trailing slash avoids creating an additional directory level at the destination. You can think of a trailing <code>/</code> on a source as meaning \u201ccopy the contents of this directory\u201d as opposed to \u201ccopy the directory by name\u201d, but in both cases the attributes of the containing directory are transferred to the containing directory on the destination. </p> <p>Trailing Slash</p> <p>The below command will copy all of the contents of <code>directory-name</code> into <code>tmp</code>, excluding the parent folder.</p> <p><pre><code>rsync -ravz  computer-name:src/directory-name/  user@remote.host:/data/tmp --log-file=hpc-user-rsync.log \n</code></pre> The folder <code>remote.host:/data/tmp</code> will then contain anything it held previously in addition to the subfolders of <code>directory-name</code>.</p> <p>No Trailing Slash</p> <p>On the other hand, the below command will copy <code>directory-name</code> as a parent folder into <code>tmp</code> resulting in a new directory <code>tmp/directory-name</code>. The contents of <code>directory-name</code> will appear exactly as they did on the local machine. </p> <pre><code>rsync -ravz  computer-name:src/directory-name  user@remote.host:/data/tmp --log-file=hpc-user-rsync.log \n</code></pre> <p>Note that including <code>&lt;computer-name&gt;</code> is optional when referring to the local machine. Log files are optional but recommended.</p>"},{"location":"storage_and_transfers/transfers/rsync/#additional-options","title":"Additional Options","text":"Flag Meaning <code>-r</code> Recursive mode; loop through contents of all subfolders <code>-a</code> Archive mode; will preserve time stamps and other metadata <code>-v</code> Increase verbosity <code>-z</code> Compress file data during the transfer <code>--log-file=FILE</code> Log everything done in specified <code>FILE</code>"},{"location":"storage_and_transfers/transfers/scp/","title":"SCP","text":"<p>Secure Copy, or SCP, uses Secure Shell (SSH) for data transfer and utilizes the same mechanisms for authentication, thereby ensuring the authenticity and confidentiality of the data in transit.</p>"},{"location":"storage_and_transfers/transfers/scp/#maclinux","title":"Mac/Linux","text":"<p>You will need to use an SSH v2 compliant terminal to move files to/from HPC. For more information on using SCP, use <code>man scp</code>.</p> <p>Copying to HPC</p> <p>In your terminal, navigate to the desired working directory on your local machine (laptop or desktop usually). To move a file or directory to a designated subdirectory in your account on HPC:</p> <pre><code>scp -rp filenameordirectory NetId@filexfer.hpc.arizona.edu:subdirectory\n</code></pre> <p>Copying from HPC</p> <p>In your terminal, navigate to the desired working directory on your local machine. The copy a remote file from HPC to your current directory: <pre><code>scp -rp NetId@filexfer.hpc.arizona.edu:filenameordirectory .\n</code></pre></p> <p>Shorthand</p> <p>Note that the trailing period above refers to the current directory. See our Linux Cheat Sheet for more tips like this. </p> <p>Wildcards can be used for multiple file transfers (e.g. all files with .dat extension). Note the backslash <code>\\</code> preceding <code>*</code></p> <pre><code>scp NetId@filexfer.hpc.arizona.edu:subdirectory/\\*.dat .\n</code></pre>"},{"location":"storage_and_transfers/transfers/scp/#windows","title":"Windows","text":"<p>Windows users can use software like WinSCP to make SCP transfers. To use WinSCP, first download/install the software from: https://winscp.net/eng/download.php</p> <p>To connect, enter <code>filexfer.hpc.arizona.edu</code> in the Host Name field, enter your NetID under User name, and enter your password. Accept by clicking Login. You'll be prompted to Duo Authenticate:</p> <p></p>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/","title":"SFTP/FTP/LFTP","text":"<p>Tip</p> <p>To use SFTP/LFTP, you will need to be connected to our file transfer nodes, hostname: <code>filexfer.hpc.arizona.edu</code>.</p>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/#sftp","title":"SFTP","text":"<p>SFTP (Secure File Transfer Protocol) ensures data encryption during transmission over the network. It offers features such as resuming interrupted transfers, directory listing, and remote file management.</p> <p>To perform file transfers using SFTP, you'll need an SSH v2 compliant terminal. To connect to HPC's data transfer nodes, run:</p> <pre><code>sftp your_netid@filexfer.hpc.arizona.edu\n</code></pre> <p>You will then be able to move files between your machine and HPC using <code>get</code> and <code>put</code> commands. For example:</p> <pre><code>sftp&gt; get /path/to/remote/file /path/to/local/directory ### Retrieves file from HPC. Omitting paths will default to working directories.\nsftp&gt; put /path/to/local/file /path/to/remote/directory ### Uploads a file from your local computer to HPC. Omitting paths will default to working directories.\nsftp&gt; help ### prints detailed sftp usage\n</code></pre>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/#ftplftp","title":"FTP/LFTP","text":"<p>Warning</p> <p>Due to security risks, it is not possible to FTP to the file transfer node from a remote machine, however, you may FTP from the file transfer node to a remote machine.</p> <p>HPC uses the FTP client LFTP to transfer files between the file transfer node and remote machines. This can be done using get and put commands. To use lftp, you must first connect to our file transfer node using an SSH v2 compliant terminal:</p> <pre><code>ssh your_netid@filexfer.hpc.arizona.edu\n</code></pre> <p>Once connected, you may connect to the external host using the command <code>lftp</code>. For example:</p> <pre><code>lftp ftp.hostname.gov\n</code></pre> <p>You will then be able to move files between HPC and the remote host using <code>get</code> and <code>put</code> commands. For example:</p> <pre><code>&gt; get /path/to/remote/file /path/to/local/directory ### retrieves file from remote host\n&gt; put /path/to/local/file /path/to/remote/directory ### Uploads file from HPC to remote host\n</code></pre>"},{"location":"support_and_training/cheat_sheet/","title":"Linux Cheat Sheet","text":""},{"location":"support_and_training/cheat_sheet/#why-learn-bash","title":"Why Learn Bash","text":"<p>Bash is a powerful command line shell that allows you to interact with our systems efficiently. It enables scripting and automation, job submission, file and directory management, remote access, resource monitoring, environment customization, parallel computing, data preprocessing, version control, error handling, software dependency management, custom workflows, and offers a valuable skill set applicable beyond HPC tasks. In other words, it allows you to do a lot!</p> <p>In the cheat sheet below, we offer some basics as a reference. We also highly suggest you explore some more comprehensive guides such as the following: Introduction to Linux on HPC</p>"},{"location":"support_and_training/cheat_sheet/#shell-basics","title":"Shell Basics","text":""},{"location":"support_and_training/cheat_sheet/#shortcuts","title":"Shortcuts","text":"<p>Shortcuts can be used as standins for system locations, avoiding the need for full paths. A few examples:</p> Shortcut Description <code>~</code> Your home directory <code>.</code> Your working directory <code>..</code> One directory above your working directory"},{"location":"support_and_training/cheat_sheet/#commands","title":"Commands","text":"<code>ls</code> List the contents of a directory <code>ls -la</code> List the contents of a directory including hidden files <code>cd</code> Change your working directory <code>pwd</code> Show the path to your working directory <code>mkdir</code> Create a new directory <code>rm</code> Delete a file. Be careful, files deleted this way can't be retrieved <code>rm -r</code> Delete a directory. Be careful, directories deleted this way can't be retrieved"},{"location":"support_and_training/cheat_sheet/#hidden-files-and-directories","title":"Hidden Files and Directories","text":"<p>Hidden files and directories start with a dot <code>.</code> and won't show up when you do a simple <code>ls</code>. Some of these files are used to configure your environment when you first log in. Be careful when removing or modifying these files since doing so can cause unintended consequences.</p> <p>Tip</p> <p>The <code>~</code> in the filenames below indicates your home directory. For example, <code>~/.bashrc</code> is specifying a file in your home called <code>.bashrc</code>.</p> File/Directory What It Does Warnings <code>~/.bash_profile</code> This file sets your working environment when you first log into HPC. This file sources your <code>~/.bashrc</code> (see below). See the list in the danger block below. <code>~/.bashrc</code> This file sets your working environment when you first log into HPC. See the list in the danger block below. <code>~/.local</code> This is a hidden directory in your home where pip-installed python packages, jupyter kernels, RStudio session information, etc. goes. If you pip-install python packages when a virtual environment is not active, they will be installed in this directory. These will then be automatically loaded for all future python sessions (version-specific), including in Singularity images. This may cause versioning issues. We recommend always using virtual environments. <code>~/.apptainer</code> A hidden directory in your home where Apptainer images and cache files are stored by default. This directory can grow large quickly and fill up your home. You can modify your <code>~/.bashrc</code> to set a different cache directory location that has a larger quota. <p>Danger</p> <p>When working with hidden configuration files in your account (<code>.bashrc</code> and <code>.bash_profile</code>), be careful of:</p> <ol> <li> <p>Aliasing important Linux commands</p> <p>For example, the character <code>.</code> is a shortcut for <code>source</code>. If you do something like add <code>alias .=&lt;command&gt;</code> to your bashrc, you will lose basic functionality in the terminal, e.g., access to modules, virtual environments, etc. </p> </li> <li> <p>Recursively sourcing configuration files</p> <p>If you add <code>source ~/.bashrc</code> or <code>source ~/.bash_profile</code> to your bashrc, then you will enter an infinite sourcing loop. This means when you try to log in, your terminal will freeze, then your access will be denied. </p> </li> <li> <p>Using <code>echo</code></p> <p>If you use CLI tools for data transfer, e.g. <code>scp</code> or <code>sftp</code>, they may require a \"silent\" terminal. If you're trying to initiate transfers and are getting the error \"Received message too long\", check your bashrc to make sure you aren't printing anything to the terminal. </p> </li> <li> <p>Removing your files</p> <p>If you delete either your <code>~/.bashrc</code> or <code>~/.bash_profile</code>, you will lose access to <code>module</code> commands. </p> </li> </ol>"},{"location":"support_and_training/cheat_sheet/#environment-variables","title":"Environment Variables","text":"<p>Bash variables control how your environment is configured. For example: what executables, libraries, header files, etc. are findable by default; information about your Slurm job; MPI settings; GPU configuration; etc. To see all the environment variables that are defined for your session, try running the command <code>env</code>.</p> <p>An example of an important environment variable is <code>PATH</code>. This is set to a <code>:</code>-delimited list of paths that it uses to search for executables. You can see what it's set to by running <code>echo $PATH</code>. Any time you run a command, for example <code>ls</code>, that list is searched in order. To add new executables to your environment, you can add a path to <code>PATH</code>. For example:</p> <pre><code>export PATH=/path/to/new/directory:$PATH\n</code></pre> <p>This is what modules do: they update your environment variables to put new software and libraries into your environment for use. Some examples of some important variables are listed below:</p> Variable Function <code>PATH</code> A colon-delimited list of paths used to search for executables. When you run a command, such as <code>ls</code>, the directories listed in <code>PATH</code> are searched in order. <code>LD_LIBRARY_PATH</code> A colon-delimited list of directories in which the linker should search for shared libraries at runtime. Useful for specifying additional library directories for dynamically linked programs. For example: <code>export LD_LIBRARY_PATH=/path/to/library:$LD_LIBRARY_PATH</code> <code>CFLAGS</code>, <code>CXXFLAGS</code>, <code>LDFLAGS</code> Environment variables used to specify compiler flags for compiling C, C++, and linking respectively. These variables are often used to customize the build process of software. For example: <code>export CFLAGS=\"-O2 -march=native $CFLAGS\"</code> <code>CC</code>, <code>CXX</code>, <code>FC</code> Environment variables specifying the C, C++, and Fortran compilers, respectively. These variables allow users to specify which compiler should be used during the build process of software. For example: <code>export CC=gcc</code> <code>OMP_NUM_THREADS</code> Specifies the number of OpenMP threads to use for parallelized programs. This variable controls the number of threads used in parallel regions of OpenMP-enabled programs. For example: <code>export OMP_NUM_THREADS=4</code> <p>Slurm also sets its own environment variables.</p>"},{"location":"support_and_training/cheat_sheet/#linux-file-permissions","title":"Linux File Permissions","text":"<p>Linux file permissions control who can access files and directories and what they are able to do with them.</p> <p>HPC users may find it useful or necessary to share files and/or directories with collaborators. To do this effectively, it may become necessary to familiarize yourself with the linux file permission system in order to give collaborators access to your files. </p>"},{"location":"support_and_training/cheat_sheet/#types-of-permissions","title":"Types of Permissions","text":"<p>All files and directories on a Linux system have permissions defined for them. There are three types:</p> Permission Symbol Directory meaning File meaning Read <code>r</code> Users can view the contents of the directory Users can read the contents of a file Write <code>w</code> Users can modify the contents of a directory Users can write to a file Execute <code>x</code> Users can <code>cd</code> into a directory Users can directly execute a file <p>When a permission is missing, you will see a <code>-</code></p>"},{"location":"support_and_training/cheat_sheet/#viewing-permissions","title":"Viewing Permissions","text":"<p>To see a file or directory's file permissions, run the command <code>ls -l</code>. This will show you output that looks like the following:</p> <p><code>-</code><code>rwx</code><code>r-x</code><code>---</code><code> 1 </code><code>username</code><code> </code><code>groupname</code><code> 0 Feb 27 09:54 file</code></p> String Access Group Meaning <code>-</code> Tells you whether this item is a regular file (<code>-</code>), directory (<code>d</code>), or link <code>(l)</code> In this example, we're looking at a regular file <code>rwx</code> This describes the permissions that are set at the user level. In this case, they apply to the user with the username <code>username</code>. Your username is your NetID In this example, the file's owner can read, modify, and execute this file. <code>r-x</code> This describes the permissions that are set at the group level. In this case, they apply to anyone who is a member of the group <code>groupname</code>. To see your groups you're a member of, run the command <code>groups</code>. In this example, group members are allowed to see and execute the contents of this file, but they cannot modify it. <code>---</code> This describes the permissions that are set for anyone else on the system. In this example, the rest of the HPC community can't see or edit the contents of this file and can't execute it <p>Changing Permissions</p> <p>To change the permissions of a file or directory, you can use the command <code>chmod</code>. This command accepts two types of input: symbolic and octal.</p>"},{"location":"support_and_training/cheat_sheet/#symbolic","title":"Symbolic","text":"<p>Symbolic notation is the most intuitive to understand. It involves a comma-delimited list of permissions symbols, each representing a specific permission type (read, write, execute) for a particular user or group. Here's a breakdown of the symbols used:</p> <ul> <li><code>u</code> refers to the user who owns the file.</li> <li><code>g</code> refers to the group that the file belongs to.</li> <li><code>o</code> refers to other users who are not the owner or in the group.</li> <li><code>a</code> refers to all users (<code>u</code>, <code>g</code>, and <code>o</code>).</li> </ul> <p>For each of these, you can use <code>+</code> to add a permission, <code>-</code> to remove a permission, or <code>=</code> to set the permissions explicitly. The permission symbols are:</p> <ul> <li><code>r</code> for read permission.</li> <li><code>w</code> for write permission.</li> <li><code>x</code> for execute permission.</li> </ul> <p>Here are some examples of how you might use symbolic notation with the <code>chmod</code> command:</p> <ul> <li>To give the owner read and write permissions: <code>chmod u+rw file.txt</code></li> <li>To remove execute permissions for the group: <code>chmod g-x file.txt</code></li> <li>To give all users read and execute permissions: <code>chmod a+rx file.txt</code></li> </ul>"},{"location":"support_and_training/cheat_sheet/#octal","title":"Octal","text":"<p>Octal notation is a more compact way of representing permissions using numbers. Each permission type (read, write, execute) is assigned a numeric value:</p> <ul> <li><code>4</code> for read permission.</li> <li><code>2</code> for write permission.</li> <li><code>1</code> for execute permission.</li> </ul> <p>To set permissions, you add these values together for each permission type. For example:</p> <ul> <li>Read and write permissions: <code>4 (read) + 2 (write) = 6</code></li> <li>Execute permission only: <code>1 (execute)</code></li> </ul> <p>You then represent the permissions for the user, group, and others as a three-digit number. For instance:</p> <ul> <li><code>600</code> gives read and write permissions to the owner and no permissions to the group and others.</li> <li><code>755</code> gives read, write, and execute permissions to the owner, and read and execute permissions to the group and others.</li> </ul> <p>To use octal notation with the <code>chmod</code> command, you specify the permissions directly as numbers. For example:</p> <ul> <li><code>chmod 600 file.txt</code></li> <li><code>chmod 755 file.txt</code></li> </ul> <p>These commands will set the permissions of <code>file.txt</code> accordingly.</p>"},{"location":"support_and_training/cheat_sheet/#changing-group-ownership","title":"Changing Group Ownership","text":"<p>Tip</p> <p>Users can't change the user ownership of a file. This is because this requires administrator privileges. User ownership changes can be requested from our consulting team. </p> <p>To change the group ownership of a file or directory, you can use <code>chgrp</code>. For example:</p> <pre><code>chgrp groupname /path/to/file\n</code></pre>"},{"location":"support_and_training/cheat_sheet/#compression-and-archiving","title":"Compression and Archiving","text":"<p>Tip</p> <p>For very large directories, use the file transfer node. Hostname: <code>filexfer.hpc.arizona.edu</code></p> <p>Are you planning on transferring files to or from HPC? Do you have a lot of them? Then archiving is for you! </p> <p>Archiving files is the process of consolidating one or more files or directories into a single, compressed package or archive file. This simplifies data management, reduces storage space, and streamlines file transfer and backup operations. Transferring a single archived file to an external backup location my result in transfer speeds that are an order of magnitude faster than transferring the same data as an uncompressed directory with thousands (or sometimes millions) of files.</p> <p>As an example, we'll use the archiving tool <code>tar</code>. The syntax to create an archive is  <pre><code>tar &lt;options&gt; &lt;output_archive_name&gt; &lt;contents&gt;\n</code></pre></p> <p>We'll use the options <code>czvf</code> which stands for:</p> <ul> <li><code>c</code>: Create a new archive</li> <li><code>z</code>: Filter the archive through gzip</li> <li><code>v</code>: Verbosely print the output of the archiving process. Optional</li> <li><code>f</code>: User archive file</li> </ul> <p>Archiving a directory <code>dir</code> and its contents into a single file called <code>dir.tar.gz</code> then looks like:</p> <pre><code>(puma) [user@wentletrap archiving_example]$ tar czvf dir.tar.gz dir\ndir/\ndir/subdir1/\ndir/subdir1/file.txt\ndir/file1.txt\ndir/file3.txt\ndir/file2.txt\n(puma) [user@wentletrap archiving_example]$ ls\ndir  dir.tar.gz\n</code></pre> <p>To unpack this archive, change the <code>c</code> to an <code>x</code> which stands for \"extract\":</p> <pre><code>tar xzvf dir.tar.gz\n</code></pre>"},{"location":"support_and_training/consulting_services/","title":"Consulting Services","text":""},{"location":"support_and_training/consulting_services/#hpc-consulting-services","title":"HPC Consulting Services","text":"<p> Consulting is available free of cost to everyone and we welcome you to reach out! Our services include, but are not limited to:</p> <ul> <li>Help for new users getting started with our resources. We know using HPC systems for the first time can be intimidating so scheduling an in-person meeting where you can ask loads of questions can help a lot.</li> <li>General issues that may occur, e.g., why is my job spending so long in the queue, where can I find information on topic &lt;A&gt;, why is this strange and unexpected thing happening, etc.</li> <li>Advice on debugging software, package installations, and job failures.</li> <li>Advice on code optimization and utilizing our resources more effectively/efficiently.</li> </ul> When can I ask for help? <p>Any time! But first, we encourage you to:</p> <ul> <li>Double-check our FAQs - We keep track of commonly asked questions and document their solutions. You might find what you're looking for there.</li> <li>Look through our online documentation - There's lots of information to help get you started that may answer your question or help give you a better idea of what to ask.</li> </ul> How can I effectively write a support request? <p>Glad you asked! Helping us help you goes a long way and can give you better answers faster. Some general rules of thumb:</p> <ul> <li> <p>Detail detail detailA full error log may seem like a lot to send, but the more information we have, the more likely we are to be able to diagnose and/or replicate your issue.</p> </li> <li> <p>Use reply-all to email chains We will cc hpc-consult in our responses so that our small consulting team is able to view the issue and contribute.</p> </li> <li> <p>Provide contextThere's a common support issue called The XY Problem. Say you have a problem and try to solve it yourself but the attempted solution produces an additional problem. Submitting a ticket requesting help with the attempted solution without information about the original issue can lead to more confusion. </p> </li> <li> <p>Submit a ticket for your questionsIf you reply to general system announcements or send emails to an HPC staff member's private inbox without cc'ing hpc-consult, your ticket may get lost and go unanswered. Submitting a ticket will ensure we have a record of your question and will get to it as promptly as we are able.</p> </li> </ul> What are our support policies? <p>Mostly it is common sense rather than strict rules. The primary consideration is that our consultants work regular hours with some flexibility built-in. So don't expect detailed responses at night or on the weekends. You might get a quick response but don't count on it.</p> <p>Our consultants typically don't know how to run your applications unless they have broad usage like Python or R. So once we determine it is likely an issue with the code we will refer you to the provider. We don't troubleshoot bugs or run profilers, but we support tools like Valgrind for you to use.</p> <p>We don't mind you asking lots of questions. We encourage you to ask for a consulting session via Zoom or Office Hours (see below). We are not in the office so you can't drop by (although we kind of miss that personal engagement).</p> <p>The bottom line is that the supercomputers are only really productive tools when you have the support to gain the most out of them to improve both your results and the time to get results.</p> How can I reach HPC consulting? <p>We use ServiceNow and can be reached with a support ticket. Many in our research community are accustomed to using the list service for hpc-consult. That continues to work but is not as efficient. And we really want to discourage sending emails directly to your favorite consultant.  </p>"},{"location":"support_and_training/consulting_services/#office-hours","title":"Office Hours","text":"<p>We host virtual, drop-in office hours every Wednesday from 2:00-4:00pm. Drop by to visit with our consultants to ask any questions you have about using HPC resources. It might be a bunch of getting started questions, or you might want to share your screen to walk us through a particular problem you're hung up on. We have private spaces for one-on-one consults. You can join us in Gather Town at this link. </p> <p>If you have never used Gather Town before and would like additional information, check out this page.</p>"},{"location":"support_and_training/consulting_services/#visualization-consulting","title":"Visualization Consulting","text":"<p>Visualization Consulting is a service that allows researchers to create graphical representations of their data. These computer-generated images and animations allow researchers to visually analyze that data and see the results of changing specific parameters. Services include consultation, demonstration, and training for high-resolution visualization output and simulation. </p> <p>Visualization consulting can be requested by contacting vislab-consult@list.arizona.edu</p>"},{"location":"support_and_training/external_resources/","title":"External Resources","text":"<p>Explore a number of resources beyond our center's offerings, from local community events to comprehensive software support and opportunities for accessing national supercomputing clusters. We've curated a collection of external organizations aimed at enhancing your HPC journey, providing diverse avenues for support and collaboration.</p>"},{"location":"support_and_training/external_resources/#access","title":"ACCESS","text":"<p>XSEDE supports 13 supercomputers and high-end visualization and data analysis resources across the country.  These are made available for use by researchers including at the University of Arizona. This program ends at the end of August 2022 to be replaced with a new program called ACCESS. This is also funded by the National Science Foundation.</p> <p>The new program is going through many changes so we will just include their website while has the most current information: https://access-ci.org/.</p> <p>Researchers who have current project allocations that were awarded via XSEDE \u2013including projects awarded at the August 22 XRAC meeting \u2013 should notice no changes to their resource access after August 31. For projects that expire on December 31, the ACCESS team recommends planning to submit a proposal during the usual September 15 to October 15 submission window under the existing XRAC guidelines.</p> <p>Researchers whose needs are at the smaller end of the scale should review the new ACCESS Allocations Marketplace information that is previewed on the XSEDE site pending the launch of the ACCESS site.</p>"},{"location":"support_and_training/external_resources/#ansys","title":"Ansys","text":"<p>For help with local installations, contact the College of Engineering IT services: support@engr.arizona.edu</p> <p>Ansys-specific support (debugging, questions about usage, etc) is available through PADT: support@padtinc.com</p> <p>To report license connection issues, contact: HPC consulting</p>"},{"location":"support_and_training/external_resources/#code-commons","title":"Code Commons","text":"<p>Code Commons provides a physical space for community and collaboration. Join to share experience, learn, mentor, discover opportunities, and work on your programming projects in the presence of others doing the same. Held every Wednesday from 2:00-6:00pm at the UArizona Library in the CATalyst Data Studios. For more information, see: https://codecommons.net/</p>"},{"location":"support_and_training/external_resources/#cyverse","title":"CyVerse","text":"<p>CyVerse provides life scientists with powerful computational infrastructure to handle huge datasets and complex analyses, thus enabling data-driven discovery. Their extensible platforms provide data storage, bioinformatics tools, image analyses, cloud services, APIs, and more: http://www.cyverse.org/about</p> <p>CyVerse is funded by the National Science Foundation\u2019s Directorate for Biological Sciences. They are a dynamic virtual organization led by the University of Arizona to fulfill a broad mission that spans our partner institutions: Texas Advanced Computing Center, Cold Spring Harbor Laboratory, and the University of North Carolina at Wilmington.</p>"},{"location":"support_and_training/external_resources/#open-science-framework","title":"Open Science Framework","text":"<p>The OSF is a free, open source service maintained by the Center for Open Science. Here are a few things you can do with the OSF:</p> <ul> <li> <p>Store your filesArchive your materials, data, manuscripts, or anything else associated with your research during the research process or after it is complete.</p> </li> <li> <p>Affiliate your projects with your institutionAssociate your projects with the University of Arizona which is a member. They will be added to UArizona's central commons, improving discoverability of your work and fostering collaboration.</p> </li> <li> <p>Share your workKeep your research materials and data private, make it accessible to specific others with view-only links, or make it publicly accessible. You have full control of what parts of your research are public and what remains private.</p> </li> <li> <p>Register your researchCreate a permanent, time-stamped version of your projects and files. Do this to preregister your design and analysis plan to conduct a confirmatory study, or archive your materials, data, and analysis scripts when publishing a report.</p> </li> <li> <p>Make your work citableEvery project and file on the OSF has a permanent unique identifier, and every registration can be assigned a DOI. Citations for public projects are generated automatically so that visitors can give you credit for your research.</p> </li> <li> <p>Measure your impactYou can monitor traffic to your public projects and downloads of your public files.</p> </li> <li> <p>Connect services that you useGitHub, Dropbox, Google Drive, Box, Dataverse, figshare, Amazon S3, ownCloud, Bitbucket, GitLab, OneDrive, Mendeley, Zotero. Do you use any of these? Link the services that you use to your OSF projects so that all parts of your project are in one place.</p> </li> <li> <p>CollaborateAdd your collaborators to have a shared environment for maintaining your research materials and data and never lose files again.</p> </li> </ul> <p>Learn more about the OSF on their Guides page, or email contact@osf.io with questions for support.</p>"},{"location":"support_and_training/external_resources/#research-bazaar","title":"Research Bazaar","text":"<p>Want to get involved with the Tucson coding community? ResBaz AZ offers weekly events that brings together scientists, software engineers, and enthusiasts of all skill levels. Additionally, an annual Research Bazaar is held each spring hosting research computing workshops and career panels: https://researchbazaar.arizona.edu/</p>"},{"location":"support_and_training/external_resources/#uarizona-data-science","title":"UArizona Data Science","text":"<p>Have some code-specific, data science, or related questions? Consider joining the UArizona Data Science Slack channel: https://jcoliver.github.io/uadatascience-slack/user-guide.html</p>"},{"location":"support_and_training/faqs/account_access/","title":"Account Access","text":"How do I create an account? <p>A step by step guide is available in our Account Creation page. </p> Why can't I log in? <p>There are many reasons you may be having issues logging in. A possible list of reasons may include:     <ul> <li>You haven't created an account yet or you have not yet been sponsored.</li> <li>You aren't using two-factor authentication (NetID+).</li> <li>You need to wait 15 minutes. If you just created your account, it takes time before you can log in.</li> <li>You're trying to connect using ssh <code>&lt;netid&gt;@login.hpc.arizona.edu</code>. This will not work. Instead, use: <code>ssh &lt;netid&gt;@hpc.arizona.edu</code>.</li> <li>You're using <code>&lt;netid&gt;@hpc.arizona.edu</code> or <code>&lt;netid&gt;@email.arizona.edu</code> as your username in PuTTY. Instead, use only your NetID.</li> <li>You've entered your password incorrectly too many times. After multiple failed password entries, the system will place a 60 minute ban on your account for security reasons. Your account will automatically unlock after 60 minutes. Attempting to log in before your account unlocks will reset the timer. </li> </ul></p> Why can't I enter my password in my terminal? <p>Linux systems do not display character strokes while entering your password which can make it look like the SSH client is frozen. Even though it doesn't appear that anything is happening, the system is still logging your input. To proceed, type your password at the prompt and press enter.</p> Why am I getting \"You do not appear to have registered for an HPC account\"? <p>If you have just registered for an HPC account, you need to wait a little while for the request to propagate through the University systems (this can take up to an hour).  Patience \ud83d\ude42 </p> Why am I getting \"permission denied\" when I try to log in? <p>You need an HPC account - see our Account Creation page for details.  Once you've done that, you'll need to wait a little while to log in. If your PI hasn't already added you to their group, you'll need to wait for that as well.</p> Why am I unable to log in with the error \"Your account is disabled and cannot access this application. Please contact your administrator.\"? <p>This error shows up when your NetID has been locked, usually due to multiple failed login attempts when trying to access university services. Contact 24/7 to unlock your account: https://it.arizona.edu/get-support</p> Why am I getting the message \"incorrect password\" when I try to log in? <p> <ul> <li>Ensure you are using the correct password. Sometimes typing your password into a plain text file and copying/pasting it into the terminal can help.</li> <li>You need to wait about 15 minutes after your account is approved for the account to be available</li> <li>You must enroll in NetID+. Depending on the application you use to log in, you may not get the typical NetID+/Duo menu of options, or an error message indicating this is your problem</li> </ul> </p> I've forgotten my password, how can I reset it? <p>HPC uses the same NetID login credentials as all UArizona services. If you need to reset your NetID password you can do so using the NetID portal.</p> How do I add members to my HPC research group? <p>Faculty members who manage their own HPC groups can follow the instructions in our Research and Class Groups page.</p> I'm leaving the university/not affiliated with the university, can I maintain/receive access to HPC? <p>Yes, if you are a former university affiliate or campus collaborator participating in research, you may register as a Designated Campus Colleague (DCC). Once your DCC status has been approved, you will receive a NetID+ which you may use to create an HPC Account. If you already have an HPC Account, no further action is required.</p>"},{"location":"support_and_training/faqs/general_computing/","title":"General Computing","text":"Why aren't common commands working? <p>There may be a few reasons for this. First, make sure your shell is set to Bash. If your shell is not set to Bash, contact our consultants so that they can reset it for you.      If your shell is set to Bash, double-check that you haven't changed, overwritten, or aliased anything important either your <code>~/.bashrc</code> or <code>~/.bash_profile</code>. E.g., unsetting your <code>PATH</code>, aliasing <code>.</code>, and so on will corrupt your environment and prevent you from interacting with HPC normally.    </p> Why is my terminal glitching (e.g. Ctrl+a puts me in the middle of my command prompt)? <p>When you log into HPC, the variable <code>$COMMAND_PROMPT</code> is set to display your current cluster (e.g. <code>(puma)</code>). Sometimes this can cause formatting problems. If you'd prefer to modify your <code>$PS1</code> (command prompt variable) instead, you can add the following to your <code>~/.bashrc</code>:    <pre><code>if [ -n \"${PROMPT_COMMAND}\" -a -r /usr/local/bin/slurm-selector.sh ]; then\n  SavePS1=${PS1}\n  Cur_Cluster=$(eval ${PROMPT_COMMAND} 2&gt;/dev/null)\n  PS1=\"${Cur_Cluster}${SavePS1}\"\n  unset PROMPT_COMMAND\n  for c in puma ocelote elgato; do\n    alias ${c}=\"PS1=\\\"(${c}) ${SavePS1}\\\"; . /usr/local/bin/slurm-selector.sh ${c}; unset PROMPT_COMMAND\"\n  done\n  unset Cur_Cluster SavePS1\nfi\n  </code></pre> </p> What are dot files and what is a <code>~/.bashrc</code>? <p>Files that start with a dot, i.e. a <code>.</code>, are hidden when executing <code>ls</code> without additional options. If you use <code>ls -la</code>, that will show you all the files that exist, both standard and hidden. Dot files are generally important configuration files so it's important to be careful with their contents and deleting them.       Your bashrc is a specific dot file that lives in your home directory (<code>~</code> is just shorthand for your home) and defines your environment every time you log in. Any commands you add to that file will be run whenever you access the system. This means, if you have common aliases you like to use, paths you like exported in your environment, etc., these can be added to your bashrc to avoid the headache of having to define them in every session.      For more information on working with hidden files, see our Linux cheat sheet page."},{"location":"support_and_training/faqs/jobs_and_scheduling/","title":"Jobs and Scheduling","text":"Why isn't my job running? <p> <ul> <li>There are a few reasons your job may not be running, check below for some ideas on diagnosing the issue:       Run <code>squeue --job  and see if there is anything listed under <code>(REASON)</code>. This may give an idea why your job is stuck in queue. We have a table in our Slurm documentation that describes what each Reason code means.  <li>Due to the number of HPC users, it may not always be possible to run a submitted job immediately. If there are insufficient resources available, your job will be queued and it may take up to a few hours for it to begin executing.</li> <li>Your group may have run out of standard hours. You can check your allocation using the command <code>va</code>.</li> <li>Your group/job has reached a resource usage limit (e.g., number of GPUs that may be used concurrently by a group, or a job has requested more than the 10 day max walltime). Try running <code>job-limits &lt;group_name&gt;</code> to see what limits you're subject to and if there are any problem jobs listed.</li> <li>You may be requesting a rare resource (e.g., 4 GPUs on a single node on Puma or a high memory node).       <ul> <li>If you are requesting a single GPU on Puma and are frustrated with the wait times, you might consider checking if Ocelote will work for your analyses. There are more GPU nodes available on that cluster, typically with shorter wait times.</li> <li>If you are trying to run a job on a standard node and have been waiting for a very long time, try checking its status using <code>job-history &lt;jobid&gt;</code>. If you see <code>Allocated RAM/CPU</code> above 5 GB on Puma or above 6 GB on Ocelote, then you are queued for the high memory node which can have very long wait times. To queue for a standard node, cancel your job and check that your script has the correct ratios.</li> </ul></li> My job has a Reason code when I check it with squeue. What does this mean? <p>If your job is in queue, sometimes Slurm will give you information on why it's not running. This may be for a number of reasons, for example there may be an upcoming maintenance cycle, your group's allocation may be exhausted, you may have requested resources that surpass system limits, or the node type you've requested may be very busy running jobs. We have a list of reason codes in our Monitoring Jobs and Resources page that will give more comprehensive information on what these messages mean. If you don't see the reason code listed, contact our consultants.   </p> Why do my jobs keep getting interrupted? <p>If your jobs keep stopping and restarting, it's likely because you are using Windfall. Windfall is considered lower priority and is subject to preemption by higher priority jobs. Before submitting a job to Windfall, consider using your group's allotted monthly hours first. Jobs using Standard hours will queue for a shorter period of time and will not be interrupted. You can check your group's remaining hours using the command <code>va</code>. To see more information on your allotted hours and the different job queues, see our page on compute allocations.If your job is not using the Windfall partition and is being interrupted, contact our consultants.    </p> Can I run programs on the login nodes? <p>No, software to run applications is not available on the login nodes. To run/test your code interactively, start an interactive session on one of the system's compute nodes. Processes running on the login nodes are subject to termination if we think they are affecting other users. Think of these as 'submit' nodes where you prepare and submit job scripts.    </p> Can I get root access to my compute nodes? <p> Unfortunately, that is not possible. The compute nodes get their image from the head node and have to remain the same. If you need to install software, for example, you can install the software locally in your account. See this example. Another option is to use containers as part of your workflow.   </p> Can I ssh to compute nodes? <p>Slurm will let you <code>ssh</code> to nodes that are assigned to your job, but not to others.    </p> Why am I getting out of memory errors? <p>       There are a few reasons you might get out of memory errors:       <ul> <li>You're using <code>-c &lt;N&gt;</code> to request CPUs. Based on the way our scheduler is set up, this will reduce the memory allocation for your job to 4 MB. To solve this, change your CPU request by either setting <code>--ntasks=&lt;N&gt;</code> or <code>--ntasks=1 --cpus-per-task=&lt;N&gt;</code>.</li> <li>You may not have specified the number of nodes required for your job. For non-MPI workflows, if Slurm scatters your CPUs across multiple nodes, you will only have access to the resources on the executing node. Explicitly setting <code>--nodes</code> in your script should help, e.g.:            <pre><code>#SBATCH --nodes=1</code></pre> </li> <li>You may not have allocated enough memory to your job. Try running <code>seff &lt;jobid&gt;</code> to see your memory usage. You may consider using memory profiling techniques, allocating more CPUs, or using a high memory node.</li> </ul> </p> Why shouldn't I use Windfall with OnDemand? <p>       Windfall jobs can be preempted by a higher priority queue. Each session creates an interactive job on a node and it is unsatisfactory to be dumped in the middle of that session. A desktop session would have the same unpleasant result.  Windfall can be used if you do not have enough standard time left. Consider though that a one hour session using one compute core only takes up 1 CPU hour.    </p> My interactive terminal session has been disconnected, can I return to it? <p>       No, unfortunately when an interactive job ends it is no longer accessible. This applies to both OnDemand sessions and those accessed via the command line. We recommend using the standard partition rather than windfall when running interactive jobs to prevent preemption.    </p> Are any modules loaded by default? <p>       Yes, when you start an interactive session via the terminal or submit a batch job, the modules gnu8, openmpi3, and cmake are loaded by default. If you need to use intel, you'll want to unload openmpi3 and gnu8 first.       However, if you start a terminal in an interactive desktop session through Open OnDemand, no modules are loaded by default in this environment. To start, at the minimum you'll want to run the command:       <pre><code>module load ohpc</code></pre> </p>"},{"location":"support_and_training/faqs/open_on_demand_issues/","title":"Open OnDemand Issues","text":"Why am I getting a message saying I'm not sponsored when trying to log in? <p>       If you are trying to log in to Open Ondemand and are seeing the following:        <ul> <li>You have not yet been sponsored by a faculty member. See our Account Creation page for instructions on getting registered for HPC.</li> <li>If you are already registered for HPC, this may be a browser issue. Try logging in again in an incognito session or different browser to test. If this succeeds, clearing your browser's cookies should help.</li> </ul> </p> Why am I getting a \"Bad Request\" message when trying to connect? <p>       If you are trying to log in to Open Ondemand and are seeing the following:               this may be a browser issue. Try logging in again in an incognito session or different browser to test. If this succeeds, clearing your browser's cache should help.   </p> Why am I getting an error saying \"We're sorry, but something went wrong\" when trying to log in? <p>       If you are trying to log in to Open Ondemand and are seeing the following:               check your storage usage in your home directory. You can do this by logging into HPC in a terminal session and using the command <code>uquota</code>. If your storage usage is &gt;50 GB, OnDemand cannot create the temporary files necessary to give access to the website. Try clearing out some space in your home and then logging back into OnDemand.   </p> Why are my Desktop sessions failing with 'Could not connect to session bus: failed to connect to socket /tmp/dbus-'? <p>       If you're seeing:               when trying to connect to an interactive desktop session, the likely culprit is Anaconda. For a permanent solution, you can run the following command from an interactive terminal session:       <pre><code>\nconda config --set auto_activate_base false\n      </code></pre>       This will prevent conda from auto-activating when you first log in and allow you to have more control over your environment. When you'd like to activate Anaconda, run <code>conda activate</code>.    </p>"},{"location":"support_and_training/faqs/secure_services/","title":"Secure Services","text":"How do I get access to Soteria? <p>     You will first need to request access. Follow the instructions in our Secure HPC page to see all the steps that are required.   </p> Why am I getting \"Operation timed out\" when I try to ssh to Soteria? <p>     It's possible you're not connected to the VPN. You will need to be connected to access any Soteria resources.   </p> How do I transfer my data to Soteria? <p>     The easiest way to transfer your data to Soteria is using Globus. We have a high assurance endpoint set up accessible under the endpoint UA HPC HIPAA Filesystems.   </p> Does my personal computer's Globus endpoint need to be set to high assurance? <p>   It depends.    You do not need your personal computer's Globus endpoint set to high assurance for the purposes of transferring data to Soteria using our UA HPC HIPAA Filesystems endpoint. HIPAA requires data transfer auditing. We log transfers on our DTN that use the HIPAA endpoint, regardless of the client's managed status.      If you are working with another institution/endpoint that only allows connections from high assurance endpoints, then you must enable High Assurance during your Globus Personal Connect configuration process. This requires that you are added to our Globus subscription which can be done by opening a support ticket to request help from our consultants. Your endpoint must be set to public to allow us to add you. Once you have been added, you may set your endpoint to private.    </p>"},{"location":"support_and_training/faqs/software_and_modules/","title":"Software and Modules","text":"Are any software modules loaded by default? <p>       Yes, when you start an interactive terminal session or submit a batch script, the modules ohpc, gnu8, openmpi3, and cmake are automatically loaded. If your code uses Intel compilers, you will want to manually unload gnu8 and openmpi3 to prevent conflicts.              The exception: If you are working in a terminal in an Open OnDemand interactive desktop session, nothing is loaded by default and you will need to manually load any necessary modules. To start, always use <code>module load ohpc</code> </p> What executables are available when I load a module? <p>       Load the module, find the path to the executable by checking the <code>$PATH</code> variable, then list the contents.  For example:       <pre><code>\nmodule load lammps\necho $PATH\nls /opt/ohpc/pub/apps/lammps/3Mar20/bin\nlmp_mpi\n      </code></pre>       Alternatively, use the command <code>module show &lt;software&gt;</code> to see all the environment variables that are set at load time.    </p> Why am I getting \"command: module not found\"?  <p>       There are a few different possibilities:       <ul> <li>You are not in an interactive session. Modules are not available on the login nodes. You may request an interactive session by using the command <code>interactive</code>. </li> <li>Your shell is not set to bash. If this is the case, contact our consultants so that they can reset it for you.</li> <li>You have modified or deleted your <code>~/.bashrc</code>. If this is the case, open (if the file exists) or create and open (if the file is missing) the file .bashrc in your home directory and add the lines:</li> <pre><code>\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n          </code></pre> </ul> </p> How do I access the module for Gaussian or Gaussview? <p>     You need to belong to a special group called g03.  You can request to be added by submitting a help ticket. This is a constraint in Gaussian that other modules do not have.   </p> How can I maximize my software performance on Puma? <p>       If you are able to compile your software you can take advantage of most of the AMD Zen architecture.   </p> Compiler Arch-Specific Arch-Favorable GCC 9 <code>-march=znver2</code> <code>-mtune=znver2</code> LLVM 9 <code>-march=znver2</code> <code>-mtune=znver2</code> Is the Intel compiler faster than GCC on Puma? <p>     Intel compilers are optimized for Intel processors. There is some debate around the concept of unfair CPU dispatching in Intel compilers. By default, software on the HPC clusters is built with GCC (on Puma it is GCC 8.3).  This is in keeping with our preference for community software.   </p> I've been using an older version of Apptainer (formerly Singularity), why isn't available anymore? <p>     Prior versions of Apptainer are routinely removed since only the latest one is considered secure. Notify the consultants if you need help with transition to the current version. Apptainer is installed on the operating systems of all compute nodes so does not need to be loaded with a module.    </p> Can I use Windows applications on HPC? <p>     Unfortunately, Windows applications can't be run on HPC. However, AWS has been used successfully for Windows software with GPU needs. It\u2019s easy to set up, cost effective, and very scalable. Amazon also has a cloud credit for research program available.          You may also consider trying Jetstream2, a national resource where you can create and use Windows virtual machines. More information can be found here: https://jetstream-cloud.org/ </p> How do I install this R package/Why can't I install this R package? <p>       R installations can sometimes be frustrating. We have instructions for how to set up a usable R environment, how to diagnose and troubleshoot problems, and steps to help with known troublesome packages documented in in our Using and Customizing R Packages section.    </p> How do I install Python packages? <p>      You can install python packages locally using either a virtual environment or a local conda environment.    </p> How do I access custom Python packages from an OOD Jupyter session? <p>   You can install packages and make them available by first creating a virtual environment or conda environment, then setting up a custom Jupyter kernel. See instructions in our Python or Anaconda documentation for details.    </p> How do I take advantage of the Distributed capability of Ansys? <p>     Ansys has the Distributed capability built in to increase performance. Ansys uses the Intel compiler and so uses Intel MPI.  By default, we load OpenMPI, so you will need to do this:      <pre><code>\nmodule unload gnu8 openmpi3\nmodule load intel\nmodule load ansys\n    </code></pre> </p>"},{"location":"support_and_training/faqs/storage/","title":"Storage","text":""},{"location":"support_and_training/faqs/storage/#hpc-storage","title":"HPC Storage","text":"General Storage Do you allow users to NFS mount their own storage onto the compute nodes? <p>        No. We NFS mount storage across all compute nodes so that data is available independent of which compute nodes are used. See our page on transferring data for more information.   </p> I can't transfer my data to HPC with an active account. What's wrong? <p>       After creating your HPC Account, your home directory will not be created until you log in for the first time. Without your home directory, you will not be able to transfer your data to HPC. If you are struggling and receiving errors, sign into your account either using the CLI through the bastion or logging into Open OnDemand and then try again.   </p> I accidentally deleted files, can I get them back? <p>       It's unlikely. Backups are not made and anything deleted is permanently erased. If you have deleted an important file, reach out to our consultants as soon as possible and they may be able to retrieve it, however, this is not guaranteed.               To ensure your data are safe, we strongly recommend:       <ul> <li>Making frequent backups, ideally in three places and two formats. Helpful information on moving data can be found on our page Transferring Data.</li> <li>Use <code>rm</code> and <code>rm -r</code> with caution as these commands cannot be undone! Consider using <code>rm -i</code> when removing files/directories. The <code>-i</code> flag will prompt you to manually confirm file removals to make really sure they can be deleted.</li> <li>You can open a support ticket to request assistance.  Files that are deleted may not have been removed from the storage array immediately (though this is not guaranteed), don't wait more than a few days.</li> </ul> </p> My home directory is full, what's using all the space? <p>       If your home directory is full and you can't find what is taking up all the space, it's possible the culprit is a hidden file or directory. Hidden objects are used for storing libraries, cached singularity/apptainer objects, saved R session, Anaconda environments, configuration files, and more. It's important to be careful with hidden, or \"dot\", files since they often control your environment and modifying them can lead to unintended consequences.       To view the sizes of all the objects (including hidden) in your home, one quick command is <code>du -hs $(ls -A ~)</code>, for example:       <pre><code>\n[netid@junonia ~]$ du -hs $(ls -A ~)\n32K     Archives\n192M    bin\n4.7G    Software\n46M     .anaconda\n1.9M    .ansys\n4.0K    .apptainer\n16K     .bash_history\n4.0K    .bash_logout\n4.0K    .bash_profile\n12K     .bashrc\n20M     ondemand\n      </code></pre> </p> I'd like to share data I have stored on HPC with an external collaborator, is this possible? <p>       Unfortunately, without active university credentials it is not possible to access HPC compute or storage resources. External collaborates who need ongoing access may apply for Designated Campus Colleague, or DCC, status. This is a process done through HR and will give the applicant active university credentials allowing them to receive HPC sponsorship.              Otherwise, data will need to be moved off HPC and made available on a mutually-accessible platform. This may include (but is not limited to): Google Drive, AWS S3, Box, and CyVerse's Data Store.   </p> xdisk Can someone other than a PI manage a group's xdisk? <p>     Yes, a PI can add a trusted group member as a delegate by following the instructions in our Research and Class Groups page. Once a group member is added as a delegate, they can manage the group's xdisk allocation on behalf of their PI in the user portal.   </p> Who owns our group's /xdisk? <p>       A group's PI owns the xdisk allocation. By default, your PI has exclusive read/write/execute privileges for the root folder <code>/xdisk/&lt;pi_netid&gt;</code>.   </p> Can a PI make their /xdisk writeable for their whole group?  <p>       By default, members of a research group only have write access to their subdirectories under <code>/xdisk/&lt;pi_netid&gt;</code>. If they so choose, a PI may allow their group members to write directly to that location by running the following command:       <pre><code>\nchmod g+w /xdisk/&lt;pi_netid&gt;\n      </code></pre>       For more information on Linux file permissions, see our cheat sheet.    </p> Where can group members store their files? <p>       When an xdisk allocation is created, a subdirectory is automatically generated for and owned by each individual group member. Group members can access their individual spaces by:       <pre><code>\ncd /xdisk/&lt;pi_netid&gt;/&lt;netid&gt;\n      </code></pre>       If a user joins the group after the xdisk was created and <code>/xdisk/&lt;pi_netid&gt;</code> is not writeable for group members, contact our consultants and they can create one.   </p> A group member's directory isn't in our /xdisk, how can we add it? <p>       Typically when an xdisk allocation is created, it will automatically generate a directory for each group member. In the unlikely event that it doesn't or, more commonly, a group member is added after the allocation has been created, contact our consultants and they can create one.    </p> Do we need to request an individual allocation within the /xdisk for each user in our group? <p>       No, the full xdisk allocation is available for every member of the group. It's up to group members to communicate with one another on how they want to utilize the space.   </p> Why am I getting xdisk emails?  <p>        xdisk is a temporary storage space available to your research group. When it's close to its expiration date, notifications will be sent to all members of your group..   </p> Why am I getting \"/xdisk allocations can only be authorized by principal investigators\"?  <p>       xdisks are managed by your group's PI by default. This means if you want to request an xdisk or modify an existing allocation (e.g., extending the time limit or increasing the storage quota), you will need to consult your PI. Your PI may either perform these actions directly or, if they want to delegate xdisk management to a group member, they may do so by following the instructions under Delegating Group Management Rights.   </p> How can we modify our xdisk allocation? <p>       To modify your allocation's time limit or storage quota, your PI can either do so through the Web Portal under the Storage tab, or via the command line. If your PI would like to delegate management rights to a group member, they may follow the instructions under Delegating Group Management Rights. Once a group member has received management rights, they may manage the allocation through our web portal.   </p> Why am I getting \"xdisk: command not found\"? <p>       If you're getting errors using <code>xdisk</code> commands in a terminal session, check that you are on a login node. If you are on the bastion host (hostname: <code>gatekeeper</code>), are in an interactive session, or are on the filexfer node, you won't be able to check or modify your xdisk. When you are on a login node, your terminal prompt should show the hostname junonia or wentletrap. You can also check your hostname using the command <code>hostname</code> </p> Why am I getting errors when trying to extend my allocation? <p>       If you're trying to extend your group's allocation but are seeing something like:       <pre><code>\n(puma) [netid@junonia ~]$ xdisk -c expire -d 1\ninvalid request_days: 1\n      </code></pre>       for every value you enter, your xdisk has likely reached its maximum time limit. To check, have a delegate or PI go to portal.hpc.arizona.edu, click Manage XDISK, and look at the box next to Duration. If you see 300, your allocation cannot be extended further.      If your allocation is at its limit, you will need to back up your data to external storage (e.g., a local machine, lab server, or cloud service). Once your xdisk has been removed (either by expiring or through manual deletion), you can immediately create a new allocation and restore your data. Detailed xdisk information can be found on our HPC High Performance Storage page. You may also want to look at our page on Transferring Data.   </p> Can we keep our xdisk allocation for more than 300 days? <p>       No, once an xdisk has reached its time limit it will expire. It's a good idea to start preparing for this early by making frequent backups and paying attention to xdisk expiration emails.    </p> What happens when our xdisk allocation expires? <p>       Once an xdisk expires, all the associated data are deleted. Deleted data are non-retrievable since HPC is not backed up. It's advised to keep frequent backups of your data on different platforms, for example a local hard drive or a cloud-based service, or (even better) both! Check our Storage documentation for more information on alternative storage offerings.   </p> What's the best way to backup/transfer our data before our xdisk expires? <p>       Before your group's xdisk expires, you'll want to make an external backup of anything you need to keep. External storage options include personal computers, lab servers, external hard drives, or cloud services such as AWS.       If you're moving large quantities of data, Globus is a great option. We have instructions in our Globus documentation for setting up and using this software.      We strongly recommend making archives (.tar, .zip, files etc.) of large directories prior to transferring them off the system. In general, transfer software struggles with moving many small files and performs much more efficiently moving fewer large files. You will get the better transfer speeds (sometimes by orders of magnitude) if you compress your files prior to transferring them. This can be done on our filexfer node which is designed for large file management operations (hostname: <code>filexfer.hpc.arizona.edu</code>).    </p> Once our xdisk expires, can we request a new one? <p>       Yes, a new xdisk may be requested immediately after the old partition expires. Data, however, may not be transferred directly from the old partition to the new one.    </p> Can a PI have more than one xdisk active at a time? <p>       No, only one xdisk may be active per PI at a given time.    </p>"},{"location":"support_and_training/faqs/storage/#rental-storage","title":"Rental Storage","text":"How do I request rental storage? <p>        Rental storage can be requested by PIs through the user portal. A guide with screenshots can found in our rental storage documentation.    </p> Why can't I see my rental allocation when I log into HPC? <p>       Rental allocations are not mounted on the login or compute nodes. You can access your allocation by logging in to our data transfer nodes: <code>filexfer.hpc.arizona.edu</code> </p> Can I analyze data stored in /rental directly in my jobs? <p>       No, rental storage is not mounted on the HPC login or compute nodes which means jobs cannot access <code>/rental</code> directly. Data stored in <code>/rental</code> need to be moved to <code>/home</code>, <code>/groups</code>, or <code>/xdisk</code> to be available.    </p>"},{"location":"support_and_training/faqs/storage/#r-das-storage","title":"R-DAS Storage","text":"If my VPN connection is dropped, will my connection survive?  <p>       Yes, if you reconnect to the VPN, your connection will still be available.    </p> Why am I getting \"There was a problem connecting to the server\"? <p>     This error is seen if you are not connected to the University VPN.   </p> Can I access R-DAS for HPC usage? <p>       R-DAS is not mounted on the HPC compute nodes or login nodes, and is not meant for running computations. But you can follow the steps in our R-DAS documentation to share data between your R-DAS allocation and your HPC storage (<code>/home</code>, <code>/groups</code>, <code>/xdisk</code>):    </p>"},{"location":"support_and_training/faqs/storage/#tier-2-aws-storage","title":"Tier 2 AWS Storage","text":"Who can submit a Tier 2 storage request? <p>     A PI must submit their group's storage request. The exception to this is if a group member has been designated xdisk/storage delegate. Delegates may submit a request on behalf of their PI by switching users in the user portal.   </p> How long is the turnaround time to between submitting the storage request and obtaining an S3 account? <p>      In most cases it will be in one business day.   </p> What is the pricing for S3? <p>     You should check the Amazon site. As of March 2022:     <ul> <li>Frequent Access Tier, First 50 TB / Month $0.023 per GB</li> <li>Frequent Access Tier, Next 450 TB / Month $0.022 per GB</li> <li>Frequent Access Tier, Over 500 TB / Month $0.021 per GB</li> </ul> </p> Can I move my data directly to Glacier if I know it is archival. That way I can skip the S3 expenses? <p>     Not in the first release, but potentially as a future offering.   </p> Is the monthly billing based on average usage? <p>      Yes. The capacity used is recorded daily and the billing is a monthly average.   </p> What is a KFS number? <p>      It is used for accounting purposes and used by your Department's finance specialist. If you are unsure what your KFS number is, contact your department's financial services office.   </p> Can I view my storage in each of S3, Glacier and Deep Glacier? <p>      Yes, you can use the CLI (command line interface) for information about your usage   </p> Can I limit the amount of data that goes into S3 so I can control the expense? <p>      No, but you can track the usage and remove any data that should not be there.   </p> What is the difference between Glacier and Deep Glacier? <p>      Glacier is effectively large, slow disks and Deep Glacier is tape storage.   </p> I use workflows to move data using Google Drive, and to share with others. Will AWS support something similar? <p>      Amazon S3 will likely support what you do. Perhaps our consultants can help to rework your workflows.   </p> Are there charges for data movement? <p>      You will not be charged for data ingress, egress or other operations.   </p> Can I have multiple S3 buckets associated with my account? <p>     Yes   </p> Are there any limits on uploads, e.g. may transfer size/day? <p>     The max file size is 5 TB based on Amazon's official documentation.   </p> What why am I receiving an email: \"ALARM: \"ua-rt-t2-netid capacity growth alarm\" in US West (Oregon)\"? <p>     This alert is sent to notify you whenever your storage usage grows by 10% relative to the previous week. This ensures that you're aware of any unintended spikes and the potential resulting costs.    </p>"},{"location":"support_and_training/glossary/","title":"Glossary","text":"<p>Cluster</p> <p>A group of nodes connected to each other by a fast network.  The network in ElGato and Ocelote is 56Gb Infiniband.  What this gains for the user is the ability to connect nodes together to perform work beyond the capacity of a single node. Some jobs use hundreds of cores and terabytes of memory.</p> <p>CPU/processor/socket/core</p> <p>These terms are often used interchangeably, especially processor and CPU. The most straight forward way to think of the compute nodes is that they contain two physical sockets (or processor chips) which are located under their heatsinks. Each socket contains multiple cores.  Each core functions like a separate processor. Ocelote has 2 sockets with 14 cores in each so all you need to know is that there are 28 cores.  ElGato has 2 sockets with 6 cores in each, for a total of 12 cores. If your laptop is quad core, it has one socket with four cores, as a comparison.</p> <p>Data Mover Node</p> <p>A node connected to the public internet and dedicated to moving data to/from external computers. We have two DTN nodes known collectively as <code>filexfer.hpc.arizona.edu</code>.</p> <p>Distributed memory computing</p> <p>In software, a program or group of programs that run on multiple nodes or shared-memory instances and use programs such as MPI to communicate between the nodes. In hardware, a cluster that runs distributed-memory programs. Distributed-memory programs are limited in memory size only by job limits to support many users. </p> <p>Embarrassingly parallel</p> <p>A program where little effort is involved in separating the code into parallel tasks and so parallel scaling is very efficient.  Some astronomy codes fit this model.</p> <p>Encumbered hours</p> <p>This refers to any hours in your allocation that are reserved by running jobs. When you submit a job, any hours that it requires are moved to the \"encumbered\" category. As soon as your job ends, unused hours will be refunded and used hours will permanently be deducted from your monthly allotment. </p> <p>GPU</p> <p>A graphical processing unit, a specialized type of CPU derived from a graphics card. Effectively has hundreds of small cores. For certain tasks (those that can be effectively parallelized), a GPU is much faster than a general-purpose CPU.</p> <p>Head node</p> <p>The head node is for managing the cluster and is not available to users.</p> <p>HPC</p> <p>High performance computing. Implies a program too large for, or that takes too long on, a laptop or workstation. Also HTC (high throughput computing) similar but oriented to processing many small compute jobs.</p> <p>Hyperthreading</p> <p>Intel processors (in this case \"cores\") have hyper-threading which can make one core look like two; but it does not add compute capacity in most HPC cases, so we turn it off.</p> <p>Login node</p> <p>A cluster node accessible to users and dedicated to logins, editing, moving data, submitting jobs.  </p> <p>MPI computing</p> <p>Message passing interface, software standard used for most programs that use distributed memory. MPI calls lower-level functions, either networking or shared memory. On a cluster that means it can run transparently either on one node or multiple nodes. MPI has multiple implementations (OpenMPI, MVAPICH, OpenMPI or Intel MPI) that must be used consistently to both compile and run an MPI program.</p> <p>Network bandwidth</p> <p>The amount of data that can be moved over a network per second. For FDR Infiniband on Ocelote that is 56Gbps (Giga bits per second)</p> <p>Network latency</p> <p>In HPC terms, it is usually the delay in the network for messages being passed from one node to another.  This is optimized by a hardware technology called RDMA (Remote Direct Memory Access)</p> <p>Node (aka compute node)</p> <p>A single computer in a box, functionally similar to a desktop computer but typically more powerful and packaged for rackmount in a datacenter. Usually two CPU sockets or four sockets with very large memory vs. one socket for a desktop. Ocelote standard nodes have 28 cores and 192GB memory.</p> <p>Parallel Programming</p> <p>A program that is either multi-tasking (like MPI) or multi-threaded (like OpenMP) or both, in order to effectively use more cores and more nodes and get more computing done. May be either shared-memory or distributed-memory. Unlike a serial program.</p> <p>Parallel Scaling</p> <p>The efficiency of a parallel program, usually defined as the parallel speedup of the program divided by the number of cores occupied. Speedup is defined as the serial run time divided by the parallel run time. Usually parallel computing introduces overhead, and scaling is less than 1 (or 100%).  In most cases, scaling starts at 1 on 1 core (by definition) and decreases as more cores are added, until some point is reached at which adding more cores adds overhead and makes the program slower.</p> <p>Scheduler/HPC scheduler</p> <p>A program that maintains a list of batch jobs to be executed on a cluster, ranks them in some priority order, and executes batch jobs on compute nodes as they become available. It tries to keep the cluster from being overloaded or idle. Puma, Ocelote, and ElGato use SLURM.</p> <p>Scratch storage</p> <p>A temporary file system, designed for speed rather than reliability, and the first tier in the storage hierarchy. On Ocelote and ElGato these are internal SATA disks and referenced as <code>/tmp</code>.</p> <p>Shared memory computing</p> <p>A program that runs multiple tasks or software threads, each of which sees the same available memory available from the operating system, and shares that memory using one of the multiple shared memory/multi-threading communication methods (OpenMP, pthreads, POSIX shm, MPI over shared memory, etc.). Shared memory programs cannot run across multiple nodes. Implies a limit (a little less than the amount of memory in the node) to the memory size of the running program.</p> <p>Single-threaded computing</p> <p>A software program that cannot take advantage of multi-threading because it was written without multi-threading support. Essentially can use only one core on one node regardless of the number of cores available. Multiple single-threaded programs can be run on a single node on multiple cores.</p> <p>SSD</p> <p>Solid state disk, memory chips packaged with an interface that appears to the computer to be a disk drive. Faster than rotating disk drives and still more expensive, though decreasing in price over time.</p> <p>Storage hierarchy</p> <p>Each tier of storage is larger and slower than the preceding tier. The first is data in the processor including the processor cache.  The next tier is memory.  Page or swap is an extension of memory but is very inefficient since it actually writes to disk. You should next consider <code>/tmp</code> which is the local disk on each node.  You have no access to <code>/tmp</code> once the job ends.  Shared storage is all of <code>/home</code>, <code>/groups/PI</code>, and <code>/xdisk</code>, and is the slowest.</p> <p>Supercomputer</p> <p>A large and powerful cluster. We currently have three: Puma, Ocelote, and ElGato.</p> <p>VM or virtual machine</p> <p>This compute model is not usually found in the HPC environment.  It is a method of running several or many virtual machines on one physical machine.  Since HPC nodes are busy most of the time the cost of the VM overhead and management is not worthwhile. </p>"},{"location":"support_and_training/grants/","title":"Grants","text":""},{"location":"support_and_training/grants/#grant-resources","title":"Grant Resources","text":""},{"location":"support_and_training/grants/#overview","title":"Overview","text":"<p>University of Arizona (UArizona) researchers have access to a variety of high performance computing and storage resources through both UArizona HPC and the NSF-funded CyVerse project. A moderate amount of compute time and storage is free to any research faculty, also known as principal investigators (PIs).</p> <p>You might consider these scenarios:</p> <ol> <li> <p>Run proof-of-concept on the HPC clusters</p> <p>Compute time and storage are free on a moderate scale so no funding is needed to test and develop code, workflows, and methodologies for grants.</p> </li> <li> <p>Run the funded compute on the HPC clusters </p> <p>Using our buy-in model, the compute part of the funding can complement the existing compute resources and reduce personnel costs.</p> </li> <li> <p>Co-locate your cluster in the Computer Center</p> <p>Take advantage of the enterprise class facilities and support to save space and reduce administration and setup costs. More information here.</p> </li> <li> <p>CyVerse data center</p> <p>These reside at UArizona, are funded by the National Science Foundation\u2019s Directorate for Biological Sciences, and provide life scientists with powerful computational infrastructure.</p> </li> <li> <p>ACCESS (formerly XSEDE) national cyberinfrastructure HPC and Cloud providers.</p> <p>Available for free to any US-based researcher. Get started here. UArizona researchers can request startup allocations to test out their system(s) of choice. After the startup allocation, full allocations can be requested for millions of CPU-hours.</p> </li> </ol>"},{"location":"support_and_training/grants/#the-university-of-arizona-research-computing-and-cyberinfrastructure-plan-2022","title":"The University of Arizona Research Computing and Cyberinfrastructure Plan 2022","text":"<p> Click here to download .docx version <p></p>"},{"location":"support_and_training/grants/#data-management-plans","title":"Data Management Plans","text":"<p>A data management plan documents the lifecycle of your data. The plan provides details on data collection for storage, access, sharing, and reproducibility of your results.  A good data management plan will ensure the availability and accessibility of your research results after your project is complete and you have published the results, increasing the value of your research and possible reuse by other researchers. </p> <p>The UArizona Libraries host automated tools to design and execute data management plans for grants. More information here: https://data.library.arizona.edu/</p>"},{"location":"support_and_training/grants/#funding-agency-requirements","title":"Funding Agency Requirements","text":"<p>In 1999, the U.S. Office of Management and Budget amended OMB Circular A-110 to require research data produced with funding from Federal agencies be made publicly available through procedures established through the Freedom of Information Act (FOIA).</p> <p>Federally funded research - access to publications and data</p> <p>The White House Office of Science and Technology Policy (OSTP) released a policy memorandum, \u201cIncreasing Access to the Results of Federally Funded Scientific Research,\u201d on February 22, 2013. See the Federal Agency Policies for Public Access website for brief descriptions of the policies for top UA federal funding agencies and where to get further help.</p> <p>The Library has created a summary table of public access plans for all of the 19 federal funding agencies that fall under the OSTP memo.</p> <p>The University offers other resources for grant applications here: https://research.arizona.edu/development/training-grants-resources</p>"},{"location":"support_and_training/people/","title":"Research Computing Support Staff","text":"<p>Welcome to our Support Staff page, where you can get acquainted with the individuals who make up our HPC consulting team.</p>"},{"location":"support_and_training/people/#research-computing-facilitation-manager","title":"Research Computing Facilitation Manager","text":"<ul> <li> <p> Chris Reidy</p> <p>Foo bar baz</p> </li> </ul>"},{"location":"support_and_training/people/#hpc-consulting","title":"HPC Consulting","text":"<ul> <li> <p> Ethan Jahn</p> <p>barring foo</p> </li> <li> <p> Soham Pal</p> <p>fooing baz</p> </li> <li> <p> Sara Willis</p> <p>burble burble </p> </li> <li> <p> Derrick Zwickl</p> <p>meep snarf</p> </li> </ul>"},{"location":"support_and_training/people/#visualization-consulting","title":"Visualization Consulting","text":"<ul> <li> <p> Devin Bayly</p> <p>Bazzing foo</p> </li> </ul>"},{"location":"support_and_training/workshops/data_management_workshops/","title":"Data Management Workshops","text":""},{"location":"support_and_training/workshops/data_management_workshops/#data-management-workshops","title":"Data Management Workshops","text":"<p>Learn about managing your data on UA's HPC cluster. Co-sponsored with University Libraries. Virtual only</p> <p>These workshops are held twice a year. This semester there will be one two-hour workshop rather than two one-hour workshops.</p> <p>Attendance will be on zoom so there is no registration needed.</p>"},{"location":"support_and_training/workshops/data_management_workshops/#presentation-pdfs","title":"Presentation PDFs","text":"<p> Click here to download PDF (Part 1) Click here to download PDF (Part 2) </p>"},{"location":"support_and_training/workshops/data_management_workshops/#video-presentation-part-1","title":"Video Presentation (Part 1)","text":"<p>Will upload once on YouTube</p>"},{"location":"support_and_training/workshops/intro_to_containers/","title":"Intro to Containers","text":""},{"location":"support_and_training/workshops/intro_to_containers/#intro-to-containers-on-hpc","title":"Intro to Containers on HPC","text":"<p>There are times when the use of containers is more efficient for using HPC resources, and there are other times when it is the only way. This workshop will give you a quick view of the concepts and how we implement them. We can also bring these to department meetings.</p>"},{"location":"support_and_training/workshops/intro_to_containers/#workshop-presentation-files","title":"Workshop Presentation Files","text":"<p> Click here to download PDF Version Click here to download PowerPoint Version </p>"},{"location":"support_and_training/workshops/intro_to_containers/#interactive-materials","title":"Interactive Materials","text":""},{"location":"support_and_training/workshops/intro_to_containers/#apptainer","title":"Apptainer","text":"<p>note that if you create many containers many gigibytes of cache files may be written  to ~/.apptainer.  The files can be safely deleted, so be careful not to fill your home directory. </p> <p>First start an interactive session <pre><code>[laptop ~]$ ssh netid@hpc.arizona.edu\n[netid@gatekeeper ~]$ shell\n(puma) [netid@wentletrap ~]$ elgato\n(elgato) [netid@wentletrap ~]$ interactive\n[netid@cpu37 ~]$ apptainer help\n[netid@cpu37 ~]$ apptainer help build\n</code></pre></p>"},{"location":"support_and_training/workshops/intro_to_containers/#creating-a-container","title":"Creating a Container","text":""},{"location":"support_and_training/workshops/intro_to_containers/#pull-from-docker-registry","title":"Pull from Docker registry","text":"<p>Less reproducible -- image can change</p> <pre><code>apptainer pull docker://godlovedc/lolcow\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_containers/#pull-from-container-library","title":"Pull from container library","text":"<p>More reproducible</p> <pre><code>apptainer pull library://sylabsed/examples/lolcow\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_containers/#build-from-docker-registry","title":"Build from Docker registry","text":"<p>More optionsconverts to latest format, &amp; needs a name</p> <pre><code>apptainer build lolcow.sif docker://godlovedc/lolcow\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_containers/#running-apptainer-on-hpc","title":"Running Apptainer on HPC","text":""},{"location":"support_and_training/workshops/intro_to_containers/#apptainer-shell","title":"Apptainer shell","text":"<p><code>apptainer shell</code> opens a command prompt to the environment inside the container. Try executing some commands.  Notice that although you are inside  the container (a separate operating system), you will be able to  list the contents of your home directory.</p> <pre><code>[netid@i16n2 ~]$ apptainer shell lolcow_latest.sif\nApptainer&gt; \nApptainer&gt; exit\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_containers/#apptainer-run","title":"Apptainer run","text":"<p><code>apptainer run</code> executes the default run script within the container, which was specified when the container was created.</p> <pre><code>[netid@i16n2 ~]$ apptainer run lolcow_latest.sif\n[netid@i16n2 ~]$ # Or\n[netid@i16n2 ~]$ ./lolcow_latest.sif\n</code></pre> <pre><code>[netid@i16n2 ~]$ apptainer run library://sylabsed/examples/lolcow\nINFO:    Using cached image\n _______________________________________\n/ Wrinkles should merely indicate where \\\n| smiles have been.                     |\n|                                       |\n\\ -- Mark Twain                         /\n ---------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_containers/#apptainer-on-hpc-as-a-batch-job","title":"Apptainer on HPC as a Batch Job","text":"<p>No modules need to be loaded to execute a container in  a Slurm batch submission.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=lolcow\n#SBATCH --ntasks=1 \n#SBATCH --nodes=1 \n#SBATCH --mem=1gb \n#SBATCH --time=00:01:00 \n#SBATCH --partition=standard \n#SBATCH --account=YOUR_GROUP \n\n\ncd /path/to/container\napptainer run lolcow_latest.sif\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_containers/#binding-file-paths","title":"Binding File Paths","text":"<p>Access to your files outside the container: Binding  You automatically get <code>/home</code>, <code>/tmp</code>, <code>/xdisk</code>, <code>/groups</code>, and <code>$PWD</code></p> <pre><code>[netid@i16n8 ~]$ echo \"Hello from inside the container\" &gt; $HOME/hostfile.txt\n[netid@i16n8 ~]$ apptainer exec lolcow_latest.sif cat $HOME/hostfile.txt\nHello from inside the container\n</code></pre> <p><pre><code>[netid@i16n8 ~]$ echo \"Drink milk (and never eat hamburgers).\" &gt; data/cow_advice.txt\n[netid@i16n8 ~]$ apptainer exec --bind $PWD/data:/mnt lolcow_latest.sif cat /mnt/cow_advice.txt\nDrink milk (and never eat hamburgers).\n</code></pre> Alternatives:</p> <p><pre><code>$ apptainer shell --bind /data my-container.sif\n</code></pre> <pre><code>$ export APPTAINER_BINDPATH=/data\n</code></pre></p>"},{"location":"support_and_training/workshops/intro_to_containers/#gpus-and-containers","title":"GPUs and Containers","text":"<p>We show three ways to use tensorflow containers on HPC. We just use Tensorflow as an example</p> <ol> <li>Tensorflow example using a system module*</li> <li>Tensorflow using a custom image pulled from Nvidia</li> <li>Tensorflow from DockerHub</li> </ol> <p>*Containers as modules: caffe, pytorch, rapids, tensorflow, and theano</p>"},{"location":"support_and_training/workshops/intro_to_containers/#references","title":"References","text":"<p>In addition to our Apptainer documentation, see the following resources for helpful information on getting started with containers:</p> Reference Link UArizona HPC Examples Github https://ua-researchcomputing-hpc.github.io NIH Containers Documentation https://hpc.nih.gov/apps/Apptainer.html Sylabs User Guide https://sylabs.io/guides/3.5/user-guide/introduction.html Sylabs Examples https://github.com/sylabs/examples TACC Container Basics https://containers-at-tacc.readthedocs.io/en/latest/ CyVerse Container Camp https://cyverse-container-camp-workshop-2018.readthedocs-hosted.com"},{"location":"support_and_training/workshops/intro_to_hpc/","title":"Intro to HPC","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#intro-to-hpc-workshop-materials","title":"Intro to HPC Workshop Materials","text":"<p> Click here to download PDF (Fall 2023) Click here to download PDF (Spring 2024) <p></p>"},{"location":"support_and_training/workshops/intro_to_hpc/#video-presentation","title":"Video Presentation","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#interactive-materials","title":"Interactive Materials","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#logging-in","title":"Logging In","text":"<p>Instructions on Logging In</p>"},{"location":"support_and_training/workshops/intro_to_hpc/#activity-lets-run-a-batch-job","title":"Activity: Let\u2019s run a batch job!","text":"<p>Let\u2019s write a basic Python script, then submit it to the queue </p> <p>Process:</p> <ol> <li>write a Python script</li> <li>write a batch script</li> <li>submit the batch script</li> </ol> <p>Step 1: Write a Python script</p> <p>You can write anything you want! It can be as simple as print(\u2018hello world\u2019), or you can make it more interesting.. up to you ;)</p> <ol> <li>If you haven't already, log onto the HPC</li> <li>create a blank python file: <code>touch myscript.py</code></li> <li> <p>edit the script: <code>nano myscript.py</code></p> <p>a. feel free to use vim or emacs or any other text editor if you are more comfortable with one of those</p> <p>b. add something simple like     <code>print(\u201chello world!\u201d)</code> 4. be sure to save it!</p> </li> </ol> <p>Step 2: Write your batch script</p> <p>Batch Script</p> <p>Part 1: Use directives to tell the scheduler what resources you want</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test \n#SBATCH --output=test_%A.out \n#SBATCH --error=test_%A.err \n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --partition=standard \n#SBATCH --account=&lt;your_account&gt; \n#SBATCH --time=00:05:00\n</code></pre> <p>Part 2: Load your modules</p> <pre><code>module load python/3.11\n</code></pre> <p>Part 3: Run your script</p> <pre><code>python3 myscript.py\n</code></pre> <p>Step 3: Submit your job</p> <ol> <li> <p>Send to scheduler</p> <p><pre><code>sbatch myscript.slurm\n</code></pre> 2. Check the queue</p> <pre><code>squeue --user=&lt;my netid&gt;\n</code></pre> </li> <li> <p>View the output files</p> <p><pre><code>cat &lt;name&gt;.out\ncat &lt;name&gt;.err\n</code></pre> 4. View the job history report</p> <pre><code>job-history &lt;job id&gt;\n</code></pre> </li> </ol> <p>Congrats! You just ran your first batch job!</p>"},{"location":"support_and_training/workshops/intro_to_hpc/#additional-resources","title":"Additional Resources","text":"<p>The HPC Consult Team is here for you!</p> <ul> <li>Documentation: docs.hpc.arizona.edu</li> <li>GitHub: ua-researchcomputing-hpc.github.io</li> <li>ServiceNow: HPC Support and Consulting Request</li> <li>Office Hours: every Wednesday 2-4pm on GatherTown</li> </ul>"},{"location":"support_and_training/workshops/intro_to_linux/","title":"Intro to Linux","text":""},{"location":"support_and_training/workshops/intro_to_linux/#intro-to-linux-workshop-materials","title":"Intro to Linux Workshop Materials","text":""},{"location":"support_and_training/workshops/intro_to_linux/#workshop-files","title":"Workshop Files","text":"<p> Click here to download PDF Version Click here to download PowerPoint Version <p></p>"},{"location":"support_and_training/workshops/intro_to_linux/#video-presentation","title":"Video Presentation","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/R/","title":"R","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#machine-learning-in-r","title":"Machine Learning in R","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#training-documents","title":"Training Documents","text":"<p> Click here to download PDF Version </p> <p> Click here to download PowerPoint Version </p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#video-presentation","title":"Video Presentation","text":"<p>Will upload once on YouTube</p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#interactive-materials","title":"Interactive Materials","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#exercise-data-pre-processing","title":"Exercise: Data Pre-processing","text":"<pre><code># install package \u201cVIM\u201d\ninstall.packages(\"VIM\")\n# Hint: use multiple cores. It goes much faster\n# To use the package in an R session, we need to load it in an R session via\nlibrary()\nlibrary(VIM)\n# Load dataset \u201csleep\u201d, which comes within the package \u201cVIM\u201d\ndata(sleep, package =\"VIM\")\n# call function head() to get a feeling about data, or call sleep to see all values\nhead(sleep)\n# download package \u201cmice\u201d and load it into R\ninstall.packages(\"mice\")\nlibrary(mice)\n\n# First, we need to know how many rows in \u201csleep\u201d\nnrow(sleep)\n## [1] 62\n# We use complete.cases() or na.omit() to see tuples without missing value.\nsleep[complete.cases(sleep),]\n# or try\nna.omit(sleep)\n# Count the number of rows without missing value\nnrow(sleep[complete.cases(sleep),])\n## [1] 42\n# To reverse the condition logic (rows containing one or more missing value), we\nuse the exclamation mark highlighted in Red\nsleep[!complete.cases(sleep),]\nnrow(sleep[!complete.cases(sleep),])\n## [1] 20\n\n# Check how many observations contain missing value in column \u201cDream\u201d\nsum(is.na(sleep$Dream))\n## [1] 12\n# About 19% of obs (observations) in column Dream contain missing value\nmean(is.na(sleep$Dream))\n## [1] 0.1935484\n# 32% obs in data frame sleep containing one or more missing value\nmean(!complete.cases(sleep))\n## [1] 0.3225806\n\n# call function md.pattern(). Make sure you loaded package mice into R first\nmd.pattern(sleep)\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#exercise-visualization-aggr","title":"Exercise: Visualization \u2013 aggr","text":"<pre><code># call function aggr (), prop = FALSE convert percentage value into counts\naggr(sleep, prop = FALSE, numbers = TRUE)\n\n# call function marginplot (), pch indicates notation of obs, col tells R how you\nwould like to see results in different color\nmarginplot(sleep[c(\"Gest\", \"Dream\")], pch=c(20),\ncol = c(\"darkgray\",\"red\",\"blue\") )\n\nboxplot(mpg ~ cyl,\n# mpg is the target variable\n# cyl is the explanatory variable\ndata = mtcars,\ncol = \"grey\",\nmain = \"Mileage Data\",\nylab = \"MPG\",\nxlab = \"Number of Cylinders\" )\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#exercise-visualization-violin-plot","title":"Exercise: Visualization \u2013 violin plot","text":"<pre><code>install.packages(\u201cvioplot\u201d)\nlibrary(vioplot)\nv1 &lt;- mtcars$mpg[mtcars$cyl == 4]\nv2 &lt;- mtcars$mpg[mtcars$cyl == 6]\nv3 &lt;- mtcars$mpg[mtcars$cyl == 8]\n# draw violin plots for vectors\nvioplot(v1,v2,v3,\nnames=c(\u201c4 cylinders\u201d, \u201c6 cylinders\u201d, \u201c8 cylinders\u201d),\ncol=\u201cgold\u201d)\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#exercise-visualization-scatter-plot","title":"Exercise: Visualization \u2013 scatter plot","text":"<pre><code>plot(mpg ~ wt, data = mtcars)\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#exercise-naive-bayes-classifier","title":"Exercise: Naive Bayes Classifier","text":"<pre><code>getwd()\nsetwd(\u2018/xdisk/chrisreidy/workshops\u2019)\n\ninstall.packages(\u2018e1071\u2019)\nlibrary(e1071)\n\n# read in csv file mushroom.csv. Note the question mark\nrepresents null value\nmushroom &lt;- read.csv(\u201cMushroom.csv\u201d, na.strings = \u201c?\u201d)\nsummary(mushroom)\n\n# check completion\nnrow(mushroom[!complete.cases(mushroom),])\n## [1] 2480\n\n# we can retain observations that do not contain NA(null) value\nmushroom = mushroom[complete.cases(mushroom),]\n\n# 70% of original data will be used for training\nsample_size &lt;- floor(0.7 * nrow(mushroom))\n# randomly select index of observations for training\ntraining_index &lt;- sample(nrow(mushroom), size = sample_size,\nreplace = FALSE)\ntrain &lt;- mushroom[training_index,]\ntest &lt;- mushroom[-training_index,]\n\n# note the period coming after tilde. It means all the other\nvariables in that dataset will be predictive variable\nmushroom.model &lt;- naiveBayes(classes ~ . , data = train)\n# We can explore the detail conditional probabilities for each\nvariables by calling the object mushroom.model itself.\nmushroom.model\n\n# The result of prediction, a vector, will be attached to test\nset labelled as \u201cclass\u201d. The return of prediction is a vector\nincluding predicted type of mushroom\nmushroom.predict &lt;- predict(mushroom.model, test, type = \u201cclass\u201d)\n\n# pick actual value and predicted value together in a dataframe\ncalled results\nresults &lt;- data.frame(actual = test[,'classes'], predicted =\nmushroom.predict)\n# We can get a popular matrix called confusion matrix\ntable(results)\n# columns indicate the number of mushrooms in actual type;\nlikewise, rows indicate the number those in predicted type.\npredicted\nactual edible poisonous\nedible TN=1067 FP= 2\npoisonous FN= 46 TP=580\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/R/#exercise-neural-network","title":"Exercise: Neural Network","text":"<pre><code>install.packages('ISLR')\ninstall.packages('caTools')\ninstall.packages('neuralnet')\n\nlibrary(ISLR)\nprint(head(College, 2))\n\n# Create Vector of Column Max and Min Values\nmaxs &lt;- apply(College[,2:18], 2, max)\nmins &lt;- apply(College[,2:18], 2, min)# Use scale() and convert the resulting matrix to a data frame\nscaled.data &lt;- as.data.frame(scale(College[,2:18], center = mins,\n                scale = maxs - mins))\n\n# Check out results\nprint(head(scaled.data,2))\n\n# Convert Private column from Yes/No to 1/0\nPrivate = as.numeric(College$Private)-1\ndata = cbind(Private, scaled.data)\nlibrary(caTools)\nset.seed(101) # sets random numbers# Create Split (any column is fine)\nsplit = sample.split(data$Private, SplitRatio = 0.70)# Split based off split Boolean Vector\ntrain = subset(data, split == TRUE)\ntest = subset(data, split == FALSE)\n\nfeats &lt;- names(scaled.data)\n# Concatenate strings\nf &lt;- paste(feats,collapse=' + ')\nf &lt;- paste('Private ~', f)\n# Convert to formula\nf &lt;- as.formula(f)\nf\n## Private ~ Apps + Accept + Enroll + Top10perc + Top25perc + F.Undergrad +\n## P.Undergrad + Outstate + Room.Board + Books + Personal +\n## PhD + Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate\nlibrary(neuralnet)\nnn &lt;- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE)\n\n# Compute Predictions off Test Set\npredicted.nn.values &lt;- compute(nn, test[2:18])# Check out net.result\nprint(head(predicted.nn.values$net.result))\n\npredicted.nn.values$net.result &lt;- sapply(predicted.nn.values$net.result,\n                                  round, digits = 0)\n\ntable(test$Private, predicted.nn.values$net.result)\nplot(nn)\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/","title":"Python","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#intro-to-machine-learning-in-python","title":"Intro to Machine Learning in Python","text":"<p>This short training class provides a brief introduction to key concepts of machine learning.  The short lecture will be followed by two hands-on examples that emphasize running a Jupyter notebook on the HPC supercomputers. For the in-person workshop you can stick around and use this as a consulting session.</p> <p> Click here to download PDF </p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#video-presentation","title":"Video Presentation","text":"<p>Will upload once on YouTube</p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#hands-on-content","title":"Hands On Content","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#setting-up","title":"Setting up","text":"<p>A component of these workshops is interactive where users will learn to:</p> <ol> <li>Train and visualize a linear regression model using a training set.</li> <li>Build and visualize a clustering model using the elbow method.</li> </ol> <p>Both of these exercises will make use of Python in a Jupyter Notebook through Open OnDemand.</p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#accessing-the-data","title":"Accessing the Data","text":"<p>To begin, start a terminal to log into the system and copy the necessary files into your account. If you're unsure of how to use or access a terminal, see our system access page for information (or, if you're in a live workshop, flag one of us down and we can help). To get the files you need, use the following commands:  <pre><code>ssh your_netid@hpc.arizona.edu\nshell\nelgato\n</code></pre> Now, to download the example, use: <pre><code>wget https://ua-researchcomputing-hpc.github.io/Intro-to-Machine-Learning/intro-to-ML.tar.gz\ntar xzvf intro-to-ML.tar.gz\nrm intro-to-ML.tar.gz\n</code></pre></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#starting-a-jupyter-session","title":"Starting a Jupyter Session","text":"<p>For this tutorial, we'll use a Jupyter Notebook which is available as an interactive application and can be accessed through Open OnDemand.</p> <p> </p> <p>Once you log in using your university credentials, click the Interactive Apps dropdown menu and select Jupyter Notebook. This will bring you to a web form that's used to request compute resources on one of our clusters. Use the following options in your request:</p> Option Value Cluster ElGato Cluter Run Time 2 Core count on a single node 1 Memory per core 4 PI Group your group** Queue standard <p>** If you don't know your group's name, go to a terminal session (see section above) and use the command <code>va</code>.</p> <p>Once you complete the form, click Launch. This will bring you to a page with a tile that shows your pending job. When it's first submitted, its status will show as Queued. Once it starts, it's status will change to Running and you'll be given a link you can use to connect. </p> <p></p> <p></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#opening-a-notebook","title":"Opening a Notebook","text":"<p>Once you've clicked Launch, you'll see a file navigator. This is your home directory on HPC. To access the files for the examples, click the intro-to-hpc directory you created earlier. To open a notebook, click the New dropdown menu in the upper right and select python3.</p> <p></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#running-code","title":"Running Code","text":"<p>To run Python code in a notebook, enter your commands into a cell and click <code>Run</code>. To add a new cell, click the <code>+</code> in the upper left. </p> <p></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#linear-regression-example","title":"Linear Regression Example","text":"<p>Import Libraries <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n</code></pre> Use Pandas to load the data and view the first five rows <pre><code>data = pd.read_excel(\"king_county_house_data.xls\")\ndata.head(5)\n</code></pre> Choose the columns from the data and split into train and test sets <pre><code>space = data['sqft_living']\nprice = data['price']\n# Change X into 2D array\nX = np.array(space).reshape(-1, 1)\nY = np.array(price)\n# Split data into train sets and test sets\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=1/3,random_state=0)\n</code></pre> Visualize the train set <pre><code># Visualize training set\nplt.scatter(X_train,Y_train,color=\"red\",label=\"Living Area\")\nplt.title(\"Housing Prices\")\nplt.xlabel(\"Area\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.show()\n</code></pre> Train the model with the training set and predict with the test set <pre><code># Train\nregressor = LinearRegression()\nregressor.fit(X_train, Y_train)\n# Prediction\ny_pred = regressor.predict(X_test)\n</code></pre></p> <p>Visualize the train data and the best fit line <pre><code># Visualize the data and the best fit line\nplt.scatter(X_train,Y_train,color=\"red\",label=\"Living Area\")\nplt.title(\"Housing Prices in King County\")\nplt.plot(X_train,regressor.predict(X_train),color=\"blue\",label=\"Price\")\nplt.xlabel(\"Area\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.show()\n</code></pre> Predict the price of a house with a certain area <pre><code># Make a prediction\narea = 5000\nprice = regressor.predict([[area]])\nprint('House of %d sq-ft costs about $%d' % (area, price))\n</code></pre> Visualize the test data <pre><code># Visualize test set\nplt.scatter(X_test,Y_test,color='red',label=\"Living Area\")\nplt.plot(X_test,regressor.predict(X_test),color=\"blue\",label=\"Price\")\nplt.xlabel=\"Area (sq-ft)\")\nplt.ylabel(\"Price (USD)\")\nplt.legend()\nplt.show()\n</code></pre></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#clustering-model-example","title":"Clustering Model Example","text":"<p>Import libraries  <pre><code># import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\n</code></pre> Load the data <pre><code># load the data\niris=load_iris()\niris\n</code></pre> Convert to a dataframe <pre><code>df=pd.DataFrame(data=iris.data, columns=['sepal length','sepal width','petal length','petal width'])\ndf['target']=pd.Series(iris.target)\ndf\n</code></pre> Visualize the data <pre><code># visualize the data\nplt.scatter(x=df['sepal length'], y=df['sepal width'] ,c=iris.target, cmap='gist_rainbow')\nplt.xlabel('Sepal Width', fontsize=18)\nplt.ylabel('Sepal Length', fontsize=18)\n</code></pre> Estimate k with elbow method. First try k=5. <pre><code># Estimate k with elbow at k=5\nx = iris.data\nkmeans5 = KMeans(n_clusters=5,init = 'k-means++', random_state = 0)\ny = kmeans5.fit_predict(x)\nprint(y)\n</code></pre> Visualize centers. <pre><code>kmeans5.cluster_centers_\nplt.scatter(x[y == 0,0], x[y==0,1], s = 15, c= 'red', label = 'Cluster_1')\nplt.scatter(x[y == 1,0], x[y==1,1], s = 15, c= 'blue', label = 'Cluster_2')\nplt.scatter(x[y == 2,0], x[y==2,1], s = 15, c= 'green', label = 'Cluster_3')\nplt.scatter(x[y == 3,0], x[y==3,1], s = 15, c= 'cyan', label = 'Cluster_4')\nplt.scatter(x[y == 4,0], x[y==4,1], s = 15, c= 'magenta', label = 'Cluster_5')\nplt.scatter(kmeans5.cluster_centers_[:,0], kmeans5.cluster_centers_[:,1], s = 25, c = 'yellow', label = 'Centroids')\nplt.legend()\nplt.show()\n</code></pre> Estimate k with elbow method. <pre><code># estimate k with elbow method\nError =[]\nfor i in range(1, 11):\n    kmeans11 = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0).fit(x)\n    kmeans11.fit(x)\n    Error.append(kmeans11.inertia_)\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 11), Error)\nplt.title('Elbow Method with k=1-11') #within cluster sum of squares\nplt.xlabel('Number of clusters')\nplt.ylabel('Error')\nplt.show()\n</code></pre> Get the optimal k=3 from the elbow method. Cluster centers and visualize.</p> <pre><code>kmeans3 = KMeans(n_clusters=3, random_state=21)\ny = kmeans3.fit_predict(x)\nkmeans3.cluster_centers_\nplt.scatter(x[y == 0,0], x[y==0,1], s = 15, c= 'red', label = 'Cluster_1')\nplt.scatter(x[y == 1,0], x[y==1,1], s = 15, c= 'blue', label = 'Cluster_2')\nplt.scatter(x[y == 2,0], x[y==2,1], s = 15, c= 'green', label = 'Cluster_3')\nplt.scatter(kmeans3.cluster_centers_[:,0], kmeans3.cluster_centers_[:,1], s = 25, c = 'black', label = 'Centroids')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#neural-network-classification-example","title":"Neural Network Classification Example","text":"<p>In this example we will use the same Iris dataset that we used in the unsupervised clustering example to train a neural network model to classify the plants using labeled input.</p> <p>We will use the Keras interface to the popular Tensorflow neural network package</p> <p>Import Sscikit learn and Tensorflow/Keras packages:</p> <pre><code>from numpy import argmax\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.datasets import load_iris\nimport tensorflow\n</code></pre> <p>Prepare the built in input data * change measurements to floating point numbers * Change names to integers</p> <pre><code>iris = load_iris()\nX, y = iris.data, iris.target\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y)\n</code></pre> <p>split into train and test datasets</p> <p>Try this</p> <p>use different proportions for test_size, to change the amount of training data and make the classification problem more or less difficult</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nn_features = X_train.shape[1]\nprint(\"%d measurements per sample\" %  n_features)\n</code></pre> <p>Now set up the details of the neural network model.</p> <ul> <li>Add \"Dense\" (fully connected) neural network layers, specifying the number of nodes per layer</li> <li>activation functions define node output values as a function of node input values</li> <li>kernel_initializer specifies initial values for node connection weights</li> <li>input_shape ensures that the inputs to the first layer are equal to the number of measurements per sample</li> <li>the 3 nodes of the final  layer correspond to the three species designations</li> <li>softmax activation function ensures that the layer outputs all sum to 1.0 (i.e., are probabilities)</li> </ul> <p>Try this</p> <p>change the number of nodes in the first and second layers</p> <pre><code>model = Sequential()\n\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n\nmodel.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n</code></pre> <p>Now train the model using the training data subset * epochs and batch_size control the efficiency of the fitting</p> <pre><code>model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n</code></pre> <p>Evaluate and use the model:</p> <pre><code>loss, acc = model.evaluate(X_test, y_test)\nprint('Test Accuracy: %.3f' % acc)\nrow = [5.1,3.5,1.4,0.2]\nyhat = model.predict([row])\nprint('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n</code></pre> <p>Try making changes and see how it affects the classification accuracy.</p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#image-classification-with-pytorch","title":"Image Classification with PyTorch","text":"<p>For a more in-depth Neural Network example that is run as a batch submission instead of through Jupyter, see this page:</p> <p> Image Classification </p>"},{"location":"support_and_training/workshops/intro_to_parallel_computing/","title":"Intro to Parallel Computing","text":""},{"location":"support_and_training/workshops/intro_to_parallel_computing/#introduction-to-parallel-computing","title":"Introduction to Parallel Computing","text":"<p> Click here to download PDF </p>"},{"location":"support_and_training/workshops/intro_to_parallel_computing/#video-presentation","title":"Video Presentation","text":""},{"location":"support_and_training/workshops/intro_to_visualization/","title":"Intro to Visualization","text":""},{"location":"support_and_training/workshops/intro_to_visualization/#introduction-to-visualization-on-hpc","title":"Introduction to Visualization on HPC","text":"<p>This workshop provides an introduction to some concepts of visualization. It is done in the context of HPC so you will be able to follow along with the practical examples section using your HPC Account.</p> <p> Click here to download PDF </p>"}]}